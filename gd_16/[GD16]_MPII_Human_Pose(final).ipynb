{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f975d4ff",
   "metadata": {},
   "source": [
    "## 16. 행동 스티커 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c3090",
   "metadata": {},
   "source": [
    "### [ 목  차 ]\n",
    "1. 데이터 전처리하기\n",
    "2. TFRecord 파일 만들기\n",
    "3. Ray\n",
    "4. data label 로 만들기\n",
    "5. 모델 학습\n",
    "  * Hourglass 모델링\n",
    "6. 학습 엔진 만들기\n",
    "7. 예측 엔진 만들기\n",
    "8. Project: 모델 바꿔보기\n",
    "  * Simplebaseline 모델링\n",
    "9. 결론\n",
    "\n",
    "[ 회 고 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b0e88",
   "metadata": {},
   "source": [
    "### 1. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd530a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4fe6517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# json 파싱하기\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff2746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# json annotation 을 파싱하는 함수\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eeadf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "# parse_one_annotation()함수를 테스트\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76de1ed",
   "metadata": {},
   "source": [
    "### 2. TFRecord 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c770f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 하나의 annotation을 하나의 tf.train.Example로 만들어 주는 함수\n",
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9d7e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 얼마나 많은 TFRecord를 만들지 결정할 함수\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed582839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# chunkify함수 테스트\n",
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa26fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 하나의 chunk를 TFRecord로 만들어 줄 함수\n",
    "# Ray는 병렬 처리를 위한 라이브러리인데요. 파이썬에서 기본적으로 제공하는 multiprocessing 패키지보다 편하게 다양한 환경에서 사용\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d41092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터를 적당한 수의 TFRecord 파일로 만들어주는 함수\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11f3e9",
   "metadata": {},
   "source": [
    "### 3. Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "351758c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 23:05:50,920\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.85gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=188)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=187)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=189)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecordsSuccessfully wrote 25204 annotations to TF Records.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 작성한 함수를 사용해 데이터를 TFRecord로 작성\n",
    "# train 데이터는 64개로, val 데이터는 8개의 파일로\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d935b",
   "metadata": {},
   "source": [
    "### 4. data label 로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef877e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# TFRecord로 저장된 데이터를 모델에 학습에 필요한 데이터로 바꿔줄 함수\n",
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e3b387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 이미지를 그대로 사용하지 않고 적당히 정사각형으로 crop하여 사용\n",
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c6fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# (x, y) 좌표로 되어있는 keypoint 를 heatmap 으로 변경\n",
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a7e4f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 지금까지 만든 함수들을 객체(class) 형태로 조합\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3b362",
   "metadata": {},
   "source": [
    "### 5. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8150a1",
   "metadata": {},
   "source": [
    "#### Hourglass 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440a8581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69cf1d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0035b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# intermediate output을 위한 linear layer\n",
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6fbcee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# stacked hourglass\n",
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4cffb",
   "metadata": {},
   "source": [
    "### 6. 학습 엔진 만들기\n",
    "GPU가 여러 개인 환경:     \n",
    "가장 핵심 키워드는 tf.distribute.MirroredStrategy입니다. 한 컴퓨터에 GPU가 여러 개인 경우 사용할 수 있는 방법인데요. 여러 GPU가 모델을 학습한 후 각각의 Loss를 계산하면 CPU가 전체 Loss를 종합합니다. 그런 후 모델의 가중치를 업데이트 하도록 하는 것이죠.    \n",
    "각 GPU에서 계산한 Loss를 토대로 전체 Loss를 종합해주는 역할은 strategy.reduce 함수가 담당합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "422c0db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model_HG-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "102a5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 만드는 함수\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c10cf843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.63002634 epoch total loss 2.63002634\n",
      "Trained batch 2 batch loss 2.37071157 epoch total loss 2.50036907\n",
      "Trained batch 3 batch loss 2.54354453 epoch total loss 2.51476097\n",
      "Trained batch 4 batch loss 2.12307787 epoch total loss 2.41684\n",
      "Trained batch 5 batch loss 2.12745762 epoch total loss 2.35896349\n",
      "Trained batch 6 batch loss 2.10977554 epoch total loss 2.31743217\n",
      "Trained batch 7 batch loss 2.07241297 epoch total loss 2.28242946\n",
      "Trained batch 8 batch loss 2.1175077 epoch total loss 2.26181436\n",
      "Trained batch 9 batch loss 2.12731934 epoch total loss 2.24687052\n",
      "Trained batch 10 batch loss 1.81066942 epoch total loss 2.20325041\n",
      "Trained batch 11 batch loss 1.99760377 epoch total loss 2.18455529\n",
      "Trained batch 12 batch loss 1.97001588 epoch total loss 2.166677\n",
      "Trained batch 13 batch loss 1.66800332 epoch total loss 2.12831736\n",
      "Trained batch 14 batch loss 1.97750592 epoch total loss 2.11754537\n",
      "Trained batch 15 batch loss 2.03217268 epoch total loss 2.11185384\n",
      "Trained batch 16 batch loss 1.99769235 epoch total loss 2.10471869\n",
      "Trained batch 17 batch loss 2.01683068 epoch total loss 2.09954882\n",
      "Trained batch 18 batch loss 2.04420829 epoch total loss 2.09647441\n",
      "Trained batch 19 batch loss 2.01392078 epoch total loss 2.09212947\n",
      "Trained batch 20 batch loss 1.98805642 epoch total loss 2.08692575\n",
      "Trained batch 21 batch loss 1.9511199 epoch total loss 2.08045864\n",
      "Trained batch 22 batch loss 1.89280176 epoch total loss 2.07192898\n",
      "Trained batch 23 batch loss 1.78474092 epoch total loss 2.05944252\n",
      "Trained batch 24 batch loss 1.88905 epoch total loss 2.05234265\n",
      "Trained batch 25 batch loss 1.91328311 epoch total loss 2.04678035\n",
      "Trained batch 26 batch loss 1.98131847 epoch total loss 2.04426265\n",
      "Trained batch 27 batch loss 1.88144588 epoch total loss 2.03823256\n",
      "Trained batch 28 batch loss 1.94990146 epoch total loss 2.03507781\n",
      "Trained batch 29 batch loss 1.84017527 epoch total loss 2.02835703\n",
      "Trained batch 30 batch loss 1.7984792 epoch total loss 2.02069449\n",
      "Trained batch 31 batch loss 1.80464602 epoch total loss 2.01372504\n",
      "Trained batch 32 batch loss 1.83397973 epoch total loss 2.00810814\n",
      "Trained batch 33 batch loss 1.83734131 epoch total loss 2.0029335\n",
      "Trained batch 34 batch loss 1.86848676 epoch total loss 1.99897897\n",
      "Trained batch 35 batch loss 1.89850044 epoch total loss 1.99610817\n",
      "Trained batch 36 batch loss 1.88755369 epoch total loss 1.99309266\n",
      "Trained batch 37 batch loss 1.89402366 epoch total loss 1.99041498\n",
      "Trained batch 38 batch loss 1.89899325 epoch total loss 1.98800921\n",
      "Trained batch 39 batch loss 1.92930734 epoch total loss 1.98650396\n",
      "Trained batch 40 batch loss 1.92990494 epoch total loss 1.98508906\n",
      "Trained batch 41 batch loss 1.90561581 epoch total loss 1.98315072\n",
      "Trained batch 42 batch loss 1.86581957 epoch total loss 1.98035717\n",
      "Trained batch 43 batch loss 1.69378924 epoch total loss 1.97369277\n",
      "Trained batch 44 batch loss 1.74006343 epoch total loss 1.96838307\n",
      "Trained batch 45 batch loss 1.81663358 epoch total loss 1.96501088\n",
      "Trained batch 46 batch loss 1.83463848 epoch total loss 1.9621768\n",
      "Trained batch 47 batch loss 1.82157159 epoch total loss 1.95918512\n",
      "Trained batch 48 batch loss 1.8181808 epoch total loss 1.95624769\n",
      "Trained batch 49 batch loss 1.83593607 epoch total loss 1.95379233\n",
      "Trained batch 50 batch loss 1.81777322 epoch total loss 1.95107198\n",
      "Trained batch 51 batch loss 1.77175772 epoch total loss 1.94755602\n",
      "Trained batch 52 batch loss 1.79335535 epoch total loss 1.94459069\n",
      "Trained batch 53 batch loss 1.74573195 epoch total loss 1.94083869\n",
      "Trained batch 54 batch loss 1.83215368 epoch total loss 1.93882596\n",
      "Trained batch 55 batch loss 1.74697089 epoch total loss 1.93533766\n",
      "Trained batch 56 batch loss 1.73069894 epoch total loss 1.93168342\n",
      "Trained batch 57 batch loss 1.71449065 epoch total loss 1.92787302\n",
      "Trained batch 58 batch loss 1.62221098 epoch total loss 1.92260289\n",
      "Trained batch 59 batch loss 1.6218003 epoch total loss 1.91750467\n",
      "Trained batch 60 batch loss 1.73840761 epoch total loss 1.91451979\n",
      "Trained batch 61 batch loss 1.75719297 epoch total loss 1.91194069\n",
      "Trained batch 62 batch loss 1.76512754 epoch total loss 1.90957272\n",
      "Trained batch 63 batch loss 1.7265538 epoch total loss 1.90666771\n",
      "Trained batch 64 batch loss 1.59624696 epoch total loss 1.90181732\n",
      "Trained batch 65 batch loss 1.69120455 epoch total loss 1.89857721\n",
      "Trained batch 66 batch loss 1.79100788 epoch total loss 1.89694738\n",
      "Trained batch 67 batch loss 1.6647023 epoch total loss 1.89348102\n",
      "Trained batch 68 batch loss 1.74186754 epoch total loss 1.89125156\n",
      "Trained batch 69 batch loss 1.71548414 epoch total loss 1.88870418\n",
      "Trained batch 70 batch loss 1.78377771 epoch total loss 1.88720524\n",
      "Trained batch 71 batch loss 1.77794123 epoch total loss 1.88566637\n",
      "Trained batch 72 batch loss 1.80483675 epoch total loss 1.88454378\n",
      "Trained batch 73 batch loss 1.69412088 epoch total loss 1.88193524\n",
      "Trained batch 74 batch loss 1.55021203 epoch total loss 1.87745249\n",
      "Trained batch 75 batch loss 1.54171264 epoch total loss 1.87297606\n",
      "Trained batch 76 batch loss 1.47704291 epoch total loss 1.86776638\n",
      "Trained batch 77 batch loss 1.61736691 epoch total loss 1.86451447\n",
      "Trained batch 78 batch loss 1.49201918 epoch total loss 1.85973883\n",
      "Trained batch 79 batch loss 1.46165121 epoch total loss 1.85469985\n",
      "Trained batch 80 batch loss 1.4163264 epoch total loss 1.84922\n",
      "Trained batch 81 batch loss 1.44024873 epoch total loss 1.84417105\n",
      "Trained batch 82 batch loss 1.63047791 epoch total loss 1.84156501\n",
      "Trained batch 83 batch loss 1.79131246 epoch total loss 1.84095943\n",
      "Trained batch 84 batch loss 1.71862745 epoch total loss 1.83950317\n",
      "Trained batch 85 batch loss 1.66107523 epoch total loss 1.83740401\n",
      "Trained batch 86 batch loss 1.67036963 epoch total loss 1.83546162\n",
      "Trained batch 87 batch loss 1.76539242 epoch total loss 1.83465624\n",
      "Trained batch 88 batch loss 1.77517545 epoch total loss 1.83398044\n",
      "Trained batch 89 batch loss 1.68181658 epoch total loss 1.83227074\n",
      "Trained batch 90 batch loss 1.62927341 epoch total loss 1.83001518\n",
      "Trained batch 91 batch loss 1.4229399 epoch total loss 1.82554185\n",
      "Trained batch 92 batch loss 1.53652835 epoch total loss 1.82240045\n",
      "Trained batch 93 batch loss 1.59859169 epoch total loss 1.81999385\n",
      "Trained batch 94 batch loss 1.66501844 epoch total loss 1.81834531\n",
      "Trained batch 95 batch loss 1.65467489 epoch total loss 1.8166225\n",
      "Trained batch 96 batch loss 1.76787579 epoch total loss 1.81611478\n",
      "Trained batch 97 batch loss 1.70229173 epoch total loss 1.81494129\n",
      "Trained batch 98 batch loss 1.67307043 epoch total loss 1.81349349\n",
      "Trained batch 99 batch loss 1.76341796 epoch total loss 1.81298769\n",
      "Trained batch 100 batch loss 1.7266295 epoch total loss 1.81212401\n",
      "Trained batch 101 batch loss 1.67397809 epoch total loss 1.81075633\n",
      "Trained batch 102 batch loss 1.68760574 epoch total loss 1.80954897\n",
      "Trained batch 103 batch loss 1.69534862 epoch total loss 1.80844009\n",
      "Trained batch 104 batch loss 1.64439249 epoch total loss 1.80686271\n",
      "Trained batch 105 batch loss 1.57802689 epoch total loss 1.80468345\n",
      "Trained batch 106 batch loss 1.71158671 epoch total loss 1.80380523\n",
      "Trained batch 107 batch loss 1.57628882 epoch total loss 1.8016789\n",
      "Trained batch 108 batch loss 1.6303103 epoch total loss 1.80009222\n",
      "Trained batch 109 batch loss 1.75140166 epoch total loss 1.79964554\n",
      "Trained batch 110 batch loss 1.74263358 epoch total loss 1.79912722\n",
      "Trained batch 111 batch loss 1.68426847 epoch total loss 1.79809237\n",
      "Trained batch 112 batch loss 1.63121581 epoch total loss 1.79660237\n",
      "Trained batch 113 batch loss 1.75708318 epoch total loss 1.79625261\n",
      "Trained batch 114 batch loss 1.73157406 epoch total loss 1.79568517\n",
      "Trained batch 115 batch loss 1.68613911 epoch total loss 1.79473269\n",
      "Trained batch 116 batch loss 1.6454494 epoch total loss 1.79344571\n",
      "Trained batch 117 batch loss 1.63928783 epoch total loss 1.79212809\n",
      "Trained batch 118 batch loss 1.59543347 epoch total loss 1.79046118\n",
      "Trained batch 119 batch loss 1.73366332 epoch total loss 1.78998375\n",
      "Trained batch 120 batch loss 1.68999088 epoch total loss 1.78915048\n",
      "Trained batch 121 batch loss 1.70076251 epoch total loss 1.78842\n",
      "Trained batch 122 batch loss 1.75538754 epoch total loss 1.78814924\n",
      "Trained batch 123 batch loss 1.74974144 epoch total loss 1.78783691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 124 batch loss 1.69745803 epoch total loss 1.78710818\n",
      "Trained batch 125 batch loss 1.74132276 epoch total loss 1.78674185\n",
      "Trained batch 126 batch loss 1.68301725 epoch total loss 1.78591859\n",
      "Trained batch 127 batch loss 1.57026649 epoch total loss 1.78422058\n",
      "Trained batch 128 batch loss 1.5964663 epoch total loss 1.78275371\n",
      "Trained batch 129 batch loss 1.57566595 epoch total loss 1.78114843\n",
      "Trained batch 130 batch loss 1.65486991 epoch total loss 1.78017712\n",
      "Trained batch 131 batch loss 1.73217583 epoch total loss 1.77981067\n",
      "Trained batch 132 batch loss 1.65057862 epoch total loss 1.7788316\n",
      "Trained batch 133 batch loss 1.57329106 epoch total loss 1.77728617\n",
      "Trained batch 134 batch loss 1.53051615 epoch total loss 1.77544463\n",
      "Trained batch 135 batch loss 1.55718374 epoch total loss 1.77382791\n",
      "Trained batch 136 batch loss 1.66388583 epoch total loss 1.77301943\n",
      "Trained batch 137 batch loss 1.49610829 epoch total loss 1.77099824\n",
      "Trained batch 138 batch loss 1.46602595 epoch total loss 1.76878822\n",
      "Trained batch 139 batch loss 1.43728828 epoch total loss 1.76640332\n",
      "Trained batch 140 batch loss 1.464921 epoch total loss 1.7642498\n",
      "Trained batch 141 batch loss 1.57234049 epoch total loss 1.76288879\n",
      "Trained batch 142 batch loss 1.46321869 epoch total loss 1.76077855\n",
      "Trained batch 143 batch loss 1.52317643 epoch total loss 1.75911701\n",
      "Trained batch 144 batch loss 1.59772074 epoch total loss 1.75799608\n",
      "Trained batch 145 batch loss 1.59526348 epoch total loss 1.75687385\n",
      "Trained batch 146 batch loss 1.54649127 epoch total loss 1.75543272\n",
      "Trained batch 147 batch loss 1.56857812 epoch total loss 1.7541616\n",
      "Trained batch 148 batch loss 1.57402074 epoch total loss 1.75294459\n",
      "Trained batch 149 batch loss 1.6393832 epoch total loss 1.75218236\n",
      "Trained batch 150 batch loss 1.69353807 epoch total loss 1.75179136\n",
      "Trained batch 151 batch loss 1.77440417 epoch total loss 1.7519412\n",
      "Trained batch 152 batch loss 1.6892221 epoch total loss 1.7515285\n",
      "Trained batch 153 batch loss 1.69924283 epoch total loss 1.75118685\n",
      "Trained batch 154 batch loss 1.55061412 epoch total loss 1.74988449\n",
      "Trained batch 155 batch loss 1.6772213 epoch total loss 1.74941564\n",
      "Trained batch 156 batch loss 1.66078115 epoch total loss 1.74884737\n",
      "Trained batch 157 batch loss 1.70948768 epoch total loss 1.74859655\n",
      "Trained batch 158 batch loss 1.72023273 epoch total loss 1.74841714\n",
      "Trained batch 159 batch loss 1.67395711 epoch total loss 1.74794877\n",
      "Trained batch 160 batch loss 1.58043694 epoch total loss 1.74690187\n",
      "Trained batch 161 batch loss 1.54983556 epoch total loss 1.74567783\n",
      "Trained batch 162 batch loss 1.63434744 epoch total loss 1.74499059\n",
      "Trained batch 163 batch loss 1.58264768 epoch total loss 1.74399459\n",
      "Trained batch 164 batch loss 1.60922623 epoch total loss 1.74317276\n",
      "Trained batch 165 batch loss 1.54347467 epoch total loss 1.74196255\n",
      "Trained batch 166 batch loss 1.57915616 epoch total loss 1.74098182\n",
      "Trained batch 167 batch loss 1.59293401 epoch total loss 1.74009526\n",
      "Trained batch 168 batch loss 1.61966145 epoch total loss 1.73937845\n",
      "Trained batch 169 batch loss 1.71650815 epoch total loss 1.73924315\n",
      "Trained batch 170 batch loss 1.7140274 epoch total loss 1.73909485\n",
      "Trained batch 171 batch loss 1.67830801 epoch total loss 1.73873937\n",
      "Trained batch 172 batch loss 1.55085993 epoch total loss 1.73764718\n",
      "Trained batch 173 batch loss 1.55268741 epoch total loss 1.73657787\n",
      "Trained batch 174 batch loss 1.63064933 epoch total loss 1.73596907\n",
      "Trained batch 175 batch loss 1.62071872 epoch total loss 1.73531055\n",
      "Trained batch 176 batch loss 1.60606611 epoch total loss 1.73457634\n",
      "Trained batch 177 batch loss 1.7121197 epoch total loss 1.73444951\n",
      "Trained batch 178 batch loss 1.70626569 epoch total loss 1.7342912\n",
      "Trained batch 179 batch loss 1.63608503 epoch total loss 1.73374248\n",
      "Trained batch 180 batch loss 1.65037906 epoch total loss 1.73327947\n",
      "Trained batch 181 batch loss 1.67330909 epoch total loss 1.73294806\n",
      "Trained batch 182 batch loss 1.65247178 epoch total loss 1.73250592\n",
      "Trained batch 183 batch loss 1.63524413 epoch total loss 1.73197448\n",
      "Trained batch 184 batch loss 1.69485235 epoch total loss 1.73177266\n",
      "Trained batch 185 batch loss 1.75986683 epoch total loss 1.73192453\n",
      "Trained batch 186 batch loss 1.73414195 epoch total loss 1.73193634\n",
      "Trained batch 187 batch loss 1.73255777 epoch total loss 1.73193967\n",
      "Trained batch 188 batch loss 1.70596075 epoch total loss 1.73180151\n",
      "Trained batch 189 batch loss 1.68490338 epoch total loss 1.73155332\n",
      "Trained batch 190 batch loss 1.67449522 epoch total loss 1.73125303\n",
      "Trained batch 191 batch loss 1.65764546 epoch total loss 1.73086774\n",
      "Trained batch 192 batch loss 1.72255397 epoch total loss 1.73082447\n",
      "Trained batch 193 batch loss 1.68450284 epoch total loss 1.7305845\n",
      "Trained batch 194 batch loss 1.58814144 epoch total loss 1.72985017\n",
      "Trained batch 195 batch loss 1.63882136 epoch total loss 1.72938347\n",
      "Trained batch 196 batch loss 1.60396218 epoch total loss 1.72874355\n",
      "Trained batch 197 batch loss 1.63526082 epoch total loss 1.72826898\n",
      "Trained batch 198 batch loss 1.52498055 epoch total loss 1.72724235\n",
      "Trained batch 199 batch loss 1.58536959 epoch total loss 1.72652936\n",
      "Trained batch 200 batch loss 1.59807682 epoch total loss 1.72588718\n",
      "Trained batch 201 batch loss 1.60867286 epoch total loss 1.72530401\n",
      "Trained batch 202 batch loss 1.56165934 epoch total loss 1.72449374\n",
      "Trained batch 203 batch loss 1.62714672 epoch total loss 1.72401416\n",
      "Trained batch 204 batch loss 1.64520788 epoch total loss 1.72362792\n",
      "Trained batch 205 batch loss 1.59973145 epoch total loss 1.72302353\n",
      "Trained batch 206 batch loss 1.60664427 epoch total loss 1.7224586\n",
      "Trained batch 207 batch loss 1.63533497 epoch total loss 1.72203779\n",
      "Trained batch 208 batch loss 1.55137682 epoch total loss 1.72121739\n",
      "Trained batch 209 batch loss 1.63591 epoch total loss 1.7208091\n",
      "Trained batch 210 batch loss 1.64574218 epoch total loss 1.72045171\n",
      "Trained batch 211 batch loss 1.6518178 epoch total loss 1.72012651\n",
      "Trained batch 212 batch loss 1.65348589 epoch total loss 1.71981204\n",
      "Trained batch 213 batch loss 1.6760366 epoch total loss 1.71960652\n",
      "Trained batch 214 batch loss 1.60657835 epoch total loss 1.7190783\n",
      "Trained batch 215 batch loss 1.42436314 epoch total loss 1.71770763\n",
      "Trained batch 216 batch loss 1.68076754 epoch total loss 1.71753645\n",
      "Trained batch 217 batch loss 1.62642229 epoch total loss 1.71711671\n",
      "Trained batch 218 batch loss 1.65822363 epoch total loss 1.71684659\n",
      "Trained batch 219 batch loss 1.53504992 epoch total loss 1.71601653\n",
      "Trained batch 220 batch loss 1.44630659 epoch total loss 1.71479058\n",
      "Trained batch 221 batch loss 1.49989951 epoch total loss 1.71381831\n",
      "Trained batch 222 batch loss 1.45523512 epoch total loss 1.71265352\n",
      "Trained batch 223 batch loss 1.55964637 epoch total loss 1.71196735\n",
      "Trained batch 224 batch loss 1.50155473 epoch total loss 1.71102798\n",
      "Trained batch 225 batch loss 1.54651117 epoch total loss 1.71029675\n",
      "Trained batch 226 batch loss 1.54518008 epoch total loss 1.70956612\n",
      "Trained batch 227 batch loss 1.58972538 epoch total loss 1.70903814\n",
      "Trained batch 228 batch loss 1.5949651 epoch total loss 1.70853782\n",
      "Trained batch 229 batch loss 1.63102436 epoch total loss 1.70819926\n",
      "Trained batch 230 batch loss 1.65960503 epoch total loss 1.70798802\n",
      "Trained batch 231 batch loss 1.61656785 epoch total loss 1.70759237\n",
      "Trained batch 232 batch loss 1.42304718 epoch total loss 1.70636582\n",
      "Trained batch 233 batch loss 1.20572758 epoch total loss 1.70421708\n",
      "Trained batch 234 batch loss 1.38296866 epoch total loss 1.70284426\n",
      "Trained batch 235 batch loss 1.56188393 epoch total loss 1.7022444\n",
      "Trained batch 236 batch loss 1.80288708 epoch total loss 1.70267081\n",
      "Trained batch 237 batch loss 1.67866874 epoch total loss 1.7025696\n",
      "Trained batch 238 batch loss 1.68552446 epoch total loss 1.70249796\n",
      "Trained batch 239 batch loss 1.51210439 epoch total loss 1.7017014\n",
      "Trained batch 240 batch loss 1.42174518 epoch total loss 1.70053494\n",
      "Trained batch 241 batch loss 1.54389572 epoch total loss 1.69988489\n",
      "Trained batch 242 batch loss 1.72567487 epoch total loss 1.69999158\n",
      "Trained batch 243 batch loss 1.66592872 epoch total loss 1.69985127\n",
      "Trained batch 244 batch loss 1.72375143 epoch total loss 1.69994926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 245 batch loss 1.55842018 epoch total loss 1.69937158\n",
      "Trained batch 246 batch loss 1.50383091 epoch total loss 1.69857681\n",
      "Trained batch 247 batch loss 1.57374358 epoch total loss 1.69807136\n",
      "Trained batch 248 batch loss 1.59529209 epoch total loss 1.69765699\n",
      "Trained batch 249 batch loss 1.66137755 epoch total loss 1.6975112\n",
      "Trained batch 250 batch loss 1.65132606 epoch total loss 1.69732654\n",
      "Trained batch 251 batch loss 1.55854201 epoch total loss 1.69677353\n",
      "Trained batch 252 batch loss 1.57918239 epoch total loss 1.69630694\n",
      "Trained batch 253 batch loss 1.62283611 epoch total loss 1.69601655\n",
      "Trained batch 254 batch loss 1.63655233 epoch total loss 1.69578254\n",
      "Trained batch 255 batch loss 1.64171112 epoch total loss 1.69557047\n",
      "Trained batch 256 batch loss 1.62187278 epoch total loss 1.6952827\n",
      "Trained batch 257 batch loss 1.61438084 epoch total loss 1.69496787\n",
      "Trained batch 258 batch loss 1.58133948 epoch total loss 1.69452739\n",
      "Trained batch 259 batch loss 1.54949772 epoch total loss 1.69396746\n",
      "Trained batch 260 batch loss 1.67939878 epoch total loss 1.69391155\n",
      "Trained batch 261 batch loss 1.76724529 epoch total loss 1.69419253\n",
      "Trained batch 262 batch loss 1.72946405 epoch total loss 1.69432712\n",
      "Trained batch 263 batch loss 1.58048689 epoch total loss 1.69389415\n",
      "Trained batch 264 batch loss 1.5765295 epoch total loss 1.69344962\n",
      "Trained batch 265 batch loss 1.57640159 epoch total loss 1.69300807\n",
      "Trained batch 266 batch loss 1.54699278 epoch total loss 1.69245911\n",
      "Trained batch 267 batch loss 1.54658651 epoch total loss 1.69191277\n",
      "Trained batch 268 batch loss 1.6138767 epoch total loss 1.69162166\n",
      "Trained batch 269 batch loss 1.55061805 epoch total loss 1.6910975\n",
      "Trained batch 270 batch loss 1.64501119 epoch total loss 1.69092691\n",
      "Trained batch 271 batch loss 1.66165793 epoch total loss 1.69081891\n",
      "Trained batch 272 batch loss 1.67844701 epoch total loss 1.69077337\n",
      "Trained batch 273 batch loss 1.63075519 epoch total loss 1.69055355\n",
      "Trained batch 274 batch loss 1.63285446 epoch total loss 1.6903429\n",
      "Trained batch 275 batch loss 1.59264779 epoch total loss 1.68998766\n",
      "Trained batch 276 batch loss 1.62713337 epoch total loss 1.68976\n",
      "Trained batch 277 batch loss 1.62236261 epoch total loss 1.68951666\n",
      "Trained batch 278 batch loss 1.60372293 epoch total loss 1.68920815\n",
      "Trained batch 279 batch loss 1.54152369 epoch total loss 1.68867886\n",
      "Trained batch 280 batch loss 1.57067168 epoch total loss 1.68825734\n",
      "Trained batch 281 batch loss 1.56754875 epoch total loss 1.68782771\n",
      "Trained batch 282 batch loss 1.48941147 epoch total loss 1.68712413\n",
      "Trained batch 283 batch loss 1.57835817 epoch total loss 1.68673992\n",
      "Trained batch 284 batch loss 1.58602607 epoch total loss 1.68638527\n",
      "Trained batch 285 batch loss 1.57724881 epoch total loss 1.68600225\n",
      "Trained batch 286 batch loss 1.49613237 epoch total loss 1.68533838\n",
      "Trained batch 287 batch loss 1.59603786 epoch total loss 1.68502724\n",
      "Trained batch 288 batch loss 1.66545665 epoch total loss 1.68495929\n",
      "Trained batch 289 batch loss 1.64017963 epoch total loss 1.68480432\n",
      "Trained batch 290 batch loss 1.64767194 epoch total loss 1.68467629\n",
      "Trained batch 291 batch loss 1.54336917 epoch total loss 1.68419063\n",
      "Trained batch 292 batch loss 1.62935936 epoch total loss 1.68400288\n",
      "Trained batch 293 batch loss 1.614115 epoch total loss 1.68376434\n",
      "Trained batch 294 batch loss 1.53784263 epoch total loss 1.68326807\n",
      "Trained batch 295 batch loss 1.47155523 epoch total loss 1.68255031\n",
      "Trained batch 296 batch loss 1.34388518 epoch total loss 1.68140614\n",
      "Trained batch 297 batch loss 1.60345423 epoch total loss 1.68114376\n",
      "Trained batch 298 batch loss 1.51259279 epoch total loss 1.68057811\n",
      "Trained batch 299 batch loss 1.69921315 epoch total loss 1.68064046\n",
      "Trained batch 300 batch loss 1.57340288 epoch total loss 1.68028295\n",
      "Trained batch 301 batch loss 1.55622149 epoch total loss 1.67987084\n",
      "Trained batch 302 batch loss 1.47697163 epoch total loss 1.67919886\n",
      "Trained batch 303 batch loss 1.56102133 epoch total loss 1.67880893\n",
      "Trained batch 304 batch loss 1.65473986 epoch total loss 1.67872977\n",
      "Trained batch 305 batch loss 1.58437657 epoch total loss 1.67842042\n",
      "Trained batch 306 batch loss 1.5398519 epoch total loss 1.67796755\n",
      "Trained batch 307 batch loss 1.59145522 epoch total loss 1.67768562\n",
      "Trained batch 308 batch loss 1.6191256 epoch total loss 1.6774956\n",
      "Trained batch 309 batch loss 1.55609787 epoch total loss 1.67710268\n",
      "Trained batch 310 batch loss 1.60662377 epoch total loss 1.67687535\n",
      "Trained batch 311 batch loss 1.5635891 epoch total loss 1.67651117\n",
      "Trained batch 312 batch loss 1.62197328 epoch total loss 1.67633629\n",
      "Trained batch 313 batch loss 1.63462424 epoch total loss 1.67620301\n",
      "Trained batch 314 batch loss 1.67864716 epoch total loss 1.67621088\n",
      "Trained batch 315 batch loss 1.65472698 epoch total loss 1.67614257\n",
      "Trained batch 316 batch loss 1.61206174 epoch total loss 1.6759398\n",
      "Trained batch 317 batch loss 1.52547646 epoch total loss 1.67546511\n",
      "Trained batch 318 batch loss 1.52305245 epoch total loss 1.67498589\n",
      "Trained batch 319 batch loss 1.5269444 epoch total loss 1.67452168\n",
      "Trained batch 320 batch loss 1.51785624 epoch total loss 1.67403221\n",
      "Trained batch 321 batch loss 1.47395539 epoch total loss 1.67340887\n",
      "Trained batch 322 batch loss 1.60560846 epoch total loss 1.67319822\n",
      "Trained batch 323 batch loss 1.57481813 epoch total loss 1.67289364\n",
      "Trained batch 324 batch loss 1.62624 epoch total loss 1.67274964\n",
      "Trained batch 325 batch loss 1.52409673 epoch total loss 1.67229235\n",
      "Trained batch 326 batch loss 1.59348345 epoch total loss 1.6720506\n",
      "Trained batch 327 batch loss 1.55246687 epoch total loss 1.67168498\n",
      "Trained batch 328 batch loss 1.62470174 epoch total loss 1.67154169\n",
      "Trained batch 329 batch loss 1.59211504 epoch total loss 1.67130029\n",
      "Trained batch 330 batch loss 1.47188354 epoch total loss 1.6706959\n",
      "Trained batch 331 batch loss 1.55481374 epoch total loss 1.67034578\n",
      "Trained batch 332 batch loss 1.52973175 epoch total loss 1.66992223\n",
      "Trained batch 333 batch loss 1.52363968 epoch total loss 1.66948295\n",
      "Trained batch 334 batch loss 1.58237433 epoch total loss 1.66922212\n",
      "Trained batch 335 batch loss 1.4012326 epoch total loss 1.66842222\n",
      "Trained batch 336 batch loss 1.50259256 epoch total loss 1.66792858\n",
      "Trained batch 337 batch loss 1.64104366 epoch total loss 1.66784883\n",
      "Trained batch 338 batch loss 1.5796597 epoch total loss 1.66758788\n",
      "Trained batch 339 batch loss 1.59068394 epoch total loss 1.66736114\n",
      "Trained batch 340 batch loss 1.63398468 epoch total loss 1.66726291\n",
      "Trained batch 341 batch loss 1.42479384 epoch total loss 1.66655183\n",
      "Trained batch 342 batch loss 1.60976434 epoch total loss 1.66638577\n",
      "Trained batch 343 batch loss 1.54920089 epoch total loss 1.66604412\n",
      "Trained batch 344 batch loss 1.52582574 epoch total loss 1.66563642\n",
      "Trained batch 345 batch loss 1.60319614 epoch total loss 1.66545546\n",
      "Trained batch 346 batch loss 1.54356956 epoch total loss 1.66510332\n",
      "Trained batch 347 batch loss 1.61226594 epoch total loss 1.66495097\n",
      "Trained batch 348 batch loss 1.55224752 epoch total loss 1.66462708\n",
      "Trained batch 349 batch loss 1.60145164 epoch total loss 1.664446\n",
      "Trained batch 350 batch loss 1.54244852 epoch total loss 1.66409743\n",
      "Trained batch 351 batch loss 1.66829777 epoch total loss 1.66410923\n",
      "Trained batch 352 batch loss 1.71155858 epoch total loss 1.66424406\n",
      "Trained batch 353 batch loss 1.66238046 epoch total loss 1.66423869\n",
      "Trained batch 354 batch loss 1.69756222 epoch total loss 1.66433287\n",
      "Trained batch 355 batch loss 1.53686178 epoch total loss 1.66397381\n",
      "Trained batch 356 batch loss 1.50500309 epoch total loss 1.66352725\n",
      "Trained batch 357 batch loss 1.57954168 epoch total loss 1.66329193\n",
      "Trained batch 358 batch loss 1.48897648 epoch total loss 1.66280496\n",
      "Trained batch 359 batch loss 1.53016376 epoch total loss 1.66243541\n",
      "Trained batch 360 batch loss 1.61676908 epoch total loss 1.66230857\n",
      "Trained batch 361 batch loss 1.59932733 epoch total loss 1.66213405\n",
      "Trained batch 362 batch loss 1.61968243 epoch total loss 1.66201687\n",
      "Trained batch 363 batch loss 1.58798611 epoch total loss 1.6618129\n",
      "Trained batch 364 batch loss 1.59184408 epoch total loss 1.66162074\n",
      "Trained batch 365 batch loss 1.61100197 epoch total loss 1.6614821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 366 batch loss 1.62980413 epoch total loss 1.66139567\n",
      "Trained batch 367 batch loss 1.58046305 epoch total loss 1.66117501\n",
      "Trained batch 368 batch loss 1.57876992 epoch total loss 1.66095126\n",
      "Trained batch 369 batch loss 1.55997252 epoch total loss 1.66067767\n",
      "Trained batch 370 batch loss 1.55063367 epoch total loss 1.66038024\n",
      "Trained batch 371 batch loss 1.55076802 epoch total loss 1.66008484\n",
      "Trained batch 372 batch loss 1.49409914 epoch total loss 1.65963864\n",
      "Trained batch 373 batch loss 1.48301661 epoch total loss 1.65916514\n",
      "Trained batch 374 batch loss 1.59072685 epoch total loss 1.65898204\n",
      "Trained batch 375 batch loss 1.62393451 epoch total loss 1.6588887\n",
      "Trained batch 376 batch loss 1.61786878 epoch total loss 1.6587795\n",
      "Trained batch 377 batch loss 1.5086273 epoch total loss 1.65838122\n",
      "Trained batch 378 batch loss 1.56353784 epoch total loss 1.65813029\n",
      "Trained batch 379 batch loss 1.52528453 epoch total loss 1.65777969\n",
      "Trained batch 380 batch loss 1.56542265 epoch total loss 1.65753675\n",
      "Trained batch 381 batch loss 1.50589585 epoch total loss 1.65713882\n",
      "Trained batch 382 batch loss 1.5313437 epoch total loss 1.65680957\n",
      "Trained batch 383 batch loss 1.54315805 epoch total loss 1.65651274\n",
      "Trained batch 384 batch loss 1.53723323 epoch total loss 1.6562022\n",
      "Trained batch 385 batch loss 1.56789374 epoch total loss 1.65597272\n",
      "Trained batch 386 batch loss 1.5508101 epoch total loss 1.65570021\n",
      "Trained batch 387 batch loss 1.58538842 epoch total loss 1.65551853\n",
      "Trained batch 388 batch loss 1.51817966 epoch total loss 1.6551646\n",
      "Trained batch 389 batch loss 1.58691335 epoch total loss 1.65498912\n",
      "Trained batch 390 batch loss 1.50904775 epoch total loss 1.65461493\n",
      "Trained batch 391 batch loss 1.53724897 epoch total loss 1.65431464\n",
      "Trained batch 392 batch loss 1.51010776 epoch total loss 1.65394688\n",
      "Trained batch 393 batch loss 1.40557551 epoch total loss 1.65331483\n",
      "Trained batch 394 batch loss 1.32010531 epoch total loss 1.65246928\n",
      "Trained batch 395 batch loss 1.37589955 epoch total loss 1.65176904\n",
      "Trained batch 396 batch loss 1.41363335 epoch total loss 1.65116775\n",
      "Trained batch 397 batch loss 1.5124619 epoch total loss 1.65081835\n",
      "Trained batch 398 batch loss 1.68835354 epoch total loss 1.65091264\n",
      "Trained batch 399 batch loss 1.69828594 epoch total loss 1.65103137\n",
      "Trained batch 400 batch loss 1.68662345 epoch total loss 1.65112042\n",
      "Trained batch 401 batch loss 1.61920357 epoch total loss 1.65104091\n",
      "Trained batch 402 batch loss 1.63853848 epoch total loss 1.6510098\n",
      "Trained batch 403 batch loss 1.67510581 epoch total loss 1.65106964\n",
      "Trained batch 404 batch loss 1.62644839 epoch total loss 1.65100873\n",
      "Trained batch 405 batch loss 1.67251897 epoch total loss 1.65106189\n",
      "Trained batch 406 batch loss 1.67400277 epoch total loss 1.6511184\n",
      "Trained batch 407 batch loss 1.73566246 epoch total loss 1.65132606\n",
      "Trained batch 408 batch loss 1.61606073 epoch total loss 1.65123975\n",
      "Trained batch 409 batch loss 1.63052833 epoch total loss 1.65118921\n",
      "Trained batch 410 batch loss 1.51062691 epoch total loss 1.65084636\n",
      "Trained batch 411 batch loss 1.4874208 epoch total loss 1.65044868\n",
      "Trained batch 412 batch loss 1.48538876 epoch total loss 1.65004814\n",
      "Trained batch 413 batch loss 1.46098304 epoch total loss 1.64959037\n",
      "Trained batch 414 batch loss 1.47345567 epoch total loss 1.64916492\n",
      "Trained batch 415 batch loss 1.65763044 epoch total loss 1.64918542\n",
      "Trained batch 416 batch loss 1.62143254 epoch total loss 1.64911866\n",
      "Trained batch 417 batch loss 1.40213037 epoch total loss 1.64852655\n",
      "Trained batch 418 batch loss 1.39967132 epoch total loss 1.6479311\n",
      "Trained batch 419 batch loss 1.39273584 epoch total loss 1.64732206\n",
      "Trained batch 420 batch loss 1.40976977 epoch total loss 1.64675653\n",
      "Trained batch 421 batch loss 1.50398755 epoch total loss 1.64641738\n",
      "Trained batch 422 batch loss 1.59505463 epoch total loss 1.64629567\n",
      "Trained batch 423 batch loss 1.60759139 epoch total loss 1.64620411\n",
      "Trained batch 424 batch loss 1.50015616 epoch total loss 1.64585972\n",
      "Trained batch 425 batch loss 1.49856591 epoch total loss 1.6455133\n",
      "Trained batch 426 batch loss 1.46342432 epoch total loss 1.64508581\n",
      "Trained batch 427 batch loss 1.6572125 epoch total loss 1.6451143\n",
      "Trained batch 428 batch loss 1.5745703 epoch total loss 1.64494956\n",
      "Trained batch 429 batch loss 1.5932765 epoch total loss 1.64482903\n",
      "Trained batch 430 batch loss 1.5135237 epoch total loss 1.64452374\n",
      "Trained batch 431 batch loss 1.55417073 epoch total loss 1.64431417\n",
      "Trained batch 432 batch loss 1.4690783 epoch total loss 1.6439085\n",
      "Trained batch 433 batch loss 1.5312413 epoch total loss 1.64364827\n",
      "Trained batch 434 batch loss 1.40052009 epoch total loss 1.6430881\n",
      "Trained batch 435 batch loss 1.30454624 epoch total loss 1.6423099\n",
      "Trained batch 436 batch loss 1.29618096 epoch total loss 1.64151597\n",
      "Trained batch 437 batch loss 1.43761051 epoch total loss 1.6410495\n",
      "Trained batch 438 batch loss 1.60268819 epoch total loss 1.64096177\n",
      "Trained batch 439 batch loss 1.83007085 epoch total loss 1.64139259\n",
      "Trained batch 440 batch loss 1.58080101 epoch total loss 1.6412549\n",
      "Trained batch 441 batch loss 1.52046299 epoch total loss 1.64098096\n",
      "Trained batch 442 batch loss 1.4863745 epoch total loss 1.6406312\n",
      "Trained batch 443 batch loss 1.66462636 epoch total loss 1.64068532\n",
      "Trained batch 444 batch loss 1.59486222 epoch total loss 1.64058208\n",
      "Trained batch 445 batch loss 1.65611398 epoch total loss 1.64061701\n",
      "Trained batch 446 batch loss 1.6220355 epoch total loss 1.64057529\n",
      "Trained batch 447 batch loss 1.60739076 epoch total loss 1.64050102\n",
      "Trained batch 448 batch loss 1.58399248 epoch total loss 1.6403749\n",
      "Trained batch 449 batch loss 1.62791443 epoch total loss 1.64034712\n",
      "Trained batch 450 batch loss 1.5555315 epoch total loss 1.64015865\n",
      "Trained batch 451 batch loss 1.50757313 epoch total loss 1.63986468\n",
      "Trained batch 452 batch loss 1.56798458 epoch total loss 1.63970566\n",
      "Trained batch 453 batch loss 1.6292603 epoch total loss 1.63968265\n",
      "Trained batch 454 batch loss 1.59325933 epoch total loss 1.63958037\n",
      "Trained batch 455 batch loss 1.41985416 epoch total loss 1.63909745\n",
      "Trained batch 456 batch loss 1.49833226 epoch total loss 1.63878882\n",
      "Trained batch 457 batch loss 1.57690847 epoch total loss 1.6386534\n",
      "Trained batch 458 batch loss 1.50366426 epoch total loss 1.63835871\n",
      "Trained batch 459 batch loss 1.51611042 epoch total loss 1.6380924\n",
      "Trained batch 460 batch loss 1.58151627 epoch total loss 1.63796949\n",
      "Trained batch 461 batch loss 1.48196602 epoch total loss 1.63763106\n",
      "Trained batch 462 batch loss 1.56628764 epoch total loss 1.63747668\n",
      "Trained batch 463 batch loss 1.54221678 epoch total loss 1.63727093\n",
      "Trained batch 464 batch loss 1.59191263 epoch total loss 1.63717318\n",
      "Trained batch 465 batch loss 1.62592816 epoch total loss 1.63714898\n",
      "Trained batch 466 batch loss 1.65660131 epoch total loss 1.63719082\n",
      "Trained batch 467 batch loss 1.6497004 epoch total loss 1.63721764\n",
      "Trained batch 468 batch loss 1.62042046 epoch total loss 1.63718176\n",
      "Trained batch 469 batch loss 1.6333375 epoch total loss 1.63717353\n",
      "Trained batch 470 batch loss 1.68649721 epoch total loss 1.63727856\n",
      "Trained batch 471 batch loss 1.66983664 epoch total loss 1.63734782\n",
      "Trained batch 472 batch loss 1.59731638 epoch total loss 1.63726294\n",
      "Trained batch 473 batch loss 1.59540308 epoch total loss 1.63717437\n",
      "Trained batch 474 batch loss 1.6680758 epoch total loss 1.63723958\n",
      "Trained batch 475 batch loss 1.6718905 epoch total loss 1.63731253\n",
      "Trained batch 476 batch loss 1.59921813 epoch total loss 1.63723254\n",
      "Trained batch 477 batch loss 1.50253487 epoch total loss 1.63695025\n",
      "Trained batch 478 batch loss 1.39141369 epoch total loss 1.63643658\n",
      "Trained batch 479 batch loss 1.40281236 epoch total loss 1.6359489\n",
      "Trained batch 480 batch loss 1.55721211 epoch total loss 1.63578475\n",
      "Trained batch 481 batch loss 1.56480694 epoch total loss 1.63563728\n",
      "Trained batch 482 batch loss 1.65101433 epoch total loss 1.63566911\n",
      "Trained batch 483 batch loss 1.57148266 epoch total loss 1.63553619\n",
      "Trained batch 484 batch loss 1.66288686 epoch total loss 1.6355927\n",
      "Trained batch 485 batch loss 1.6369555 epoch total loss 1.63559556\n",
      "Trained batch 486 batch loss 1.5365659 epoch total loss 1.63539183\n",
      "Trained batch 487 batch loss 1.66672361 epoch total loss 1.6354562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 488 batch loss 1.67534268 epoch total loss 1.63553798\n",
      "Trained batch 489 batch loss 1.62306178 epoch total loss 1.63551235\n",
      "Trained batch 490 batch loss 1.56913531 epoch total loss 1.63537693\n",
      "Trained batch 491 batch loss 1.52613294 epoch total loss 1.63515449\n",
      "Trained batch 492 batch loss 1.40734196 epoch total loss 1.63469148\n",
      "Trained batch 493 batch loss 1.41171956 epoch total loss 1.6342392\n",
      "Trained batch 494 batch loss 1.58843541 epoch total loss 1.63414645\n",
      "Trained batch 495 batch loss 1.60576987 epoch total loss 1.63408923\n",
      "Trained batch 496 batch loss 1.50636077 epoch total loss 1.63383162\n",
      "Trained batch 497 batch loss 1.68906808 epoch total loss 1.63394284\n",
      "Trained batch 498 batch loss 1.50738871 epoch total loss 1.63368869\n",
      "Trained batch 499 batch loss 1.55253291 epoch total loss 1.63352609\n",
      "Trained batch 500 batch loss 1.55645537 epoch total loss 1.63337195\n",
      "Trained batch 501 batch loss 1.53485918 epoch total loss 1.63317525\n",
      "Trained batch 502 batch loss 1.55085015 epoch total loss 1.63301134\n",
      "Trained batch 503 batch loss 1.5121696 epoch total loss 1.63277102\n",
      "Trained batch 504 batch loss 1.51172304 epoch total loss 1.63253081\n",
      "Trained batch 505 batch loss 1.54643679 epoch total loss 1.63236034\n",
      "Trained batch 506 batch loss 1.50672054 epoch total loss 1.63211203\n",
      "Trained batch 507 batch loss 1.49496412 epoch total loss 1.63184142\n",
      "Trained batch 508 batch loss 1.55368125 epoch total loss 1.63168764\n",
      "Trained batch 509 batch loss 1.49642837 epoch total loss 1.63142192\n",
      "Trained batch 510 batch loss 1.55157804 epoch total loss 1.63126528\n",
      "Trained batch 511 batch loss 1.61092699 epoch total loss 1.63122547\n",
      "Trained batch 512 batch loss 1.61952615 epoch total loss 1.63120258\n",
      "Trained batch 513 batch loss 1.55978942 epoch total loss 1.63106346\n",
      "Trained batch 514 batch loss 1.63043225 epoch total loss 1.63106215\n",
      "Trained batch 515 batch loss 1.53219926 epoch total loss 1.63087022\n",
      "Trained batch 516 batch loss 1.63464451 epoch total loss 1.63087761\n",
      "Trained batch 517 batch loss 1.51489794 epoch total loss 1.63065326\n",
      "Trained batch 518 batch loss 1.5144223 epoch total loss 1.63042879\n",
      "Trained batch 519 batch loss 1.43794775 epoch total loss 1.63005793\n",
      "Trained batch 520 batch loss 1.48338509 epoch total loss 1.62977588\n",
      "Trained batch 521 batch loss 1.51775801 epoch total loss 1.62956083\n",
      "Trained batch 522 batch loss 1.53340721 epoch total loss 1.62937665\n",
      "Trained batch 523 batch loss 1.61765599 epoch total loss 1.62935424\n",
      "Trained batch 524 batch loss 1.5149579 epoch total loss 1.62913597\n",
      "Trained batch 525 batch loss 1.55555415 epoch total loss 1.62899578\n",
      "Trained batch 526 batch loss 1.54763806 epoch total loss 1.62884116\n",
      "Trained batch 527 batch loss 1.52156067 epoch total loss 1.62863755\n",
      "Trained batch 528 batch loss 1.51423168 epoch total loss 1.62842083\n",
      "Trained batch 529 batch loss 1.54949236 epoch total loss 1.6282717\n",
      "Trained batch 530 batch loss 1.6276685 epoch total loss 1.62827051\n",
      "Trained batch 531 batch loss 1.57137871 epoch total loss 1.62816334\n",
      "Trained batch 532 batch loss 1.45935893 epoch total loss 1.627846\n",
      "Trained batch 533 batch loss 1.42340171 epoch total loss 1.62746251\n",
      "Trained batch 534 batch loss 1.54727077 epoch total loss 1.6273123\n",
      "Trained batch 535 batch loss 1.5631305 epoch total loss 1.62719226\n",
      "Trained batch 536 batch loss 1.59820437 epoch total loss 1.62713814\n",
      "Trained batch 537 batch loss 1.57962489 epoch total loss 1.62704968\n",
      "Trained batch 538 batch loss 1.60865092 epoch total loss 1.62701547\n",
      "Trained batch 539 batch loss 1.61648643 epoch total loss 1.62699604\n",
      "Trained batch 540 batch loss 1.60600591 epoch total loss 1.62695718\n",
      "Trained batch 541 batch loss 1.56868267 epoch total loss 1.62684941\n",
      "Trained batch 542 batch loss 1.46822739 epoch total loss 1.62655675\n",
      "Trained batch 543 batch loss 1.4302913 epoch total loss 1.62619531\n",
      "Trained batch 544 batch loss 1.55841303 epoch total loss 1.62607074\n",
      "Trained batch 545 batch loss 1.52559614 epoch total loss 1.62588632\n",
      "Trained batch 546 batch loss 1.59588349 epoch total loss 1.62583137\n",
      "Trained batch 547 batch loss 1.60189509 epoch total loss 1.6257875\n",
      "Trained batch 548 batch loss 1.54355907 epoch total loss 1.62563753\n",
      "Trained batch 549 batch loss 1.52941537 epoch total loss 1.62546229\n",
      "Trained batch 550 batch loss 1.52644753 epoch total loss 1.62528217\n",
      "Trained batch 551 batch loss 1.64609051 epoch total loss 1.62532008\n",
      "Trained batch 552 batch loss 1.52702427 epoch total loss 1.62514198\n",
      "Trained batch 553 batch loss 1.53903604 epoch total loss 1.62498629\n",
      "Trained batch 554 batch loss 1.54431009 epoch total loss 1.62484074\n",
      "Trained batch 555 batch loss 1.51385415 epoch total loss 1.6246407\n",
      "Trained batch 556 batch loss 1.41981339 epoch total loss 1.62427235\n",
      "Trained batch 557 batch loss 1.45655751 epoch total loss 1.62397122\n",
      "Trained batch 558 batch loss 1.36954176 epoch total loss 1.62351525\n",
      "Trained batch 559 batch loss 1.40434456 epoch total loss 1.62312317\n",
      "Trained batch 560 batch loss 1.52035284 epoch total loss 1.62293959\n",
      "Trained batch 561 batch loss 1.55827689 epoch total loss 1.62282443\n",
      "Trained batch 562 batch loss 1.66347218 epoch total loss 1.62289667\n",
      "Trained batch 563 batch loss 1.6349349 epoch total loss 1.62291813\n",
      "Trained batch 564 batch loss 1.63938189 epoch total loss 1.62294734\n",
      "Trained batch 565 batch loss 1.57928669 epoch total loss 1.62287009\n",
      "Trained batch 566 batch loss 1.56835938 epoch total loss 1.62277377\n",
      "Trained batch 567 batch loss 1.57556832 epoch total loss 1.62269044\n",
      "Trained batch 568 batch loss 1.57907701 epoch total loss 1.62261367\n",
      "Trained batch 569 batch loss 1.64731503 epoch total loss 1.62265718\n",
      "Trained batch 570 batch loss 1.58026063 epoch total loss 1.62258279\n",
      "Trained batch 571 batch loss 1.43632364 epoch total loss 1.62225664\n",
      "Trained batch 572 batch loss 1.53440726 epoch total loss 1.6221031\n",
      "Trained batch 573 batch loss 1.6194129 epoch total loss 1.62209833\n",
      "Trained batch 574 batch loss 1.63716137 epoch total loss 1.62212455\n",
      "Trained batch 575 batch loss 1.52107286 epoch total loss 1.62194872\n",
      "Trained batch 576 batch loss 1.43787789 epoch total loss 1.62162924\n",
      "Trained batch 577 batch loss 1.46393919 epoch total loss 1.62135589\n",
      "Trained batch 578 batch loss 1.45275676 epoch total loss 1.62106419\n",
      "Trained batch 579 batch loss 1.56765485 epoch total loss 1.62097192\n",
      "Trained batch 580 batch loss 1.57245469 epoch total loss 1.62088823\n",
      "Trained batch 581 batch loss 1.42398715 epoch total loss 1.62054932\n",
      "Trained batch 582 batch loss 1.52790582 epoch total loss 1.62039018\n",
      "Trained batch 583 batch loss 1.52351832 epoch total loss 1.620224\n",
      "Trained batch 584 batch loss 1.48381197 epoch total loss 1.61999047\n",
      "Trained batch 585 batch loss 1.43567634 epoch total loss 1.61967528\n",
      "Trained batch 586 batch loss 1.34278321 epoch total loss 1.61920285\n",
      "Trained batch 587 batch loss 1.35155857 epoch total loss 1.61874688\n",
      "Trained batch 588 batch loss 1.42118073 epoch total loss 1.61841094\n",
      "Trained batch 589 batch loss 1.42053378 epoch total loss 1.61807489\n",
      "Trained batch 590 batch loss 1.53709006 epoch total loss 1.61793768\n",
      "Trained batch 591 batch loss 1.52074468 epoch total loss 1.61777329\n",
      "Trained batch 592 batch loss 1.51241231 epoch total loss 1.61759531\n",
      "Trained batch 593 batch loss 1.4995091 epoch total loss 1.61739612\n",
      "Trained batch 594 batch loss 1.56904984 epoch total loss 1.6173147\n",
      "Trained batch 595 batch loss 1.54630494 epoch total loss 1.61719537\n",
      "Trained batch 596 batch loss 1.56899619 epoch total loss 1.61711442\n",
      "Trained batch 597 batch loss 1.49474597 epoch total loss 1.6169095\n",
      "Trained batch 598 batch loss 1.44342756 epoch total loss 1.61661935\n",
      "Trained batch 599 batch loss 1.48849905 epoch total loss 1.61640561\n",
      "Trained batch 600 batch loss 1.50698066 epoch total loss 1.6162231\n",
      "Trained batch 601 batch loss 1.47428894 epoch total loss 1.61598706\n",
      "Trained batch 602 batch loss 1.61830318 epoch total loss 1.61599088\n",
      "Trained batch 603 batch loss 1.65893936 epoch total loss 1.61606205\n",
      "Trained batch 604 batch loss 1.60814095 epoch total loss 1.61604893\n",
      "Trained batch 605 batch loss 1.42826509 epoch total loss 1.61573863\n",
      "Trained batch 606 batch loss 1.38897014 epoch total loss 1.61536443\n",
      "Trained batch 607 batch loss 1.37660384 epoch total loss 1.61497104\n",
      "Trained batch 608 batch loss 1.5096544 epoch total loss 1.61479783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 609 batch loss 1.53318834 epoch total loss 1.61466384\n",
      "Trained batch 610 batch loss 1.42294073 epoch total loss 1.61434948\n",
      "Trained batch 611 batch loss 1.43708992 epoch total loss 1.61405933\n",
      "Trained batch 612 batch loss 1.42048407 epoch total loss 1.61374295\n",
      "Trained batch 613 batch loss 1.52679992 epoch total loss 1.61360121\n",
      "Trained batch 614 batch loss 1.68890619 epoch total loss 1.61372375\n",
      "Trained batch 615 batch loss 1.65042162 epoch total loss 1.61378348\n",
      "Trained batch 616 batch loss 1.57776 epoch total loss 1.61372507\n",
      "Trained batch 617 batch loss 1.54532123 epoch total loss 1.6136142\n",
      "Trained batch 618 batch loss 1.61007631 epoch total loss 1.61360848\n",
      "Trained batch 619 batch loss 1.51851559 epoch total loss 1.61345482\n",
      "Trained batch 620 batch loss 1.52886939 epoch total loss 1.61331832\n",
      "Trained batch 621 batch loss 1.51828778 epoch total loss 1.61316538\n",
      "Trained batch 622 batch loss 1.54700601 epoch total loss 1.61305904\n",
      "Trained batch 623 batch loss 1.50560606 epoch total loss 1.61288655\n",
      "Trained batch 624 batch loss 1.56709051 epoch total loss 1.61281312\n",
      "Trained batch 625 batch loss 1.55339599 epoch total loss 1.61271811\n",
      "Trained batch 626 batch loss 1.54265523 epoch total loss 1.61260617\n",
      "Trained batch 627 batch loss 1.49737406 epoch total loss 1.61242235\n",
      "Trained batch 628 batch loss 1.48036921 epoch total loss 1.61221206\n",
      "Trained batch 629 batch loss 1.57793009 epoch total loss 1.61215758\n",
      "Trained batch 630 batch loss 1.62603068 epoch total loss 1.61217964\n",
      "Trained batch 631 batch loss 1.46711731 epoch total loss 1.61194968\n",
      "Trained batch 632 batch loss 1.3963958 epoch total loss 1.61160862\n",
      "Trained batch 633 batch loss 1.32219779 epoch total loss 1.61115146\n",
      "Trained batch 634 batch loss 1.52949548 epoch total loss 1.61102271\n",
      "Trained batch 635 batch loss 1.58724546 epoch total loss 1.61098516\n",
      "Trained batch 636 batch loss 1.51013577 epoch total loss 1.61082661\n",
      "Trained batch 637 batch loss 1.52287889 epoch total loss 1.61068845\n",
      "Trained batch 638 batch loss 1.48580766 epoch total loss 1.61049283\n",
      "Trained batch 639 batch loss 1.60712862 epoch total loss 1.61048758\n",
      "Trained batch 640 batch loss 1.53353477 epoch total loss 1.61036742\n",
      "Trained batch 641 batch loss 1.39000809 epoch total loss 1.61002362\n",
      "Trained batch 642 batch loss 1.44345284 epoch total loss 1.60976422\n",
      "Trained batch 643 batch loss 1.47213399 epoch total loss 1.60955024\n",
      "Trained batch 644 batch loss 1.51412356 epoch total loss 1.60940206\n",
      "Trained batch 645 batch loss 1.48589945 epoch total loss 1.60921049\n",
      "Trained batch 646 batch loss 1.52623153 epoch total loss 1.6090821\n",
      "Trained batch 647 batch loss 1.54559064 epoch total loss 1.60898387\n",
      "Trained batch 648 batch loss 1.5212301 epoch total loss 1.60884845\n",
      "Trained batch 649 batch loss 1.50806534 epoch total loss 1.60869312\n",
      "Trained batch 650 batch loss 1.57164168 epoch total loss 1.60863614\n",
      "Trained batch 651 batch loss 1.50962567 epoch total loss 1.60848415\n",
      "Trained batch 652 batch loss 1.47809124 epoch total loss 1.60828424\n",
      "Trained batch 653 batch loss 1.67475724 epoch total loss 1.60838616\n",
      "Trained batch 654 batch loss 1.54322338 epoch total loss 1.6082865\n",
      "Trained batch 655 batch loss 1.5477792 epoch total loss 1.60819399\n",
      "Trained batch 656 batch loss 1.49309289 epoch total loss 1.6080184\n",
      "Trained batch 657 batch loss 1.590608 epoch total loss 1.60799193\n",
      "Trained batch 658 batch loss 1.50252056 epoch total loss 1.60783172\n",
      "Trained batch 659 batch loss 1.52686918 epoch total loss 1.60770881\n",
      "Trained batch 660 batch loss 1.54668176 epoch total loss 1.60761631\n",
      "Trained batch 661 batch loss 1.494627 epoch total loss 1.60744536\n",
      "Trained batch 662 batch loss 1.38009715 epoch total loss 1.60710192\n",
      "Trained batch 663 batch loss 1.57636559 epoch total loss 1.60705566\n",
      "Trained batch 664 batch loss 1.51422727 epoch total loss 1.60691595\n",
      "Trained batch 665 batch loss 1.55229795 epoch total loss 1.6068337\n",
      "Trained batch 666 batch loss 1.50335824 epoch total loss 1.60667849\n",
      "Trained batch 667 batch loss 1.47295272 epoch total loss 1.60647786\n",
      "Trained batch 668 batch loss 1.52630985 epoch total loss 1.60635793\n",
      "Trained batch 669 batch loss 1.55778074 epoch total loss 1.60628533\n",
      "Trained batch 670 batch loss 1.59540689 epoch total loss 1.60626912\n",
      "Trained batch 671 batch loss 1.57954144 epoch total loss 1.60622942\n",
      "Trained batch 672 batch loss 1.52731645 epoch total loss 1.606112\n",
      "Trained batch 673 batch loss 1.53025007 epoch total loss 1.60599935\n",
      "Trained batch 674 batch loss 1.5843581 epoch total loss 1.60596716\n",
      "Trained batch 675 batch loss 1.6408093 epoch total loss 1.6060189\n",
      "Trained batch 676 batch loss 1.55252731 epoch total loss 1.60593975\n",
      "Trained batch 677 batch loss 1.58445561 epoch total loss 1.60590804\n",
      "Trained batch 678 batch loss 1.66688371 epoch total loss 1.60599792\n",
      "Trained batch 679 batch loss 1.62304759 epoch total loss 1.60602295\n",
      "Trained batch 680 batch loss 1.47761202 epoch total loss 1.60583425\n",
      "Trained batch 681 batch loss 1.55588436 epoch total loss 1.60576093\n",
      "Trained batch 682 batch loss 1.5566113 epoch total loss 1.60568893\n",
      "Trained batch 683 batch loss 1.66192198 epoch total loss 1.60577118\n",
      "Trained batch 684 batch loss 1.60228658 epoch total loss 1.60576606\n",
      "Trained batch 685 batch loss 1.50836515 epoch total loss 1.60562396\n",
      "Trained batch 686 batch loss 1.57530439 epoch total loss 1.60557973\n",
      "Trained batch 687 batch loss 1.65985477 epoch total loss 1.60565889\n",
      "Trained batch 688 batch loss 1.51557279 epoch total loss 1.605528\n",
      "Trained batch 689 batch loss 1.60443938 epoch total loss 1.60552657\n",
      "Trained batch 690 batch loss 1.57401133 epoch total loss 1.60548079\n",
      "Trained batch 691 batch loss 1.55521858 epoch total loss 1.60540795\n",
      "Trained batch 692 batch loss 1.38808751 epoch total loss 1.60509384\n",
      "Trained batch 693 batch loss 1.51322734 epoch total loss 1.60496128\n",
      "Trained batch 694 batch loss 1.39193892 epoch total loss 1.60465431\n",
      "Trained batch 695 batch loss 1.39424467 epoch total loss 1.60435164\n",
      "Trained batch 696 batch loss 1.48587847 epoch total loss 1.60418141\n",
      "Trained batch 697 batch loss 1.54311347 epoch total loss 1.60409379\n",
      "Trained batch 698 batch loss 1.51348019 epoch total loss 1.60396385\n",
      "Trained batch 699 batch loss 1.52404308 epoch total loss 1.60384953\n",
      "Trained batch 700 batch loss 1.49613547 epoch total loss 1.60369563\n",
      "Trained batch 701 batch loss 1.45806396 epoch total loss 1.60348773\n",
      "Trained batch 702 batch loss 1.51637745 epoch total loss 1.60336363\n",
      "Trained batch 703 batch loss 1.48192191 epoch total loss 1.6031909\n",
      "Trained batch 704 batch loss 1.4462626 epoch total loss 1.6029681\n",
      "Trained batch 705 batch loss 1.42192388 epoch total loss 1.6027112\n",
      "Trained batch 706 batch loss 1.42382026 epoch total loss 1.60245776\n",
      "Trained batch 707 batch loss 1.41174483 epoch total loss 1.60218799\n",
      "Trained batch 708 batch loss 1.38346422 epoch total loss 1.601879\n",
      "Trained batch 709 batch loss 1.4983331 epoch total loss 1.60173297\n",
      "Trained batch 710 batch loss 1.41985857 epoch total loss 1.60147667\n",
      "Trained batch 711 batch loss 1.47659492 epoch total loss 1.60130107\n",
      "Trained batch 712 batch loss 1.50430787 epoch total loss 1.6011647\n",
      "Trained batch 713 batch loss 1.52080321 epoch total loss 1.60105193\n",
      "Trained batch 714 batch loss 1.53557491 epoch total loss 1.60096014\n",
      "Trained batch 715 batch loss 1.55578029 epoch total loss 1.60089695\n",
      "Trained batch 716 batch loss 1.44145608 epoch total loss 1.60067427\n",
      "Trained batch 717 batch loss 1.48550737 epoch total loss 1.60051358\n",
      "Trained batch 718 batch loss 1.42202473 epoch total loss 1.60026491\n",
      "Trained batch 719 batch loss 1.49209285 epoch total loss 1.60011446\n",
      "Trained batch 720 batch loss 1.43048954 epoch total loss 1.59987891\n",
      "Trained batch 721 batch loss 1.52711809 epoch total loss 1.59977806\n",
      "Trained batch 722 batch loss 1.34747 epoch total loss 1.59942842\n",
      "Trained batch 723 batch loss 1.5001651 epoch total loss 1.59929109\n",
      "Trained batch 724 batch loss 1.44535756 epoch total loss 1.59907842\n",
      "Trained batch 725 batch loss 1.48263443 epoch total loss 1.59891784\n",
      "Trained batch 726 batch loss 1.56495535 epoch total loss 1.59887111\n",
      "Trained batch 727 batch loss 1.5144248 epoch total loss 1.59875488\n",
      "Trained batch 728 batch loss 1.35892332 epoch total loss 1.59842539\n",
      "Trained batch 729 batch loss 1.52376115 epoch total loss 1.59832299\n",
      "Trained batch 730 batch loss 1.4935472 epoch total loss 1.59817946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 731 batch loss 1.4984231 epoch total loss 1.59804296\n",
      "Trained batch 732 batch loss 1.5265007 epoch total loss 1.59794521\n",
      "Trained batch 733 batch loss 1.48477399 epoch total loss 1.59779084\n",
      "Trained batch 734 batch loss 1.48349309 epoch total loss 1.59763515\n",
      "Trained batch 735 batch loss 1.43139672 epoch total loss 1.59740901\n",
      "Trained batch 736 batch loss 1.51168799 epoch total loss 1.59729254\n",
      "Trained batch 737 batch loss 1.40536606 epoch total loss 1.59703219\n",
      "Trained batch 738 batch loss 1.42530501 epoch total loss 1.59679949\n",
      "Trained batch 739 batch loss 1.44123316 epoch total loss 1.59658897\n",
      "Trained batch 740 batch loss 1.38482738 epoch total loss 1.59630287\n",
      "Trained batch 741 batch loss 1.52872467 epoch total loss 1.59621167\n",
      "Trained batch 742 batch loss 1.46586943 epoch total loss 1.59603596\n",
      "Trained batch 743 batch loss 1.3854115 epoch total loss 1.59575236\n",
      "Trained batch 744 batch loss 1.53469133 epoch total loss 1.59567034\n",
      "Trained batch 745 batch loss 1.53215599 epoch total loss 1.59558499\n",
      "Trained batch 746 batch loss 1.59306681 epoch total loss 1.59558153\n",
      "Trained batch 747 batch loss 1.54181266 epoch total loss 1.59550965\n",
      "Trained batch 748 batch loss 1.47364116 epoch total loss 1.59534669\n",
      "Trained batch 749 batch loss 1.4670161 epoch total loss 1.59517539\n",
      "Trained batch 750 batch loss 1.44329274 epoch total loss 1.59497285\n",
      "Trained batch 751 batch loss 1.44589448 epoch total loss 1.59477437\n",
      "Trained batch 752 batch loss 1.34639943 epoch total loss 1.59444416\n",
      "Trained batch 753 batch loss 1.42717755 epoch total loss 1.59422195\n",
      "Trained batch 754 batch loss 1.48472166 epoch total loss 1.59407675\n",
      "Trained batch 755 batch loss 1.49732876 epoch total loss 1.5939486\n",
      "Trained batch 756 batch loss 1.52026725 epoch total loss 1.59385109\n",
      "Trained batch 757 batch loss 1.60777593 epoch total loss 1.59386945\n",
      "Trained batch 758 batch loss 1.5020771 epoch total loss 1.59374833\n",
      "Trained batch 759 batch loss 1.46844244 epoch total loss 1.59358323\n",
      "Trained batch 760 batch loss 1.38046014 epoch total loss 1.59330285\n",
      "Trained batch 761 batch loss 1.39651299 epoch total loss 1.59304416\n",
      "Trained batch 762 batch loss 1.52685952 epoch total loss 1.59295738\n",
      "Trained batch 763 batch loss 1.53573298 epoch total loss 1.59288239\n",
      "Trained batch 764 batch loss 1.53640771 epoch total loss 1.59280849\n",
      "Trained batch 765 batch loss 1.64419961 epoch total loss 1.5928756\n",
      "Trained batch 766 batch loss 1.55297673 epoch total loss 1.59282351\n",
      "Trained batch 767 batch loss 1.55467606 epoch total loss 1.5927738\n",
      "Trained batch 768 batch loss 1.58215189 epoch total loss 1.59276\n",
      "Trained batch 769 batch loss 1.59192777 epoch total loss 1.59275877\n",
      "Trained batch 770 batch loss 1.5586642 epoch total loss 1.59271467\n",
      "Trained batch 771 batch loss 1.49663365 epoch total loss 1.59259\n",
      "Trained batch 772 batch loss 1.53493762 epoch total loss 1.59251523\n",
      "Trained batch 773 batch loss 1.61856568 epoch total loss 1.59254885\n",
      "Trained batch 774 batch loss 1.53935444 epoch total loss 1.59248006\n",
      "Trained batch 775 batch loss 1.56246376 epoch total loss 1.59244144\n",
      "Trained batch 776 batch loss 1.48188925 epoch total loss 1.59229898\n",
      "Trained batch 777 batch loss 1.45598269 epoch total loss 1.59212351\n",
      "Trained batch 778 batch loss 1.4655354 epoch total loss 1.59196079\n",
      "Trained batch 779 batch loss 1.5081141 epoch total loss 1.59185314\n",
      "Trained batch 780 batch loss 1.51063538 epoch total loss 1.59174895\n",
      "Trained batch 781 batch loss 1.48390269 epoch total loss 1.59161091\n",
      "Trained batch 782 batch loss 1.46029198 epoch total loss 1.59144294\n",
      "Trained batch 783 batch loss 1.49456525 epoch total loss 1.5913192\n",
      "Trained batch 784 batch loss 1.48795867 epoch total loss 1.59118736\n",
      "Trained batch 785 batch loss 1.44622707 epoch total loss 1.59100258\n",
      "Trained batch 786 batch loss 1.44304132 epoch total loss 1.59081423\n",
      "Trained batch 787 batch loss 1.57349253 epoch total loss 1.59079218\n",
      "Trained batch 788 batch loss 1.59977126 epoch total loss 1.59080362\n",
      "Trained batch 789 batch loss 1.55555987 epoch total loss 1.59075892\n",
      "Trained batch 790 batch loss 1.52880585 epoch total loss 1.59068048\n",
      "Trained batch 791 batch loss 1.49048603 epoch total loss 1.59055376\n",
      "Trained batch 792 batch loss 1.42933369 epoch total loss 1.59035027\n",
      "Trained batch 793 batch loss 1.43506849 epoch total loss 1.59015441\n",
      "Trained batch 794 batch loss 1.32622123 epoch total loss 1.58982193\n",
      "Trained batch 795 batch loss 1.45886326 epoch total loss 1.58965719\n",
      "Trained batch 796 batch loss 1.50087786 epoch total loss 1.58954561\n",
      "Trained batch 797 batch loss 1.40542197 epoch total loss 1.58931458\n",
      "Trained batch 798 batch loss 1.43342841 epoch total loss 1.58911932\n",
      "Trained batch 799 batch loss 1.43288159 epoch total loss 1.58892369\n",
      "Trained batch 800 batch loss 1.4325937 epoch total loss 1.58872831\n",
      "Trained batch 801 batch loss 1.52950811 epoch total loss 1.5886544\n",
      "Trained batch 802 batch loss 1.30355573 epoch total loss 1.58829904\n",
      "Trained batch 803 batch loss 1.46768308 epoch total loss 1.58814871\n",
      "Trained batch 804 batch loss 1.3835969 epoch total loss 1.58789432\n",
      "Trained batch 805 batch loss 1.52644348 epoch total loss 1.58781803\n",
      "Trained batch 806 batch loss 1.46741605 epoch total loss 1.58766854\n",
      "Trained batch 807 batch loss 1.45166302 epoch total loss 1.5875001\n",
      "Trained batch 808 batch loss 1.5174458 epoch total loss 1.58741343\n",
      "Trained batch 809 batch loss 1.43712044 epoch total loss 1.58722758\n",
      "Trained batch 810 batch loss 1.61308813 epoch total loss 1.58725953\n",
      "Trained batch 811 batch loss 1.47220731 epoch total loss 1.58711755\n",
      "Trained batch 812 batch loss 1.38607168 epoch total loss 1.58687007\n",
      "Trained batch 813 batch loss 1.26006532 epoch total loss 1.58646798\n",
      "Trained batch 814 batch loss 1.35677791 epoch total loss 1.58618581\n",
      "Trained batch 815 batch loss 1.44042575 epoch total loss 1.586007\n",
      "Trained batch 816 batch loss 1.46084642 epoch total loss 1.58585358\n",
      "Trained batch 817 batch loss 1.39169216 epoch total loss 1.58561599\n",
      "Trained batch 818 batch loss 1.52341151 epoch total loss 1.58553994\n",
      "Trained batch 819 batch loss 1.49250519 epoch total loss 1.58542645\n",
      "Trained batch 820 batch loss 1.54981947 epoch total loss 1.58538294\n",
      "Trained batch 821 batch loss 1.54973245 epoch total loss 1.58533955\n",
      "Trained batch 822 batch loss 1.57269049 epoch total loss 1.58532405\n",
      "Trained batch 823 batch loss 1.61803269 epoch total loss 1.58536375\n",
      "Trained batch 824 batch loss 1.69012845 epoch total loss 1.58549094\n",
      "Trained batch 825 batch loss 1.55047452 epoch total loss 1.5854485\n",
      "Trained batch 826 batch loss 1.51691747 epoch total loss 1.58536553\n",
      "Trained batch 827 batch loss 1.49528062 epoch total loss 1.58525658\n",
      "Trained batch 828 batch loss 1.4990797 epoch total loss 1.58515239\n",
      "Trained batch 829 batch loss 1.4712081 epoch total loss 1.58501494\n",
      "Trained batch 830 batch loss 1.49453175 epoch total loss 1.58490598\n",
      "Trained batch 831 batch loss 1.48582494 epoch total loss 1.58478677\n",
      "Trained batch 832 batch loss 1.28742862 epoch total loss 1.58442938\n",
      "Trained batch 833 batch loss 1.26628876 epoch total loss 1.58404744\n",
      "Trained batch 834 batch loss 1.19132984 epoch total loss 1.58357644\n",
      "Trained batch 835 batch loss 1.39025593 epoch total loss 1.58334494\n",
      "Trained batch 836 batch loss 1.54171395 epoch total loss 1.58329523\n",
      "Trained batch 837 batch loss 1.60921216 epoch total loss 1.58332622\n",
      "Trained batch 838 batch loss 1.55850172 epoch total loss 1.58329654\n",
      "Trained batch 839 batch loss 1.54182887 epoch total loss 1.58324718\n",
      "Trained batch 840 batch loss 1.53352404 epoch total loss 1.58318806\n",
      "Trained batch 841 batch loss 1.38284 epoch total loss 1.58294976\n",
      "Trained batch 842 batch loss 1.31436586 epoch total loss 1.58263075\n",
      "Trained batch 843 batch loss 1.48058462 epoch total loss 1.58250964\n",
      "Trained batch 844 batch loss 1.41808689 epoch total loss 1.58231485\n",
      "Trained batch 845 batch loss 1.43867517 epoch total loss 1.58214498\n",
      "Trained batch 846 batch loss 1.44554651 epoch total loss 1.58198345\n",
      "Trained batch 847 batch loss 1.46758866 epoch total loss 1.58184838\n",
      "Trained batch 848 batch loss 1.54306459 epoch total loss 1.58180261\n",
      "Trained batch 849 batch loss 1.6851114 epoch total loss 1.58192432\n",
      "Trained batch 850 batch loss 1.7186892 epoch total loss 1.58208525\n",
      "Trained batch 851 batch loss 1.69519067 epoch total loss 1.58221817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 852 batch loss 1.72646797 epoch total loss 1.58238745\n",
      "Trained batch 853 batch loss 1.57695663 epoch total loss 1.58238101\n",
      "Trained batch 854 batch loss 1.54372144 epoch total loss 1.58233571\n",
      "Trained batch 855 batch loss 1.44123125 epoch total loss 1.58217072\n",
      "Trained batch 856 batch loss 1.41607463 epoch total loss 1.58197665\n",
      "Trained batch 857 batch loss 1.54015625 epoch total loss 1.5819279\n",
      "Trained batch 858 batch loss 1.61627781 epoch total loss 1.58196795\n",
      "Trained batch 859 batch loss 1.49457586 epoch total loss 1.58186626\n",
      "Trained batch 860 batch loss 1.47537434 epoch total loss 1.58174241\n",
      "Trained batch 861 batch loss 1.42694616 epoch total loss 1.58156264\n",
      "Trained batch 862 batch loss 1.49404764 epoch total loss 1.58146107\n",
      "Trained batch 863 batch loss 1.43015409 epoch total loss 1.58128583\n",
      "Trained batch 864 batch loss 1.48492801 epoch total loss 1.58117437\n",
      "Trained batch 865 batch loss 1.45495915 epoch total loss 1.58102846\n",
      "Trained batch 866 batch loss 1.52975893 epoch total loss 1.58096921\n",
      "Trained batch 867 batch loss 1.43109155 epoch total loss 1.58079648\n",
      "Trained batch 868 batch loss 1.43481255 epoch total loss 1.58062828\n",
      "Trained batch 869 batch loss 1.42657399 epoch total loss 1.58045089\n",
      "Trained batch 870 batch loss 1.61717856 epoch total loss 1.58049321\n",
      "Trained batch 871 batch loss 1.506706 epoch total loss 1.58040845\n",
      "Trained batch 872 batch loss 1.48577869 epoch total loss 1.58029985\n",
      "Trained batch 873 batch loss 1.52247 epoch total loss 1.58023357\n",
      "Trained batch 874 batch loss 1.51011467 epoch total loss 1.58015347\n",
      "Trained batch 875 batch loss 1.52216363 epoch total loss 1.58008718\n",
      "Trained batch 876 batch loss 1.61924708 epoch total loss 1.58013189\n",
      "Trained batch 877 batch loss 1.61631548 epoch total loss 1.58017313\n",
      "Trained batch 878 batch loss 1.56983507 epoch total loss 1.58016145\n",
      "Trained batch 879 batch loss 1.55213571 epoch total loss 1.5801295\n",
      "Trained batch 880 batch loss 1.4306066 epoch total loss 1.57995963\n",
      "Trained batch 881 batch loss 1.45639431 epoch total loss 1.57981944\n",
      "Trained batch 882 batch loss 1.48660183 epoch total loss 1.5797137\n",
      "Trained batch 883 batch loss 1.57873964 epoch total loss 1.57971263\n",
      "Trained batch 884 batch loss 1.63414 epoch total loss 1.57977414\n",
      "Trained batch 885 batch loss 1.60552561 epoch total loss 1.57980323\n",
      "Trained batch 886 batch loss 1.41446638 epoch total loss 1.57961655\n",
      "Trained batch 887 batch loss 1.47590148 epoch total loss 1.57949972\n",
      "Trained batch 888 batch loss 1.40734339 epoch total loss 1.57930589\n",
      "Trained batch 889 batch loss 1.60964048 epoch total loss 1.57934\n",
      "Trained batch 890 batch loss 1.58812428 epoch total loss 1.57934988\n",
      "Trained batch 891 batch loss 1.44537425 epoch total loss 1.57919955\n",
      "Trained batch 892 batch loss 1.5684855 epoch total loss 1.57918751\n",
      "Trained batch 893 batch loss 1.55449688 epoch total loss 1.57915974\n",
      "Trained batch 894 batch loss 1.55096793 epoch total loss 1.57912838\n",
      "Trained batch 895 batch loss 1.50725198 epoch total loss 1.57904792\n",
      "Trained batch 896 batch loss 1.47566676 epoch total loss 1.57893264\n",
      "Trained batch 897 batch loss 1.52726257 epoch total loss 1.57887495\n",
      "Trained batch 898 batch loss 1.55556417 epoch total loss 1.57884896\n",
      "Trained batch 899 batch loss 1.56669044 epoch total loss 1.57883537\n",
      "Trained batch 900 batch loss 1.5422219 epoch total loss 1.57879472\n",
      "Trained batch 901 batch loss 1.47802 epoch total loss 1.5786829\n",
      "Trained batch 902 batch loss 1.56764245 epoch total loss 1.57867062\n",
      "Trained batch 903 batch loss 1.51520872 epoch total loss 1.57860041\n",
      "Trained batch 904 batch loss 1.52824175 epoch total loss 1.57854474\n",
      "Trained batch 905 batch loss 1.53457332 epoch total loss 1.5784961\n",
      "Trained batch 906 batch loss 1.47335315 epoch total loss 1.57838011\n",
      "Trained batch 907 batch loss 1.5110774 epoch total loss 1.57830584\n",
      "Trained batch 908 batch loss 1.44156599 epoch total loss 1.57815528\n",
      "Trained batch 909 batch loss 1.49891019 epoch total loss 1.57806802\n",
      "Trained batch 910 batch loss 1.31038916 epoch total loss 1.57777393\n",
      "Trained batch 911 batch loss 1.53735936 epoch total loss 1.57772958\n",
      "Trained batch 912 batch loss 1.44795609 epoch total loss 1.57758737\n",
      "Trained batch 913 batch loss 1.49000597 epoch total loss 1.5774914\n",
      "Trained batch 914 batch loss 1.5357374 epoch total loss 1.57744575\n",
      "Trained batch 915 batch loss 1.56039095 epoch total loss 1.57742715\n",
      "Trained batch 916 batch loss 1.54217207 epoch total loss 1.57738853\n",
      "Trained batch 917 batch loss 1.53368819 epoch total loss 1.57734096\n",
      "Trained batch 918 batch loss 1.48236918 epoch total loss 1.57723749\n",
      "Trained batch 919 batch loss 1.53836727 epoch total loss 1.57719517\n",
      "Trained batch 920 batch loss 1.4445827 epoch total loss 1.57705104\n",
      "Trained batch 921 batch loss 1.44534922 epoch total loss 1.57690799\n",
      "Trained batch 922 batch loss 1.52219582 epoch total loss 1.57684875\n",
      "Trained batch 923 batch loss 1.57072258 epoch total loss 1.57684195\n",
      "Trained batch 924 batch loss 1.46541572 epoch total loss 1.57672143\n",
      "Trained batch 925 batch loss 1.42227316 epoch total loss 1.57655442\n",
      "Trained batch 926 batch loss 1.41666222 epoch total loss 1.57638168\n",
      "Trained batch 927 batch loss 1.33485794 epoch total loss 1.57612121\n",
      "Trained batch 928 batch loss 1.44553947 epoch total loss 1.57598042\n",
      "Trained batch 929 batch loss 1.41879511 epoch total loss 1.57581127\n",
      "Trained batch 930 batch loss 1.44550157 epoch total loss 1.5756712\n",
      "Trained batch 931 batch loss 1.39885676 epoch total loss 1.5754813\n",
      "Trained batch 932 batch loss 1.38899922 epoch total loss 1.57528126\n",
      "Trained batch 933 batch loss 1.40434444 epoch total loss 1.57509792\n",
      "Trained batch 934 batch loss 1.41673493 epoch total loss 1.5749284\n",
      "Trained batch 935 batch loss 1.4797225 epoch total loss 1.5748266\n",
      "Trained batch 936 batch loss 1.49350238 epoch total loss 1.57473981\n",
      "Trained batch 937 batch loss 1.43805468 epoch total loss 1.5745939\n",
      "Trained batch 938 batch loss 1.4776268 epoch total loss 1.57449067\n",
      "Trained batch 939 batch loss 1.36369634 epoch total loss 1.57426608\n",
      "Trained batch 940 batch loss 1.47978294 epoch total loss 1.57416546\n",
      "Trained batch 941 batch loss 1.4442234 epoch total loss 1.57402742\n",
      "Trained batch 942 batch loss 1.4443332 epoch total loss 1.57388973\n",
      "Trained batch 943 batch loss 1.45185089 epoch total loss 1.57376039\n",
      "Trained batch 944 batch loss 1.43221474 epoch total loss 1.57361042\n",
      "Trained batch 945 batch loss 1.35041809 epoch total loss 1.57337439\n",
      "Trained batch 946 batch loss 1.57623446 epoch total loss 1.57337737\n",
      "Trained batch 947 batch loss 1.60679042 epoch total loss 1.57341278\n",
      "Trained batch 948 batch loss 1.55031896 epoch total loss 1.57338834\n",
      "Trained batch 949 batch loss 1.62756991 epoch total loss 1.57344544\n",
      "Trained batch 950 batch loss 1.77961731 epoch total loss 1.57366252\n",
      "Trained batch 951 batch loss 1.58193946 epoch total loss 1.57367122\n",
      "Trained batch 952 batch loss 1.56160593 epoch total loss 1.57365859\n",
      "Trained batch 953 batch loss 1.55727744 epoch total loss 1.5736413\n",
      "Trained batch 954 batch loss 1.50257969 epoch total loss 1.57356679\n",
      "Trained batch 955 batch loss 1.59914875 epoch total loss 1.57359362\n",
      "Trained batch 956 batch loss 1.53705645 epoch total loss 1.57355535\n",
      "Trained batch 957 batch loss 1.46336067 epoch total loss 1.57344031\n",
      "Trained batch 958 batch loss 1.39917898 epoch total loss 1.5732584\n",
      "Trained batch 959 batch loss 1.50858378 epoch total loss 1.57319093\n",
      "Trained batch 960 batch loss 1.49301279 epoch total loss 1.57310736\n",
      "Trained batch 961 batch loss 1.51570559 epoch total loss 1.57304776\n",
      "Trained batch 962 batch loss 1.42938137 epoch total loss 1.57289827\n",
      "Trained batch 963 batch loss 1.58905721 epoch total loss 1.5729152\n",
      "Trained batch 964 batch loss 1.28077185 epoch total loss 1.57261205\n",
      "Trained batch 965 batch loss 1.52370203 epoch total loss 1.57256138\n",
      "Trained batch 966 batch loss 1.50005114 epoch total loss 1.57248628\n",
      "Trained batch 967 batch loss 1.59597278 epoch total loss 1.57251048\n",
      "Trained batch 968 batch loss 1.59913838 epoch total loss 1.57253802\n",
      "Trained batch 969 batch loss 1.58713365 epoch total loss 1.57255316\n",
      "Trained batch 970 batch loss 1.566715 epoch total loss 1.5725472\n",
      "Trained batch 971 batch loss 1.4509083 epoch total loss 1.57242191\n",
      "Trained batch 972 batch loss 1.35027695 epoch total loss 1.57219326\n",
      "Trained batch 973 batch loss 1.54643345 epoch total loss 1.5721668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 974 batch loss 1.59711885 epoch total loss 1.57219243\n",
      "Trained batch 975 batch loss 1.61375964 epoch total loss 1.57223511\n",
      "Trained batch 976 batch loss 1.51398754 epoch total loss 1.5721755\n",
      "Trained batch 977 batch loss 1.48876393 epoch total loss 1.57209\n",
      "Trained batch 978 batch loss 1.60920644 epoch total loss 1.57212806\n",
      "Trained batch 979 batch loss 1.40085864 epoch total loss 1.57195318\n",
      "Trained batch 980 batch loss 1.3345046 epoch total loss 1.57171082\n",
      "Trained batch 981 batch loss 1.41115391 epoch total loss 1.57154715\n",
      "Trained batch 982 batch loss 1.4341929 epoch total loss 1.57140732\n",
      "Trained batch 983 batch loss 1.34811926 epoch total loss 1.57118011\n",
      "Trained batch 984 batch loss 1.48792815 epoch total loss 1.57109559\n",
      "Trained batch 985 batch loss 1.55420578 epoch total loss 1.57107842\n",
      "Trained batch 986 batch loss 1.51861 epoch total loss 1.57102513\n",
      "Trained batch 987 batch loss 1.50594854 epoch total loss 1.57095921\n",
      "Trained batch 988 batch loss 1.46067417 epoch total loss 1.57084763\n",
      "Trained batch 989 batch loss 1.45892882 epoch total loss 1.5707345\n",
      "Trained batch 990 batch loss 1.43423009 epoch total loss 1.57059658\n",
      "Trained batch 991 batch loss 1.39045501 epoch total loss 1.5704149\n",
      "Trained batch 992 batch loss 1.46389055 epoch total loss 1.57030749\n",
      "Trained batch 993 batch loss 1.47788334 epoch total loss 1.57021439\n",
      "Trained batch 994 batch loss 1.49766028 epoch total loss 1.57014143\n",
      "Trained batch 995 batch loss 1.48155475 epoch total loss 1.57005239\n",
      "Trained batch 996 batch loss 1.46601593 epoch total loss 1.56994796\n",
      "Trained batch 997 batch loss 1.38954639 epoch total loss 1.569767\n",
      "Trained batch 998 batch loss 1.45117784 epoch total loss 1.56964827\n",
      "Trained batch 999 batch loss 1.45833814 epoch total loss 1.56953681\n",
      "Trained batch 1000 batch loss 1.48280525 epoch total loss 1.56945\n",
      "Trained batch 1001 batch loss 1.45073366 epoch total loss 1.56933141\n",
      "Trained batch 1002 batch loss 1.46249914 epoch total loss 1.56922483\n",
      "Trained batch 1003 batch loss 1.47335541 epoch total loss 1.56912923\n",
      "Trained batch 1004 batch loss 1.47031546 epoch total loss 1.56903088\n",
      "Trained batch 1005 batch loss 1.42888761 epoch total loss 1.56889141\n",
      "Trained batch 1006 batch loss 1.42242956 epoch total loss 1.56874585\n",
      "Trained batch 1007 batch loss 1.5153147 epoch total loss 1.56869268\n",
      "Trained batch 1008 batch loss 1.50212097 epoch total loss 1.56862664\n",
      "Trained batch 1009 batch loss 1.59325862 epoch total loss 1.56865108\n",
      "Trained batch 1010 batch loss 1.47922969 epoch total loss 1.56856251\n",
      "Trained batch 1011 batch loss 1.50276852 epoch total loss 1.56849754\n",
      "Trained batch 1012 batch loss 1.59937847 epoch total loss 1.56852806\n",
      "Trained batch 1013 batch loss 1.47957647 epoch total loss 1.5684402\n",
      "Trained batch 1014 batch loss 1.47751892 epoch total loss 1.56835055\n",
      "Trained batch 1015 batch loss 1.4956243 epoch total loss 1.56827891\n",
      "Trained batch 1016 batch loss 1.45963967 epoch total loss 1.56817198\n",
      "Trained batch 1017 batch loss 1.3734535 epoch total loss 1.56798041\n",
      "Trained batch 1018 batch loss 1.50709343 epoch total loss 1.56792057\n",
      "Trained batch 1019 batch loss 1.36697984 epoch total loss 1.56772339\n",
      "Trained batch 1020 batch loss 1.48296547 epoch total loss 1.56764019\n",
      "Trained batch 1021 batch loss 1.57252383 epoch total loss 1.56764495\n",
      "Trained batch 1022 batch loss 1.53014541 epoch total loss 1.56760836\n",
      "Trained batch 1023 batch loss 1.58762574 epoch total loss 1.56762791\n",
      "Trained batch 1024 batch loss 1.68023288 epoch total loss 1.56773782\n",
      "Trained batch 1025 batch loss 1.63256288 epoch total loss 1.56780112\n",
      "Trained batch 1026 batch loss 1.55178559 epoch total loss 1.56778538\n",
      "Trained batch 1027 batch loss 1.34766197 epoch total loss 1.56757104\n",
      "Trained batch 1028 batch loss 1.37117541 epoch total loss 1.56738007\n",
      "Trained batch 1029 batch loss 1.21857572 epoch total loss 1.56704116\n",
      "Trained batch 1030 batch loss 1.29707062 epoch total loss 1.56677914\n",
      "Trained batch 1031 batch loss 1.48678315 epoch total loss 1.56670153\n",
      "Trained batch 1032 batch loss 1.22609162 epoch total loss 1.56637144\n",
      "Trained batch 1033 batch loss 1.26874375 epoch total loss 1.56608343\n",
      "Trained batch 1034 batch loss 1.22332954 epoch total loss 1.56575203\n",
      "Trained batch 1035 batch loss 1.34674561 epoch total loss 1.56554043\n",
      "Trained batch 1036 batch loss 1.39190376 epoch total loss 1.56537282\n",
      "Trained batch 1037 batch loss 1.42981827 epoch total loss 1.56524205\n",
      "Trained batch 1038 batch loss 1.52403295 epoch total loss 1.56520236\n",
      "Trained batch 1039 batch loss 1.53773582 epoch total loss 1.56517589\n",
      "Trained batch 1040 batch loss 1.65235686 epoch total loss 1.5652597\n",
      "Trained batch 1041 batch loss 1.51678944 epoch total loss 1.5652132\n",
      "Trained batch 1042 batch loss 1.48510635 epoch total loss 1.56513631\n",
      "Trained batch 1043 batch loss 1.49689925 epoch total loss 1.56507099\n",
      "Trained batch 1044 batch loss 1.41769481 epoch total loss 1.56492984\n",
      "Trained batch 1045 batch loss 1.38218474 epoch total loss 1.56475496\n",
      "Trained batch 1046 batch loss 1.52976656 epoch total loss 1.56472158\n",
      "Trained batch 1047 batch loss 1.65110099 epoch total loss 1.56480408\n",
      "Trained batch 1048 batch loss 1.55069971 epoch total loss 1.56479061\n",
      "Trained batch 1049 batch loss 1.69610381 epoch total loss 1.56491566\n",
      "Trained batch 1050 batch loss 1.56604409 epoch total loss 1.56491673\n",
      "Trained batch 1051 batch loss 1.62815881 epoch total loss 1.56497693\n",
      "Trained batch 1052 batch loss 1.47687399 epoch total loss 1.56489325\n",
      "Trained batch 1053 batch loss 1.5605638 epoch total loss 1.56488907\n",
      "Trained batch 1054 batch loss 1.52910924 epoch total loss 1.5648551\n",
      "Trained batch 1055 batch loss 1.54600024 epoch total loss 1.56483722\n",
      "Trained batch 1056 batch loss 1.43670702 epoch total loss 1.56471598\n",
      "Trained batch 1057 batch loss 1.53929985 epoch total loss 1.5646919\n",
      "Trained batch 1058 batch loss 1.54739082 epoch total loss 1.56467557\n",
      "Trained batch 1059 batch loss 1.54778421 epoch total loss 1.5646596\n",
      "Trained batch 1060 batch loss 1.38862157 epoch total loss 1.56449354\n",
      "Trained batch 1061 batch loss 1.45175982 epoch total loss 1.56438732\n",
      "Trained batch 1062 batch loss 1.42244864 epoch total loss 1.56425369\n",
      "Trained batch 1063 batch loss 1.43244958 epoch total loss 1.56412971\n",
      "Trained batch 1064 batch loss 1.49499202 epoch total loss 1.56406474\n",
      "Trained batch 1065 batch loss 1.45472693 epoch total loss 1.5639621\n",
      "Trained batch 1066 batch loss 1.39245868 epoch total loss 1.56380117\n",
      "Trained batch 1067 batch loss 1.52087116 epoch total loss 1.563761\n",
      "Trained batch 1068 batch loss 1.4817313 epoch total loss 1.56368411\n",
      "Trained batch 1069 batch loss 1.49724 epoch total loss 1.56362188\n",
      "Trained batch 1070 batch loss 1.43213701 epoch total loss 1.56349909\n",
      "Trained batch 1071 batch loss 1.39896595 epoch total loss 1.56334543\n",
      "Trained batch 1072 batch loss 1.43361115 epoch total loss 1.56322432\n",
      "Trained batch 1073 batch loss 1.36834407 epoch total loss 1.56304264\n",
      "Trained batch 1074 batch loss 1.43727517 epoch total loss 1.56292558\n",
      "Trained batch 1075 batch loss 1.57372689 epoch total loss 1.56293559\n",
      "Trained batch 1076 batch loss 1.59565568 epoch total loss 1.56296599\n",
      "Trained batch 1077 batch loss 1.64351439 epoch total loss 1.56304085\n",
      "Trained batch 1078 batch loss 1.56895959 epoch total loss 1.56304634\n",
      "Trained batch 1079 batch loss 1.59279823 epoch total loss 1.56307387\n",
      "Trained batch 1080 batch loss 1.47417 epoch total loss 1.5629915\n",
      "Trained batch 1081 batch loss 1.33094966 epoch total loss 1.56277692\n",
      "Trained batch 1082 batch loss 1.32789254 epoch total loss 1.56255984\n",
      "Trained batch 1083 batch loss 1.24721646 epoch total loss 1.56226861\n",
      "Trained batch 1084 batch loss 1.28256321 epoch total loss 1.56201065\n",
      "Trained batch 1085 batch loss 1.4155283 epoch total loss 1.56187558\n",
      "Trained batch 1086 batch loss 1.42081666 epoch total loss 1.56174564\n",
      "Trained batch 1087 batch loss 1.48226869 epoch total loss 1.56167257\n",
      "Trained batch 1088 batch loss 1.46838605 epoch total loss 1.56158686\n",
      "Trained batch 1089 batch loss 1.48977208 epoch total loss 1.56152081\n",
      "Trained batch 1090 batch loss 1.56033909 epoch total loss 1.56151974\n",
      "Trained batch 1091 batch loss 1.53299344 epoch total loss 1.56149352\n",
      "Trained batch 1092 batch loss 1.45899749 epoch total loss 1.5613997\n",
      "Trained batch 1093 batch loss 1.43210483 epoch total loss 1.56128144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1094 batch loss 1.39983344 epoch total loss 1.56113374\n",
      "Trained batch 1095 batch loss 1.48117244 epoch total loss 1.56106079\n",
      "Trained batch 1096 batch loss 1.48433781 epoch total loss 1.56099081\n",
      "Trained batch 1097 batch loss 1.52109063 epoch total loss 1.56095445\n",
      "Trained batch 1098 batch loss 1.46045256 epoch total loss 1.5608629\n",
      "Trained batch 1099 batch loss 1.4534322 epoch total loss 1.56076527\n",
      "Trained batch 1100 batch loss 1.52886808 epoch total loss 1.56073618\n",
      "Trained batch 1101 batch loss 1.44526386 epoch total loss 1.56063139\n",
      "Trained batch 1102 batch loss 1.3630271 epoch total loss 1.5604521\n",
      "Trained batch 1103 batch loss 1.44488263 epoch total loss 1.5603472\n",
      "Trained batch 1104 batch loss 1.47217298 epoch total loss 1.56026733\n",
      "Trained batch 1105 batch loss 1.52567458 epoch total loss 1.56023598\n",
      "Trained batch 1106 batch loss 1.56021571 epoch total loss 1.56023598\n",
      "Trained batch 1107 batch loss 1.443156 epoch total loss 1.56013012\n",
      "Trained batch 1108 batch loss 1.39350903 epoch total loss 1.5599798\n",
      "Trained batch 1109 batch loss 1.50264454 epoch total loss 1.55992818\n",
      "Trained batch 1110 batch loss 1.4673835 epoch total loss 1.55984485\n",
      "Trained batch 1111 batch loss 1.3510294 epoch total loss 1.55965686\n",
      "Trained batch 1112 batch loss 1.45835423 epoch total loss 1.55956578\n",
      "Trained batch 1113 batch loss 1.42777991 epoch total loss 1.55944741\n",
      "Trained batch 1114 batch loss 1.49229825 epoch total loss 1.55938709\n",
      "Trained batch 1115 batch loss 1.43207049 epoch total loss 1.559273\n",
      "Trained batch 1116 batch loss 1.346457 epoch total loss 1.55908227\n",
      "Trained batch 1117 batch loss 1.33043945 epoch total loss 1.55887759\n",
      "Trained batch 1118 batch loss 1.51855111 epoch total loss 1.55884147\n",
      "Trained batch 1119 batch loss 1.34893668 epoch total loss 1.55865383\n",
      "Trained batch 1120 batch loss 1.49904811 epoch total loss 1.55860066\n",
      "Trained batch 1121 batch loss 1.39074838 epoch total loss 1.55845094\n",
      "Trained batch 1122 batch loss 1.43292749 epoch total loss 1.558339\n",
      "Trained batch 1123 batch loss 1.49297655 epoch total loss 1.55828083\n",
      "Trained batch 1124 batch loss 1.58037376 epoch total loss 1.55830038\n",
      "Trained batch 1125 batch loss 1.60960388 epoch total loss 1.55834603\n",
      "Trained batch 1126 batch loss 1.57848787 epoch total loss 1.55836391\n",
      "Trained batch 1127 batch loss 1.4346838 epoch total loss 1.55825424\n",
      "Trained batch 1128 batch loss 1.38653862 epoch total loss 1.55810201\n",
      "Trained batch 1129 batch loss 1.54235256 epoch total loss 1.55808806\n",
      "Trained batch 1130 batch loss 1.51044631 epoch total loss 1.55804598\n",
      "Trained batch 1131 batch loss 1.6909157 epoch total loss 1.5581634\n",
      "Trained batch 1132 batch loss 1.61927915 epoch total loss 1.55821741\n",
      "Trained batch 1133 batch loss 1.59851265 epoch total loss 1.55825293\n",
      "Trained batch 1134 batch loss 1.50967574 epoch total loss 1.55821013\n",
      "Trained batch 1135 batch loss 1.45285237 epoch total loss 1.55811727\n",
      "Trained batch 1136 batch loss 1.37871897 epoch total loss 1.55795932\n",
      "Trained batch 1137 batch loss 1.48282671 epoch total loss 1.55789316\n",
      "Trained batch 1138 batch loss 1.58350325 epoch total loss 1.55791569\n",
      "Trained batch 1139 batch loss 1.44427872 epoch total loss 1.55781603\n",
      "Trained batch 1140 batch loss 1.45102072 epoch total loss 1.55772233\n",
      "Trained batch 1141 batch loss 1.38899457 epoch total loss 1.55757451\n",
      "Trained batch 1142 batch loss 1.41516 epoch total loss 1.55744982\n",
      "Trained batch 1143 batch loss 1.53715372 epoch total loss 1.55743206\n",
      "Trained batch 1144 batch loss 1.59866512 epoch total loss 1.55746806\n",
      "Trained batch 1145 batch loss 1.64299572 epoch total loss 1.55754268\n",
      "Trained batch 1146 batch loss 1.63278139 epoch total loss 1.55760837\n",
      "Trained batch 1147 batch loss 1.55179548 epoch total loss 1.55760324\n",
      "Trained batch 1148 batch loss 1.54144561 epoch total loss 1.55758917\n",
      "Trained batch 1149 batch loss 1.64191294 epoch total loss 1.55766261\n",
      "Trained batch 1150 batch loss 1.48022962 epoch total loss 1.55759537\n",
      "Trained batch 1151 batch loss 1.44676805 epoch total loss 1.55749905\n",
      "Trained batch 1152 batch loss 1.38104641 epoch total loss 1.55734587\n",
      "Trained batch 1153 batch loss 1.37344766 epoch total loss 1.55718637\n",
      "Trained batch 1154 batch loss 1.55810905 epoch total loss 1.5571872\n",
      "Trained batch 1155 batch loss 1.46945536 epoch total loss 1.55711126\n",
      "Trained batch 1156 batch loss 1.49384522 epoch total loss 1.55705655\n",
      "Trained batch 1157 batch loss 1.46519136 epoch total loss 1.55697715\n",
      "Trained batch 1158 batch loss 1.34871984 epoch total loss 1.55679739\n",
      "Trained batch 1159 batch loss 1.28155887 epoch total loss 1.55655992\n",
      "Trained batch 1160 batch loss 1.43253601 epoch total loss 1.55645299\n",
      "Trained batch 1161 batch loss 1.46758699 epoch total loss 1.55637646\n",
      "Trained batch 1162 batch loss 1.51799011 epoch total loss 1.55634332\n",
      "Trained batch 1163 batch loss 1.51344037 epoch total loss 1.55630648\n",
      "Trained batch 1164 batch loss 1.43649602 epoch total loss 1.55620348\n",
      "Trained batch 1165 batch loss 1.46891654 epoch total loss 1.5561285\n",
      "Trained batch 1166 batch loss 1.43310523 epoch total loss 1.556023\n",
      "Trained batch 1167 batch loss 1.46266603 epoch total loss 1.55594301\n",
      "Trained batch 1168 batch loss 1.40980268 epoch total loss 1.55581796\n",
      "Trained batch 1169 batch loss 1.38621283 epoch total loss 1.55567288\n",
      "Trained batch 1170 batch loss 1.49652028 epoch total loss 1.55562222\n",
      "Trained batch 1171 batch loss 1.45290768 epoch total loss 1.55553448\n",
      "Trained batch 1172 batch loss 1.36748528 epoch total loss 1.55537403\n",
      "Trained batch 1173 batch loss 1.43522477 epoch total loss 1.55527151\n",
      "Trained batch 1174 batch loss 1.62941933 epoch total loss 1.55533469\n",
      "Trained batch 1175 batch loss 1.58723557 epoch total loss 1.55536187\n",
      "Trained batch 1176 batch loss 1.55743217 epoch total loss 1.55536354\n",
      "Trained batch 1177 batch loss 1.57408857 epoch total loss 1.55537951\n",
      "Trained batch 1178 batch loss 1.57840562 epoch total loss 1.55539894\n",
      "Trained batch 1179 batch loss 1.4214468 epoch total loss 1.55528533\n",
      "Trained batch 1180 batch loss 1.39563441 epoch total loss 1.55515\n",
      "Trained batch 1181 batch loss 1.40341794 epoch total loss 1.55502152\n",
      "Trained batch 1182 batch loss 1.50771296 epoch total loss 1.55498147\n",
      "Trained batch 1183 batch loss 1.44430053 epoch total loss 1.55488801\n",
      "Trained batch 1184 batch loss 1.57670903 epoch total loss 1.55490637\n",
      "Trained batch 1185 batch loss 1.48446774 epoch total loss 1.554847\n",
      "Trained batch 1186 batch loss 1.4300853 epoch total loss 1.55474174\n",
      "Trained batch 1187 batch loss 1.49879503 epoch total loss 1.55469465\n",
      "Trained batch 1188 batch loss 1.57971704 epoch total loss 1.55471563\n",
      "Trained batch 1189 batch loss 1.63338864 epoch total loss 1.55478179\n",
      "Trained batch 1190 batch loss 1.53170037 epoch total loss 1.55476248\n",
      "Trained batch 1191 batch loss 1.55427945 epoch total loss 1.55476213\n",
      "Trained batch 1192 batch loss 1.53185034 epoch total loss 1.55474293\n",
      "Trained batch 1193 batch loss 1.54832911 epoch total loss 1.55473757\n",
      "Trained batch 1194 batch loss 1.48206258 epoch total loss 1.55467665\n",
      "Trained batch 1195 batch loss 1.50019312 epoch total loss 1.55463111\n",
      "Trained batch 1196 batch loss 1.45345235 epoch total loss 1.55454659\n",
      "Trained batch 1197 batch loss 1.37416911 epoch total loss 1.55439579\n",
      "Trained batch 1198 batch loss 1.40988028 epoch total loss 1.55427527\n",
      "Trained batch 1199 batch loss 1.36501741 epoch total loss 1.55411732\n",
      "Trained batch 1200 batch loss 1.55695081 epoch total loss 1.55411983\n",
      "Trained batch 1201 batch loss 1.53534412 epoch total loss 1.55410421\n",
      "Trained batch 1202 batch loss 1.58146989 epoch total loss 1.55412686\n",
      "Trained batch 1203 batch loss 1.68801928 epoch total loss 1.5542382\n",
      "Trained batch 1204 batch loss 1.54044557 epoch total loss 1.55422664\n",
      "Trained batch 1205 batch loss 1.5417093 epoch total loss 1.55421638\n",
      "Trained batch 1206 batch loss 1.4528625 epoch total loss 1.55413234\n",
      "Trained batch 1207 batch loss 1.35439277 epoch total loss 1.55396676\n",
      "Trained batch 1208 batch loss 1.48833418 epoch total loss 1.5539124\n",
      "Trained batch 1209 batch loss 1.55497217 epoch total loss 1.55391324\n",
      "Trained batch 1210 batch loss 1.4895153 epoch total loss 1.55386007\n",
      "Trained batch 1211 batch loss 1.46964717 epoch total loss 1.55379045\n",
      "Trained batch 1212 batch loss 1.50504518 epoch total loss 1.55375028\n",
      "Trained batch 1213 batch loss 1.41633081 epoch total loss 1.55363703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1214 batch loss 1.43714511 epoch total loss 1.55354095\n",
      "Trained batch 1215 batch loss 1.50278 epoch total loss 1.55349922\n",
      "Trained batch 1216 batch loss 1.45925927 epoch total loss 1.55342174\n",
      "Trained batch 1217 batch loss 1.50039649 epoch total loss 1.55337811\n",
      "Trained batch 1218 batch loss 1.48729849 epoch total loss 1.55332386\n",
      "Trained batch 1219 batch loss 1.45130873 epoch total loss 1.55324018\n",
      "Trained batch 1220 batch loss 1.22222173 epoch total loss 1.55296886\n",
      "Trained batch 1221 batch loss 1.30543327 epoch total loss 1.55276608\n",
      "Trained batch 1222 batch loss 1.53173566 epoch total loss 1.55274892\n",
      "Trained batch 1223 batch loss 1.49608064 epoch total loss 1.55270255\n",
      "Trained batch 1224 batch loss 1.55384958 epoch total loss 1.5527035\n",
      "Trained batch 1225 batch loss 1.51677167 epoch total loss 1.55267406\n",
      "Trained batch 1226 batch loss 1.60086632 epoch total loss 1.55271339\n",
      "Trained batch 1227 batch loss 1.51769054 epoch total loss 1.55268478\n",
      "Trained batch 1228 batch loss 1.47032988 epoch total loss 1.55261779\n",
      "Trained batch 1229 batch loss 1.57889819 epoch total loss 1.55263913\n",
      "Trained batch 1230 batch loss 1.40208316 epoch total loss 1.5525167\n",
      "Trained batch 1231 batch loss 1.50070691 epoch total loss 1.55247462\n",
      "Trained batch 1232 batch loss 1.45371819 epoch total loss 1.55239451\n",
      "Trained batch 1233 batch loss 1.46245289 epoch total loss 1.55232155\n",
      "Trained batch 1234 batch loss 1.56459141 epoch total loss 1.55233145\n",
      "Trained batch 1235 batch loss 1.44754624 epoch total loss 1.55224657\n",
      "Trained batch 1236 batch loss 1.4740026 epoch total loss 1.55218327\n",
      "Trained batch 1237 batch loss 1.4336524 epoch total loss 1.55208743\n",
      "Trained batch 1238 batch loss 1.46121299 epoch total loss 1.55201399\n",
      "Trained batch 1239 batch loss 1.50452769 epoch total loss 1.55197561\n",
      "Trained batch 1240 batch loss 1.42365122 epoch total loss 1.55187225\n",
      "Trained batch 1241 batch loss 1.41659093 epoch total loss 1.55176318\n",
      "Trained batch 1242 batch loss 1.44045162 epoch total loss 1.55167353\n",
      "Trained batch 1243 batch loss 1.43612027 epoch total loss 1.55158067\n",
      "Trained batch 1244 batch loss 1.39249897 epoch total loss 1.55145276\n",
      "Trained batch 1245 batch loss 1.42558265 epoch total loss 1.55135155\n",
      "Trained batch 1246 batch loss 1.43642664 epoch total loss 1.5512594\n",
      "Trained batch 1247 batch loss 1.4320575 epoch total loss 1.55116367\n",
      "Trained batch 1248 batch loss 1.42875195 epoch total loss 1.55106556\n",
      "Trained batch 1249 batch loss 1.35039294 epoch total loss 1.55090487\n",
      "Trained batch 1250 batch loss 1.43907583 epoch total loss 1.55081546\n",
      "Trained batch 1251 batch loss 1.46171069 epoch total loss 1.55074418\n",
      "Trained batch 1252 batch loss 1.34053016 epoch total loss 1.55057633\n",
      "Trained batch 1253 batch loss 1.34073973 epoch total loss 1.55040884\n",
      "Trained batch 1254 batch loss 1.38899267 epoch total loss 1.55028009\n",
      "Trained batch 1255 batch loss 1.44145083 epoch total loss 1.55019331\n",
      "Trained batch 1256 batch loss 1.34085441 epoch total loss 1.55002666\n",
      "Trained batch 1257 batch loss 1.47689402 epoch total loss 1.54996848\n",
      "Trained batch 1258 batch loss 1.45475483 epoch total loss 1.54989278\n",
      "Trained batch 1259 batch loss 1.51216555 epoch total loss 1.54986286\n",
      "Trained batch 1260 batch loss 1.50591516 epoch total loss 1.54982793\n",
      "Trained batch 1261 batch loss 1.39087903 epoch total loss 1.54970193\n",
      "Trained batch 1262 batch loss 1.37539196 epoch total loss 1.54956377\n",
      "Trained batch 1263 batch loss 1.408571 epoch total loss 1.54945207\n",
      "Trained batch 1264 batch loss 1.37349129 epoch total loss 1.54931295\n",
      "Trained batch 1265 batch loss 1.33878422 epoch total loss 1.54914641\n",
      "Trained batch 1266 batch loss 1.40958905 epoch total loss 1.54903615\n",
      "Trained batch 1267 batch loss 1.38146567 epoch total loss 1.54890394\n",
      "Trained batch 1268 batch loss 1.37002492 epoch total loss 1.5487628\n",
      "Trained batch 1269 batch loss 1.44508982 epoch total loss 1.54868114\n",
      "Trained batch 1270 batch loss 1.42849517 epoch total loss 1.54858649\n",
      "Trained batch 1271 batch loss 1.33443642 epoch total loss 1.54841805\n",
      "Trained batch 1272 batch loss 1.50368428 epoch total loss 1.54838288\n",
      "Trained batch 1273 batch loss 1.47696519 epoch total loss 1.54832673\n",
      "Trained batch 1274 batch loss 1.60907257 epoch total loss 1.54837441\n",
      "Trained batch 1275 batch loss 1.47712862 epoch total loss 1.54831862\n",
      "Trained batch 1276 batch loss 1.50874484 epoch total loss 1.54828763\n",
      "Trained batch 1277 batch loss 1.47734 epoch total loss 1.54823196\n",
      "Trained batch 1278 batch loss 1.48857236 epoch total loss 1.54818535\n",
      "Trained batch 1279 batch loss 1.42189395 epoch total loss 1.54808652\n",
      "Trained batch 1280 batch loss 1.44909167 epoch total loss 1.54800916\n",
      "Trained batch 1281 batch loss 1.56065845 epoch total loss 1.54801905\n",
      "Trained batch 1282 batch loss 1.43059206 epoch total loss 1.5479275\n",
      "Trained batch 1283 batch loss 1.50814795 epoch total loss 1.5478965\n",
      "Trained batch 1284 batch loss 1.49342668 epoch total loss 1.54785407\n",
      "Trained batch 1285 batch loss 1.44289947 epoch total loss 1.54777229\n",
      "Trained batch 1286 batch loss 1.49815249 epoch total loss 1.54773378\n",
      "Trained batch 1287 batch loss 1.49204063 epoch total loss 1.54769051\n",
      "Trained batch 1288 batch loss 1.46093857 epoch total loss 1.54762316\n",
      "Trained batch 1289 batch loss 1.4488492 epoch total loss 1.54754651\n",
      "Trained batch 1290 batch loss 1.38435721 epoch total loss 1.54742\n",
      "Trained batch 1291 batch loss 1.47924471 epoch total loss 1.54736722\n",
      "Trained batch 1292 batch loss 1.42311382 epoch total loss 1.54727101\n",
      "Trained batch 1293 batch loss 1.48387814 epoch total loss 1.54722202\n",
      "Trained batch 1294 batch loss 1.34988666 epoch total loss 1.54706955\n",
      "Trained batch 1295 batch loss 1.38748741 epoch total loss 1.54694629\n",
      "Trained batch 1296 batch loss 1.45641518 epoch total loss 1.54687643\n",
      "Trained batch 1297 batch loss 1.45971 epoch total loss 1.5468092\n",
      "Trained batch 1298 batch loss 1.44498813 epoch total loss 1.54673076\n",
      "Trained batch 1299 batch loss 1.42151904 epoch total loss 1.54663432\n",
      "Trained batch 1300 batch loss 1.47188735 epoch total loss 1.54657686\n",
      "Trained batch 1301 batch loss 1.55776274 epoch total loss 1.54658544\n",
      "Trained batch 1302 batch loss 1.49964607 epoch total loss 1.54654944\n",
      "Trained batch 1303 batch loss 1.53085303 epoch total loss 1.5465374\n",
      "Trained batch 1304 batch loss 1.40462601 epoch total loss 1.54642856\n",
      "Trained batch 1305 batch loss 1.4485023 epoch total loss 1.54635346\n",
      "Trained batch 1306 batch loss 1.40432012 epoch total loss 1.54624474\n",
      "Trained batch 1307 batch loss 1.32174098 epoch total loss 1.54607296\n",
      "Trained batch 1308 batch loss 1.33691895 epoch total loss 1.5459131\n",
      "Trained batch 1309 batch loss 1.32219577 epoch total loss 1.54574215\n",
      "Trained batch 1310 batch loss 1.29858506 epoch total loss 1.54555345\n",
      "Trained batch 1311 batch loss 1.34317 epoch total loss 1.54539907\n",
      "Trained batch 1312 batch loss 1.41831231 epoch total loss 1.54530227\n",
      "Trained batch 1313 batch loss 1.36302471 epoch total loss 1.54516339\n",
      "Trained batch 1314 batch loss 1.48240614 epoch total loss 1.54511571\n",
      "Trained batch 1315 batch loss 1.51583195 epoch total loss 1.54509342\n",
      "Trained batch 1316 batch loss 1.47165918 epoch total loss 1.54503763\n",
      "Trained batch 1317 batch loss 1.30777097 epoch total loss 1.5448575\n",
      "Trained batch 1318 batch loss 1.4390763 epoch total loss 1.54477715\n",
      "Trained batch 1319 batch loss 1.51979947 epoch total loss 1.5447582\n",
      "Trained batch 1320 batch loss 1.35202348 epoch total loss 1.54461229\n",
      "Trained batch 1321 batch loss 1.53213775 epoch total loss 1.54460275\n",
      "Trained batch 1322 batch loss 1.45687795 epoch total loss 1.54453647\n",
      "Trained batch 1323 batch loss 1.53201938 epoch total loss 1.54452693\n",
      "Trained batch 1324 batch loss 1.50368714 epoch total loss 1.54449606\n",
      "Trained batch 1325 batch loss 1.40398622 epoch total loss 1.54439\n",
      "Trained batch 1326 batch loss 1.45400548 epoch total loss 1.54432178\n",
      "Trained batch 1327 batch loss 1.41693032 epoch total loss 1.54422593\n",
      "Trained batch 1328 batch loss 1.42979884 epoch total loss 1.54413962\n",
      "Trained batch 1329 batch loss 1.40191913 epoch total loss 1.54403257\n",
      "Trained batch 1330 batch loss 1.48873234 epoch total loss 1.54399097\n",
      "Trained batch 1331 batch loss 1.44465852 epoch total loss 1.54391634\n",
      "Trained batch 1332 batch loss 1.447083 epoch total loss 1.54384363\n",
      "Trained batch 1333 batch loss 1.51346231 epoch total loss 1.54382074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1334 batch loss 1.48513305 epoch total loss 1.54377675\n",
      "Trained batch 1335 batch loss 1.32048237 epoch total loss 1.5436095\n",
      "Trained batch 1336 batch loss 1.46845865 epoch total loss 1.54355335\n",
      "Trained batch 1337 batch loss 1.43581831 epoch total loss 1.54347277\n",
      "Trained batch 1338 batch loss 1.52450359 epoch total loss 1.54345846\n",
      "Trained batch 1339 batch loss 1.50950086 epoch total loss 1.54343319\n",
      "Trained batch 1340 batch loss 1.4413209 epoch total loss 1.54335701\n",
      "Trained batch 1341 batch loss 1.43739367 epoch total loss 1.5432781\n",
      "Trained batch 1342 batch loss 1.54076815 epoch total loss 1.54327619\n",
      "Trained batch 1343 batch loss 1.30644321 epoch total loss 1.54309976\n",
      "Trained batch 1344 batch loss 1.60936928 epoch total loss 1.54314911\n",
      "Trained batch 1345 batch loss 1.5181433 epoch total loss 1.54313052\n",
      "Trained batch 1346 batch loss 1.59245825 epoch total loss 1.54316723\n",
      "Trained batch 1347 batch loss 1.54016662 epoch total loss 1.54316509\n",
      "Trained batch 1348 batch loss 1.51785898 epoch total loss 1.54314625\n",
      "Trained batch 1349 batch loss 1.41114402 epoch total loss 1.54304838\n",
      "Trained batch 1350 batch loss 1.35811317 epoch total loss 1.54291141\n",
      "Trained batch 1351 batch loss 1.44765627 epoch total loss 1.54284096\n",
      "Trained batch 1352 batch loss 1.45688558 epoch total loss 1.5427773\n",
      "Trained batch 1353 batch loss 1.415416 epoch total loss 1.54268324\n",
      "Trained batch 1354 batch loss 1.44311869 epoch total loss 1.54260981\n",
      "Trained batch 1355 batch loss 1.47331631 epoch total loss 1.54255867\n",
      "Trained batch 1356 batch loss 1.44282496 epoch total loss 1.54248512\n",
      "Trained batch 1357 batch loss 1.41356421 epoch total loss 1.54239011\n",
      "Trained batch 1358 batch loss 1.52099013 epoch total loss 1.54237437\n",
      "Trained batch 1359 batch loss 1.3892138 epoch total loss 1.5422616\n",
      "Trained batch 1360 batch loss 1.40137625 epoch total loss 1.54215801\n",
      "Trained batch 1361 batch loss 1.43596363 epoch total loss 1.54208\n",
      "Trained batch 1362 batch loss 1.35936821 epoch total loss 1.54194593\n",
      "Trained batch 1363 batch loss 1.3826189 epoch total loss 1.54182899\n",
      "Trained batch 1364 batch loss 1.37413549 epoch total loss 1.54170597\n",
      "Trained batch 1365 batch loss 1.37632346 epoch total loss 1.54158473\n",
      "Trained batch 1366 batch loss 1.55860329 epoch total loss 1.54159725\n",
      "Trained batch 1367 batch loss 1.33378053 epoch total loss 1.54144514\n",
      "Trained batch 1368 batch loss 1.42015612 epoch total loss 1.54135644\n",
      "Trained batch 1369 batch loss 1.43728852 epoch total loss 1.54128051\n",
      "Trained batch 1370 batch loss 1.53448319 epoch total loss 1.5412755\n",
      "Trained batch 1371 batch loss 1.5933845 epoch total loss 1.54131353\n",
      "Trained batch 1372 batch loss 1.62803698 epoch total loss 1.54137671\n",
      "Trained batch 1373 batch loss 1.5198909 epoch total loss 1.54136097\n",
      "Trained batch 1374 batch loss 1.37406301 epoch total loss 1.54123914\n",
      "Trained batch 1375 batch loss 1.40872657 epoch total loss 1.5411427\n",
      "Trained batch 1376 batch loss 1.40087271 epoch total loss 1.54104078\n",
      "Trained batch 1377 batch loss 1.41932309 epoch total loss 1.54095256\n",
      "Trained batch 1378 batch loss 1.50708759 epoch total loss 1.54092789\n",
      "Trained batch 1379 batch loss 1.54587722 epoch total loss 1.54093158\n",
      "Trained batch 1380 batch loss 1.54241216 epoch total loss 1.54093266\n",
      "Trained batch 1381 batch loss 1.50637221 epoch total loss 1.54090762\n",
      "Trained batch 1382 batch loss 1.5474112 epoch total loss 1.54091227\n",
      "Trained batch 1383 batch loss 1.62758136 epoch total loss 1.54097497\n",
      "Trained batch 1384 batch loss 1.55394626 epoch total loss 1.54098439\n",
      "Trained batch 1385 batch loss 1.54472136 epoch total loss 1.54098701\n",
      "Trained batch 1386 batch loss 1.48510432 epoch total loss 1.54094672\n",
      "Trained batch 1387 batch loss 1.4902432 epoch total loss 1.54091024\n",
      "Trained batch 1388 batch loss 1.53910816 epoch total loss 1.54090881\n",
      "Epoch 1 train loss 1.5409088134765625\n",
      "Validated batch 1 batch loss 1.57267737\n",
      "Validated batch 2 batch loss 1.49671304\n",
      "Validated batch 3 batch loss 1.44813657\n",
      "Validated batch 4 batch loss 1.44553828\n",
      "Validated batch 5 batch loss 1.44120586\n",
      "Validated batch 6 batch loss 1.46332729\n",
      "Validated batch 7 batch loss 1.50053823\n",
      "Validated batch 8 batch loss 1.37636948\n",
      "Validated batch 9 batch loss 1.48563576\n",
      "Validated batch 10 batch loss 1.42008424\n",
      "Validated batch 11 batch loss 1.49113667\n",
      "Validated batch 12 batch loss 1.44188285\n",
      "Validated batch 13 batch loss 1.4541738\n",
      "Validated batch 14 batch loss 1.44274139\n",
      "Validated batch 15 batch loss 1.40558767\n",
      "Validated batch 16 batch loss 1.48787534\n",
      "Validated batch 17 batch loss 1.48125029\n",
      "Validated batch 18 batch loss 1.52585149\n",
      "Validated batch 19 batch loss 1.59413648\n",
      "Validated batch 20 batch loss 1.65768707\n",
      "Validated batch 21 batch loss 1.52638447\n",
      "Validated batch 22 batch loss 1.48845649\n",
      "Validated batch 23 batch loss 1.38505983\n",
      "Validated batch 24 batch loss 1.42468309\n",
      "Validated batch 25 batch loss 1.38513422\n",
      "Validated batch 26 batch loss 1.46328771\n",
      "Validated batch 27 batch loss 1.39797354\n",
      "Validated batch 28 batch loss 1.47074628\n",
      "Validated batch 29 batch loss 1.49049437\n",
      "Validated batch 30 batch loss 1.45989728\n",
      "Validated batch 31 batch loss 1.56460392\n",
      "Validated batch 32 batch loss 1.52606511\n",
      "Validated batch 33 batch loss 1.50419021\n",
      "Validated batch 34 batch loss 1.45327437\n",
      "Validated batch 35 batch loss 1.51033556\n",
      "Validated batch 36 batch loss 1.52617645\n",
      "Validated batch 37 batch loss 1.5138905\n",
      "Validated batch 38 batch loss 1.52185893\n",
      "Validated batch 39 batch loss 1.50928187\n",
      "Validated batch 40 batch loss 1.53195322\n",
      "Validated batch 41 batch loss 1.35319829\n",
      "Validated batch 42 batch loss 1.50299358\n",
      "Validated batch 43 batch loss 1.51878417\n",
      "Validated batch 44 batch loss 1.4588871\n",
      "Validated batch 45 batch loss 1.4977684\n",
      "Validated batch 46 batch loss 1.44454658\n",
      "Validated batch 47 batch loss 1.40351164\n",
      "Validated batch 48 batch loss 1.53695846\n",
      "Validated batch 49 batch loss 1.51166224\n",
      "Validated batch 50 batch loss 1.42374396\n",
      "Validated batch 51 batch loss 1.52168989\n",
      "Validated batch 52 batch loss 1.56376922\n",
      "Validated batch 53 batch loss 1.36997163\n",
      "Validated batch 54 batch loss 1.52132201\n",
      "Validated batch 55 batch loss 1.45182133\n",
      "Validated batch 56 batch loss 1.48101664\n",
      "Validated batch 57 batch loss 1.4749527\n",
      "Validated batch 58 batch loss 1.39724648\n",
      "Validated batch 59 batch loss 1.32505536\n",
      "Validated batch 60 batch loss 1.48146129\n",
      "Validated batch 61 batch loss 1.47933507\n",
      "Validated batch 62 batch loss 1.42176247\n",
      "Validated batch 63 batch loss 1.50378346\n",
      "Validated batch 64 batch loss 1.45976686\n",
      "Validated batch 65 batch loss 1.50300384\n",
      "Validated batch 66 batch loss 1.50592124\n",
      "Validated batch 67 batch loss 1.480317\n",
      "Validated batch 68 batch loss 1.48502493\n",
      "Validated batch 69 batch loss 1.38878036\n",
      "Validated batch 70 batch loss 1.52447844\n",
      "Validated batch 71 batch loss 1.41364884\n",
      "Validated batch 72 batch loss 1.46333504\n",
      "Validated batch 73 batch loss 1.3659488\n",
      "Validated batch 74 batch loss 1.44992697\n",
      "Validated batch 75 batch loss 1.5188055\n",
      "Validated batch 76 batch loss 1.39655828\n",
      "Validated batch 77 batch loss 1.48379469\n",
      "Validated batch 78 batch loss 1.47371876\n",
      "Validated batch 79 batch loss 1.46021461\n",
      "Validated batch 80 batch loss 1.47013116\n",
      "Validated batch 81 batch loss 1.35386515\n",
      "Validated batch 82 batch loss 1.57495356\n",
      "Validated batch 83 batch loss 1.46006393\n",
      "Validated batch 84 batch loss 1.48867428\n",
      "Validated batch 85 batch loss 1.44330812\n",
      "Validated batch 86 batch loss 1.47149968\n",
      "Validated batch 87 batch loss 1.38644171\n",
      "Validated batch 88 batch loss 1.40509129\n",
      "Validated batch 89 batch loss 1.44040775\n",
      "Validated batch 90 batch loss 1.42083597\n",
      "Validated batch 91 batch loss 1.45336282\n",
      "Validated batch 92 batch loss 1.47128582\n",
      "Validated batch 93 batch loss 1.48502076\n",
      "Validated batch 94 batch loss 1.53778946\n",
      "Validated batch 95 batch loss 1.37323976\n",
      "Validated batch 96 batch loss 1.48993504\n",
      "Validated batch 97 batch loss 1.48707414\n",
      "Validated batch 98 batch loss 1.47710419\n",
      "Validated batch 99 batch loss 1.52286649\n",
      "Validated batch 100 batch loss 1.50434589\n",
      "Validated batch 101 batch loss 1.54624331\n",
      "Validated batch 102 batch loss 1.52102685\n",
      "Validated batch 103 batch loss 1.54503739\n",
      "Validated batch 104 batch loss 1.51675487\n",
      "Validated batch 105 batch loss 1.41559517\n",
      "Validated batch 106 batch loss 1.55015135\n",
      "Validated batch 107 batch loss 1.48245144\n",
      "Validated batch 108 batch loss 1.53238964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 109 batch loss 1.55230618\n",
      "Validated batch 110 batch loss 1.35352409\n",
      "Validated batch 111 batch loss 1.44202185\n",
      "Validated batch 112 batch loss 1.44911301\n",
      "Validated batch 113 batch loss 1.43842769\n",
      "Validated batch 114 batch loss 1.56567574\n",
      "Validated batch 115 batch loss 1.42043316\n",
      "Validated batch 116 batch loss 1.49995804\n",
      "Validated batch 117 batch loss 1.54484844\n",
      "Validated batch 118 batch loss 1.45203352\n",
      "Validated batch 119 batch loss 1.39394152\n",
      "Validated batch 120 batch loss 1.4636569\n",
      "Validated batch 121 batch loss 1.44426632\n",
      "Validated batch 122 batch loss 1.44345593\n",
      "Validated batch 123 batch loss 1.41967082\n",
      "Validated batch 124 batch loss 1.38905978\n",
      "Validated batch 125 batch loss 1.54917181\n",
      "Validated batch 126 batch loss 1.40476084\n",
      "Validated batch 127 batch loss 1.40721071\n",
      "Validated batch 128 batch loss 1.42005825\n",
      "Validated batch 129 batch loss 1.55192828\n",
      "Validated batch 130 batch loss 1.48122525\n",
      "Validated batch 131 batch loss 1.5303688\n",
      "Validated batch 132 batch loss 1.41143584\n",
      "Validated batch 133 batch loss 1.5225395\n",
      "Validated batch 134 batch loss 1.44862735\n",
      "Validated batch 135 batch loss 1.54525471\n",
      "Validated batch 136 batch loss 1.52942896\n",
      "Validated batch 137 batch loss 1.29027057\n",
      "Validated batch 138 batch loss 1.48496175\n",
      "Validated batch 139 batch loss 1.50179374\n",
      "Validated batch 140 batch loss 1.37104881\n",
      "Validated batch 141 batch loss 1.46383667\n",
      "Validated batch 142 batch loss 1.43942297\n",
      "Validated batch 143 batch loss 1.43233478\n",
      "Validated batch 144 batch loss 1.46394753\n",
      "Validated batch 145 batch loss 1.43971312\n",
      "Validated batch 146 batch loss 1.46898055\n",
      "Validated batch 147 batch loss 1.50091112\n",
      "Validated batch 148 batch loss 1.40424085\n",
      "Validated batch 149 batch loss 1.58511531\n",
      "Validated batch 150 batch loss 1.52976179\n",
      "Validated batch 151 batch loss 1.41804075\n",
      "Validated batch 152 batch loss 1.47145188\n",
      "Validated batch 153 batch loss 1.52007282\n",
      "Validated batch 154 batch loss 1.42340374\n",
      "Validated batch 155 batch loss 1.57385445\n",
      "Validated batch 156 batch loss 1.4415338\n",
      "Validated batch 157 batch loss 1.51064873\n",
      "Validated batch 158 batch loss 1.46225238\n",
      "Validated batch 159 batch loss 1.43877828\n",
      "Validated batch 160 batch loss 1.44458878\n",
      "Validated batch 161 batch loss 1.4589287\n",
      "Validated batch 162 batch loss 1.45526648\n",
      "Validated batch 163 batch loss 1.45397401\n",
      "Validated batch 164 batch loss 1.44152284\n",
      "Validated batch 165 batch loss 1.32912242\n",
      "Validated batch 166 batch loss 1.41312504\n",
      "Validated batch 167 batch loss 1.53084016\n",
      "Validated batch 168 batch loss 1.36923921\n",
      "Validated batch 169 batch loss 1.4256345\n",
      "Validated batch 170 batch loss 1.46465778\n",
      "Validated batch 171 batch loss 1.49502039\n",
      "Validated batch 172 batch loss 1.42063737\n",
      "Validated batch 173 batch loss 1.45661426\n",
      "Validated batch 174 batch loss 1.4212935\n",
      "Validated batch 175 batch loss 1.51868951\n",
      "Validated batch 176 batch loss 1.48681355\n",
      "Validated batch 177 batch loss 1.52764297\n",
      "Validated batch 178 batch loss 1.46181035\n",
      "Validated batch 179 batch loss 1.52949846\n",
      "Validated batch 180 batch loss 1.42409325\n",
      "Validated batch 181 batch loss 1.36931205\n",
      "Validated batch 182 batch loss 1.49406338\n",
      "Validated batch 183 batch loss 1.44393265\n",
      "Validated batch 184 batch loss 1.54906476\n",
      "Validated batch 185 batch loss 1.57095289\n",
      "Epoch 1 val loss 1.4691832065582275\n",
      "Model /aiffel/aiffel/mpii/models/model_HG-epoch-1-loss-1.4692.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.55659652 epoch total loss 1.55659652\n",
      "Trained batch 2 batch loss 1.50536239 epoch total loss 1.53097939\n",
      "Trained batch 3 batch loss 1.42998934 epoch total loss 1.497316\n",
      "Trained batch 4 batch loss 1.43032432 epoch total loss 1.48056817\n",
      "Trained batch 5 batch loss 1.35984921 epoch total loss 1.45642436\n",
      "Trained batch 6 batch loss 1.43214989 epoch total loss 1.45237863\n",
      "Trained batch 7 batch loss 1.32060599 epoch total loss 1.43355393\n",
      "Trained batch 8 batch loss 1.22935641 epoch total loss 1.40802932\n",
      "Trained batch 9 batch loss 1.22763491 epoch total loss 1.38798547\n",
      "Trained batch 10 batch loss 1.29986823 epoch total loss 1.37917376\n",
      "Trained batch 11 batch loss 1.4071511 epoch total loss 1.38171721\n",
      "Trained batch 12 batch loss 1.22405207 epoch total loss 1.36857843\n",
      "Trained batch 13 batch loss 1.33284378 epoch total loss 1.36582959\n",
      "Trained batch 14 batch loss 1.35466957 epoch total loss 1.36503243\n",
      "Trained batch 15 batch loss 1.39804387 epoch total loss 1.36723328\n",
      "Trained batch 16 batch loss 1.44226754 epoch total loss 1.37192297\n",
      "Trained batch 17 batch loss 1.28445888 epoch total loss 1.36677802\n",
      "Trained batch 18 batch loss 1.31043923 epoch total loss 1.36364806\n",
      "Trained batch 19 batch loss 1.49537826 epoch total loss 1.37058127\n",
      "Trained batch 20 batch loss 1.50164819 epoch total loss 1.37713456\n",
      "Trained batch 21 batch loss 1.5020684 epoch total loss 1.38308382\n",
      "Trained batch 22 batch loss 1.40749621 epoch total loss 1.38419342\n",
      "Trained batch 23 batch loss 1.44484091 epoch total loss 1.38683033\n",
      "Trained batch 24 batch loss 1.33158994 epoch total loss 1.38452864\n",
      "Trained batch 25 batch loss 1.44610834 epoch total loss 1.38699186\n",
      "Trained batch 26 batch loss 1.38471079 epoch total loss 1.38690424\n",
      "Trained batch 27 batch loss 1.37621725 epoch total loss 1.38650835\n",
      "Trained batch 28 batch loss 1.33754 epoch total loss 1.38475955\n",
      "Trained batch 29 batch loss 1.33970952 epoch total loss 1.38320613\n",
      "Trained batch 30 batch loss 1.3748107 epoch total loss 1.38292623\n",
      "Trained batch 31 batch loss 1.37534547 epoch total loss 1.38268173\n",
      "Trained batch 32 batch loss 1.43347 epoch total loss 1.38426888\n",
      "Trained batch 33 batch loss 1.4032681 epoch total loss 1.38484454\n",
      "Trained batch 34 batch loss 1.28469229 epoch total loss 1.38189888\n",
      "Trained batch 35 batch loss 1.43275309 epoch total loss 1.38335192\n",
      "Trained batch 36 batch loss 1.40155077 epoch total loss 1.38385737\n",
      "Trained batch 37 batch loss 1.47134316 epoch total loss 1.38622189\n",
      "Trained batch 38 batch loss 1.44220054 epoch total loss 1.38769495\n",
      "Trained batch 39 batch loss 1.44721794 epoch total loss 1.38922119\n",
      "Trained batch 40 batch loss 1.28022575 epoch total loss 1.38649631\n",
      "Trained batch 41 batch loss 1.33429205 epoch total loss 1.38522315\n",
      "Trained batch 42 batch loss 1.4211787 epoch total loss 1.38607919\n",
      "Trained batch 43 batch loss 1.32482219 epoch total loss 1.38465452\n",
      "Trained batch 44 batch loss 1.28749371 epoch total loss 1.38244641\n",
      "Trained batch 45 batch loss 1.39662516 epoch total loss 1.38276148\n",
      "Trained batch 46 batch loss 1.41714954 epoch total loss 1.38350904\n",
      "Trained batch 47 batch loss 1.39736974 epoch total loss 1.38380396\n",
      "Trained batch 48 batch loss 1.41002285 epoch total loss 1.3843503\n",
      "Trained batch 49 batch loss 1.38726068 epoch total loss 1.38440967\n",
      "Trained batch 50 batch loss 1.36719656 epoch total loss 1.38406539\n",
      "Trained batch 51 batch loss 1.36025 epoch total loss 1.38359845\n",
      "Trained batch 52 batch loss 1.46650195 epoch total loss 1.38519275\n",
      "Trained batch 53 batch loss 1.42390335 epoch total loss 1.38592315\n",
      "Trained batch 54 batch loss 1.45163846 epoch total loss 1.38714\n",
      "Trained batch 55 batch loss 1.35729718 epoch total loss 1.38659751\n",
      "Trained batch 56 batch loss 1.34607053 epoch total loss 1.38587379\n",
      "Trained batch 57 batch loss 1.47715616 epoch total loss 1.38747525\n",
      "Trained batch 58 batch loss 1.39230371 epoch total loss 1.38755846\n",
      "Trained batch 59 batch loss 1.50211358 epoch total loss 1.38950014\n",
      "Trained batch 60 batch loss 1.48985898 epoch total loss 1.39117277\n",
      "Trained batch 61 batch loss 1.59010923 epoch total loss 1.39443409\n",
      "Trained batch 62 batch loss 1.5336833 epoch total loss 1.39668\n",
      "Trained batch 63 batch loss 1.45653546 epoch total loss 1.3976301\n",
      "Trained batch 64 batch loss 1.40350175 epoch total loss 1.39772189\n",
      "Trained batch 65 batch loss 1.44596303 epoch total loss 1.39846408\n",
      "Trained batch 66 batch loss 1.50151265 epoch total loss 1.40002537\n",
      "Trained batch 67 batch loss 1.42410362 epoch total loss 1.40038466\n",
      "Trained batch 68 batch loss 1.44840181 epoch total loss 1.40109086\n",
      "Trained batch 69 batch loss 1.46603501 epoch total loss 1.40203202\n",
      "Trained batch 70 batch loss 1.54054487 epoch total loss 1.40401077\n",
      "Trained batch 71 batch loss 1.4512434 epoch total loss 1.40467596\n",
      "Trained batch 72 batch loss 1.41064036 epoch total loss 1.40475881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 73 batch loss 1.37386036 epoch total loss 1.4043355\n",
      "Trained batch 74 batch loss 1.32115388 epoch total loss 1.40321147\n",
      "Trained batch 75 batch loss 1.28874862 epoch total loss 1.40168524\n",
      "Trained batch 76 batch loss 1.26053047 epoch total loss 1.39982796\n",
      "Trained batch 77 batch loss 1.33890033 epoch total loss 1.39903665\n",
      "Trained batch 78 batch loss 1.25433636 epoch total loss 1.39718151\n",
      "Trained batch 79 batch loss 1.15413427 epoch total loss 1.39410496\n",
      "Trained batch 80 batch loss 1.12854481 epoch total loss 1.39078546\n",
      "Trained batch 81 batch loss 1.20011079 epoch total loss 1.38843155\n",
      "Trained batch 82 batch loss 1.27027071 epoch total loss 1.38699055\n",
      "Trained batch 83 batch loss 1.52912939 epoch total loss 1.38870299\n",
      "Trained batch 84 batch loss 1.41507363 epoch total loss 1.38901699\n",
      "Trained batch 85 batch loss 1.35902286 epoch total loss 1.38866413\n",
      "Trained batch 86 batch loss 1.41828942 epoch total loss 1.38900864\n",
      "Trained batch 87 batch loss 1.42042375 epoch total loss 1.38936973\n",
      "Trained batch 88 batch loss 1.5601449 epoch total loss 1.39131033\n",
      "Trained batch 89 batch loss 1.31874871 epoch total loss 1.39049506\n",
      "Trained batch 90 batch loss 1.47316861 epoch total loss 1.39141357\n",
      "Trained batch 91 batch loss 1.41817939 epoch total loss 1.39170778\n",
      "Trained batch 92 batch loss 1.45504045 epoch total loss 1.39239621\n",
      "Trained batch 93 batch loss 1.45149457 epoch total loss 1.3930316\n",
      "Trained batch 94 batch loss 1.42321444 epoch total loss 1.39335275\n",
      "Trained batch 95 batch loss 1.39400172 epoch total loss 1.39335954\n",
      "Trained batch 96 batch loss 1.37068176 epoch total loss 1.39312327\n",
      "Trained batch 97 batch loss 1.42868161 epoch total loss 1.39348984\n",
      "Trained batch 98 batch loss 1.49903822 epoch total loss 1.39456689\n",
      "Trained batch 99 batch loss 1.44427216 epoch total loss 1.395069\n",
      "Trained batch 100 batch loss 1.4422121 epoch total loss 1.39554048\n",
      "Trained batch 101 batch loss 1.37798667 epoch total loss 1.39536667\n",
      "Trained batch 102 batch loss 1.42268682 epoch total loss 1.39563453\n",
      "Trained batch 103 batch loss 1.50321 epoch total loss 1.39667892\n",
      "Trained batch 104 batch loss 1.29600596 epoch total loss 1.39571083\n",
      "Trained batch 105 batch loss 1.32448649 epoch total loss 1.39503264\n",
      "Trained batch 106 batch loss 1.4123019 epoch total loss 1.3951956\n",
      "Trained batch 107 batch loss 1.46040416 epoch total loss 1.395805\n",
      "Trained batch 108 batch loss 1.52141237 epoch total loss 1.39696801\n",
      "Trained batch 109 batch loss 1.58703685 epoch total loss 1.39871168\n",
      "Trained batch 110 batch loss 1.40469182 epoch total loss 1.39876616\n",
      "Trained batch 111 batch loss 1.43821752 epoch total loss 1.39912152\n",
      "Trained batch 112 batch loss 1.43065739 epoch total loss 1.39940321\n",
      "Trained batch 113 batch loss 1.50253201 epoch total loss 1.40031576\n",
      "Trained batch 114 batch loss 1.47432184 epoch total loss 1.40096498\n",
      "Trained batch 115 batch loss 1.43253934 epoch total loss 1.40123951\n",
      "Trained batch 116 batch loss 1.47428155 epoch total loss 1.4018693\n",
      "Trained batch 117 batch loss 1.39462662 epoch total loss 1.40180731\n",
      "Trained batch 118 batch loss 1.39693761 epoch total loss 1.40176606\n",
      "Trained batch 119 batch loss 1.41564596 epoch total loss 1.40188277\n",
      "Trained batch 120 batch loss 1.42795646 epoch total loss 1.40210009\n",
      "Trained batch 121 batch loss 1.5098114 epoch total loss 1.40299034\n",
      "Trained batch 122 batch loss 1.32073331 epoch total loss 1.40231609\n",
      "Trained batch 123 batch loss 1.29720235 epoch total loss 1.40146148\n",
      "Trained batch 124 batch loss 1.31660318 epoch total loss 1.4007771\n",
      "Trained batch 125 batch loss 1.31406033 epoch total loss 1.40008342\n",
      "Trained batch 126 batch loss 1.30297351 epoch total loss 1.39931273\n",
      "Trained batch 127 batch loss 1.32130122 epoch total loss 1.39869845\n",
      "Trained batch 128 batch loss 1.33408916 epoch total loss 1.39819372\n",
      "Trained batch 129 batch loss 1.29536986 epoch total loss 1.39739656\n",
      "Trained batch 130 batch loss 1.28572476 epoch total loss 1.39653754\n",
      "Trained batch 131 batch loss 1.44170988 epoch total loss 1.39688241\n",
      "Trained batch 132 batch loss 1.34282112 epoch total loss 1.39647281\n",
      "Trained batch 133 batch loss 1.34710062 epoch total loss 1.39610159\n",
      "Trained batch 134 batch loss 1.44238305 epoch total loss 1.39644706\n",
      "Trained batch 135 batch loss 1.36782598 epoch total loss 1.39623499\n",
      "Trained batch 136 batch loss 1.32222581 epoch total loss 1.3956908\n",
      "Trained batch 137 batch loss 1.42169142 epoch total loss 1.39588058\n",
      "Trained batch 138 batch loss 1.38798785 epoch total loss 1.39582336\n",
      "Trained batch 139 batch loss 1.37637842 epoch total loss 1.39568341\n",
      "Trained batch 140 batch loss 1.48366344 epoch total loss 1.39631188\n",
      "Trained batch 141 batch loss 1.44155848 epoch total loss 1.39663279\n",
      "Trained batch 142 batch loss 1.41801369 epoch total loss 1.39678335\n",
      "Trained batch 143 batch loss 1.48403454 epoch total loss 1.39739347\n",
      "Trained batch 144 batch loss 1.50875568 epoch total loss 1.39816689\n",
      "Trained batch 145 batch loss 1.45656645 epoch total loss 1.3985697\n",
      "Trained batch 146 batch loss 1.39446855 epoch total loss 1.39854157\n",
      "Trained batch 147 batch loss 1.40866113 epoch total loss 1.39861047\n",
      "Trained batch 148 batch loss 1.35398591 epoch total loss 1.39830899\n",
      "Trained batch 149 batch loss 1.27833235 epoch total loss 1.39750373\n",
      "Trained batch 150 batch loss 1.2776854 epoch total loss 1.39670491\n",
      "Trained batch 151 batch loss 1.44103074 epoch total loss 1.39699841\n",
      "Trained batch 152 batch loss 1.56515 epoch total loss 1.39810467\n",
      "Trained batch 153 batch loss 1.72630131 epoch total loss 1.40024984\n",
      "Trained batch 154 batch loss 1.56542468 epoch total loss 1.40132236\n",
      "Trained batch 155 batch loss 1.24117231 epoch total loss 1.40028918\n",
      "Trained batch 156 batch loss 1.35473824 epoch total loss 1.39999712\n",
      "Trained batch 157 batch loss 1.37965417 epoch total loss 1.39986753\n",
      "Trained batch 158 batch loss 1.39188778 epoch total loss 1.39981711\n",
      "Trained batch 159 batch loss 1.31657243 epoch total loss 1.39929354\n",
      "Trained batch 160 batch loss 1.46414375 epoch total loss 1.39969885\n",
      "Trained batch 161 batch loss 1.34339607 epoch total loss 1.39934909\n",
      "Trained batch 162 batch loss 1.56224215 epoch total loss 1.40035462\n",
      "Trained batch 163 batch loss 1.57219899 epoch total loss 1.40140891\n",
      "Trained batch 164 batch loss 1.45920753 epoch total loss 1.40176141\n",
      "Trained batch 165 batch loss 1.39336324 epoch total loss 1.40171051\n",
      "Trained batch 166 batch loss 1.36325443 epoch total loss 1.40147877\n",
      "Trained batch 167 batch loss 1.40607893 epoch total loss 1.4015063\n",
      "Trained batch 168 batch loss 1.47152603 epoch total loss 1.40192318\n",
      "Trained batch 169 batch loss 1.37204909 epoch total loss 1.40174639\n",
      "Trained batch 170 batch loss 1.31860185 epoch total loss 1.40125728\n",
      "Trained batch 171 batch loss 1.31549501 epoch total loss 1.40075576\n",
      "Trained batch 172 batch loss 1.33251333 epoch total loss 1.40035903\n",
      "Trained batch 173 batch loss 1.35655057 epoch total loss 1.40010583\n",
      "Trained batch 174 batch loss 1.38708007 epoch total loss 1.40003097\n",
      "Trained batch 175 batch loss 1.40364206 epoch total loss 1.40005159\n",
      "Trained batch 176 batch loss 1.55836463 epoch total loss 1.40095115\n",
      "Trained batch 177 batch loss 1.45156014 epoch total loss 1.40123701\n",
      "Trained batch 178 batch loss 1.50132215 epoch total loss 1.40179932\n",
      "Trained batch 179 batch loss 1.558429 epoch total loss 1.40267432\n",
      "Trained batch 180 batch loss 1.46495891 epoch total loss 1.40302038\n",
      "Trained batch 181 batch loss 1.37919378 epoch total loss 1.40288877\n",
      "Trained batch 182 batch loss 1.38423157 epoch total loss 1.40278625\n",
      "Trained batch 183 batch loss 1.47829652 epoch total loss 1.40319896\n",
      "Trained batch 184 batch loss 1.58356357 epoch total loss 1.4041791\n",
      "Trained batch 185 batch loss 1.54042768 epoch total loss 1.40491569\n",
      "Trained batch 186 batch loss 1.41973329 epoch total loss 1.40499532\n",
      "Trained batch 187 batch loss 1.36312556 epoch total loss 1.40477145\n",
      "Trained batch 188 batch loss 1.49791133 epoch total loss 1.405267\n",
      "Trained batch 189 batch loss 1.33577645 epoch total loss 1.40489936\n",
      "Trained batch 190 batch loss 1.52008021 epoch total loss 1.40550554\n",
      "Trained batch 191 batch loss 1.51333189 epoch total loss 1.40607011\n",
      "Trained batch 192 batch loss 1.49915707 epoch total loss 1.40655482\n",
      "Trained batch 193 batch loss 1.35626149 epoch total loss 1.40629423\n",
      "Trained batch 194 batch loss 1.31469202 epoch total loss 1.40582216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 195 batch loss 1.25635636 epoch total loss 1.40505564\n",
      "Trained batch 196 batch loss 1.33741248 epoch total loss 1.40471041\n",
      "Trained batch 197 batch loss 1.25515199 epoch total loss 1.40395129\n",
      "Trained batch 198 batch loss 1.28991318 epoch total loss 1.40337539\n",
      "Trained batch 199 batch loss 1.33635056 epoch total loss 1.40303862\n",
      "Trained batch 200 batch loss 1.42745781 epoch total loss 1.40316069\n",
      "Trained batch 201 batch loss 1.42402232 epoch total loss 1.4032644\n",
      "Trained batch 202 batch loss 1.44779873 epoch total loss 1.40348482\n",
      "Trained batch 203 batch loss 1.41697192 epoch total loss 1.40355122\n",
      "Trained batch 204 batch loss 1.43238163 epoch total loss 1.40369248\n",
      "Trained batch 205 batch loss 1.52702641 epoch total loss 1.40429425\n",
      "Trained batch 206 batch loss 1.44555104 epoch total loss 1.40449452\n",
      "Trained batch 207 batch loss 1.36423886 epoch total loss 1.4043\n",
      "Trained batch 208 batch loss 1.4546082 epoch total loss 1.40454185\n",
      "Trained batch 209 batch loss 1.5266546 epoch total loss 1.40512609\n",
      "Trained batch 210 batch loss 1.47332835 epoch total loss 1.40545082\n",
      "Trained batch 211 batch loss 1.43254828 epoch total loss 1.40557933\n",
      "Trained batch 212 batch loss 1.46204674 epoch total loss 1.40584564\n",
      "Trained batch 213 batch loss 1.53832722 epoch total loss 1.40646768\n",
      "Trained batch 214 batch loss 1.50947964 epoch total loss 1.40694904\n",
      "Trained batch 215 batch loss 1.50569558 epoch total loss 1.40740836\n",
      "Trained batch 216 batch loss 1.40087354 epoch total loss 1.4073782\n",
      "Trained batch 217 batch loss 1.34969878 epoch total loss 1.40711236\n",
      "Trained batch 218 batch loss 1.38125467 epoch total loss 1.40699375\n",
      "Trained batch 219 batch loss 1.41947567 epoch total loss 1.40705073\n",
      "Trained batch 220 batch loss 1.51213801 epoch total loss 1.4075284\n",
      "Trained batch 221 batch loss 1.41523683 epoch total loss 1.40756321\n",
      "Trained batch 222 batch loss 1.35663009 epoch total loss 1.40733373\n",
      "Trained batch 223 batch loss 1.4569521 epoch total loss 1.40755618\n",
      "Trained batch 224 batch loss 1.36569488 epoch total loss 1.40736938\n",
      "Trained batch 225 batch loss 1.44951189 epoch total loss 1.40755665\n",
      "Trained batch 226 batch loss 1.49878049 epoch total loss 1.4079603\n",
      "Trained batch 227 batch loss 1.36538792 epoch total loss 1.40777278\n",
      "Trained batch 228 batch loss 1.30854499 epoch total loss 1.40733755\n",
      "Trained batch 229 batch loss 1.43762374 epoch total loss 1.40746975\n",
      "Trained batch 230 batch loss 1.45200443 epoch total loss 1.40766335\n",
      "Trained batch 231 batch loss 1.42942953 epoch total loss 1.40775764\n",
      "Trained batch 232 batch loss 1.40333223 epoch total loss 1.40773857\n",
      "Trained batch 233 batch loss 1.44034815 epoch total loss 1.4078784\n",
      "Trained batch 234 batch loss 1.46180534 epoch total loss 1.40810883\n",
      "Trained batch 235 batch loss 1.31464076 epoch total loss 1.40771103\n",
      "Trained batch 236 batch loss 1.51022291 epoch total loss 1.40814543\n",
      "Trained batch 237 batch loss 1.42293048 epoch total loss 1.40820789\n",
      "Trained batch 238 batch loss 1.50643873 epoch total loss 1.4086206\n",
      "Trained batch 239 batch loss 1.26674426 epoch total loss 1.40802705\n",
      "Trained batch 240 batch loss 1.43424916 epoch total loss 1.40813625\n",
      "Trained batch 241 batch loss 1.44752562 epoch total loss 1.40829968\n",
      "Trained batch 242 batch loss 1.5719943 epoch total loss 1.4089762\n",
      "Trained batch 243 batch loss 1.60829091 epoch total loss 1.40979636\n",
      "Trained batch 244 batch loss 1.51501763 epoch total loss 1.41022754\n",
      "Trained batch 245 batch loss 1.57194567 epoch total loss 1.41088772\n",
      "Trained batch 246 batch loss 1.40971 epoch total loss 1.41088283\n",
      "Trained batch 247 batch loss 1.42608583 epoch total loss 1.41094434\n",
      "Trained batch 248 batch loss 1.17345047 epoch total loss 1.40998685\n",
      "Trained batch 249 batch loss 1.46529078 epoch total loss 1.41020894\n",
      "Trained batch 250 batch loss 1.4274981 epoch total loss 1.41027808\n",
      "Trained batch 251 batch loss 1.51797307 epoch total loss 1.41070712\n",
      "Trained batch 252 batch loss 1.3204391 epoch total loss 1.41034889\n",
      "Trained batch 253 batch loss 1.40971732 epoch total loss 1.41034651\n",
      "Trained batch 254 batch loss 1.34814513 epoch total loss 1.41010153\n",
      "Trained batch 255 batch loss 1.36849761 epoch total loss 1.40993845\n",
      "Trained batch 256 batch loss 1.26433015 epoch total loss 1.40936971\n",
      "Trained batch 257 batch loss 1.40256119 epoch total loss 1.40934324\n",
      "Trained batch 258 batch loss 1.36695981 epoch total loss 1.40917897\n",
      "Trained batch 259 batch loss 1.32048655 epoch total loss 1.4088366\n",
      "Trained batch 260 batch loss 1.40876222 epoch total loss 1.40883625\n",
      "Trained batch 261 batch loss 1.431741 epoch total loss 1.40892398\n",
      "Trained batch 262 batch loss 1.44446898 epoch total loss 1.40905964\n",
      "Trained batch 263 batch loss 1.43173718 epoch total loss 1.40914583\n",
      "Trained batch 264 batch loss 1.41202283 epoch total loss 1.40915668\n",
      "Trained batch 265 batch loss 1.43216276 epoch total loss 1.40924346\n",
      "Trained batch 266 batch loss 1.33745646 epoch total loss 1.40897369\n",
      "Trained batch 267 batch loss 1.38995123 epoch total loss 1.40890241\n",
      "Trained batch 268 batch loss 1.34176886 epoch total loss 1.40865195\n",
      "Trained batch 269 batch loss 1.48602128 epoch total loss 1.40893948\n",
      "Trained batch 270 batch loss 1.39253759 epoch total loss 1.4088788\n",
      "Trained batch 271 batch loss 1.57536018 epoch total loss 1.40949309\n",
      "Trained batch 272 batch loss 1.43762541 epoch total loss 1.40959644\n",
      "Trained batch 273 batch loss 1.50126839 epoch total loss 1.40993237\n",
      "Trained batch 274 batch loss 1.41360354 epoch total loss 1.40994573\n",
      "Trained batch 275 batch loss 1.44065535 epoch total loss 1.41005743\n",
      "Trained batch 276 batch loss 1.52099097 epoch total loss 1.41045928\n",
      "Trained batch 277 batch loss 1.44381011 epoch total loss 1.4105798\n",
      "Trained batch 278 batch loss 1.41025972 epoch total loss 1.41057861\n",
      "Trained batch 279 batch loss 1.40385985 epoch total loss 1.41055453\n",
      "Trained batch 280 batch loss 1.2913363 epoch total loss 1.41012883\n",
      "Trained batch 281 batch loss 1.37120533 epoch total loss 1.40999031\n",
      "Trained batch 282 batch loss 1.31422114 epoch total loss 1.40965068\n",
      "Trained batch 283 batch loss 1.54872 epoch total loss 1.41014206\n",
      "Trained batch 284 batch loss 1.20733142 epoch total loss 1.40942788\n",
      "Trained batch 285 batch loss 1.26045012 epoch total loss 1.40890515\n",
      "Trained batch 286 batch loss 1.20954514 epoch total loss 1.40820801\n",
      "Trained batch 287 batch loss 1.28818119 epoch total loss 1.40778983\n",
      "Trained batch 288 batch loss 1.38750625 epoch total loss 1.40771937\n",
      "Trained batch 289 batch loss 1.48143589 epoch total loss 1.40797448\n",
      "Trained batch 290 batch loss 1.59765053 epoch total loss 1.40862858\n",
      "Trained batch 291 batch loss 1.44380426 epoch total loss 1.40874946\n",
      "Trained batch 292 batch loss 1.31724644 epoch total loss 1.40843618\n",
      "Trained batch 293 batch loss 1.36059475 epoch total loss 1.40827286\n",
      "Trained batch 294 batch loss 1.44935095 epoch total loss 1.40841258\n",
      "Trained batch 295 batch loss 1.47188878 epoch total loss 1.40862775\n",
      "Trained batch 296 batch loss 1.5369283 epoch total loss 1.40906119\n",
      "Trained batch 297 batch loss 1.49694085 epoch total loss 1.40935719\n",
      "Trained batch 298 batch loss 1.54367149 epoch total loss 1.4098078\n",
      "Trained batch 299 batch loss 1.53829467 epoch total loss 1.41023755\n",
      "Trained batch 300 batch loss 1.52686965 epoch total loss 1.41062629\n",
      "Trained batch 301 batch loss 1.39733732 epoch total loss 1.41058218\n",
      "Trained batch 302 batch loss 1.30427718 epoch total loss 1.41023028\n",
      "Trained batch 303 batch loss 1.30431056 epoch total loss 1.40988064\n",
      "Trained batch 304 batch loss 1.27646923 epoch total loss 1.40944183\n",
      "Trained batch 305 batch loss 1.25388098 epoch total loss 1.40893173\n",
      "Trained batch 306 batch loss 1.30377364 epoch total loss 1.40858805\n",
      "Trained batch 307 batch loss 1.44109523 epoch total loss 1.40869403\n",
      "Trained batch 308 batch loss 1.47711897 epoch total loss 1.40891612\n",
      "Trained batch 309 batch loss 1.44400692 epoch total loss 1.40902972\n",
      "Trained batch 310 batch loss 1.53288484 epoch total loss 1.40942919\n",
      "Trained batch 311 batch loss 1.39795375 epoch total loss 1.40939236\n",
      "Trained batch 312 batch loss 1.50477624 epoch total loss 1.40969813\n",
      "Trained batch 313 batch loss 1.53754258 epoch total loss 1.41010654\n",
      "Trained batch 314 batch loss 1.46231782 epoch total loss 1.41027284\n",
      "Trained batch 315 batch loss 1.37379456 epoch total loss 1.41015708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 316 batch loss 1.35893571 epoch total loss 1.40999496\n",
      "Trained batch 317 batch loss 1.48649549 epoch total loss 1.41023624\n",
      "Trained batch 318 batch loss 1.44105649 epoch total loss 1.41033316\n",
      "Trained batch 319 batch loss 1.36699092 epoch total loss 1.41019738\n",
      "Trained batch 320 batch loss 1.36406446 epoch total loss 1.41005325\n",
      "Trained batch 321 batch loss 1.34437346 epoch total loss 1.40984857\n",
      "Trained batch 322 batch loss 1.30135608 epoch total loss 1.40951169\n",
      "Trained batch 323 batch loss 1.3072983 epoch total loss 1.4091953\n",
      "Trained batch 324 batch loss 1.38693404 epoch total loss 1.40912652\n",
      "Trained batch 325 batch loss 1.37297988 epoch total loss 1.40901542\n",
      "Trained batch 326 batch loss 1.35323358 epoch total loss 1.40884423\n",
      "Trained batch 327 batch loss 1.33497167 epoch total loss 1.40861833\n",
      "Trained batch 328 batch loss 1.28488636 epoch total loss 1.40824103\n",
      "Trained batch 329 batch loss 1.25377798 epoch total loss 1.40777159\n",
      "Trained batch 330 batch loss 1.19995439 epoch total loss 1.4071418\n",
      "Trained batch 331 batch loss 1.40280986 epoch total loss 1.40712869\n",
      "Trained batch 332 batch loss 1.33014095 epoch total loss 1.40689683\n",
      "Trained batch 333 batch loss 1.40182078 epoch total loss 1.40688157\n",
      "Trained batch 334 batch loss 1.4947921 epoch total loss 1.40714478\n",
      "Trained batch 335 batch loss 1.34513009 epoch total loss 1.40695965\n",
      "Trained batch 336 batch loss 1.35025167 epoch total loss 1.40679085\n",
      "Trained batch 337 batch loss 1.36940467 epoch total loss 1.40668\n",
      "Trained batch 338 batch loss 1.3380146 epoch total loss 1.40647686\n",
      "Trained batch 339 batch loss 1.42450786 epoch total loss 1.40653\n",
      "Trained batch 340 batch loss 1.32136095 epoch total loss 1.40627944\n",
      "Trained batch 341 batch loss 1.11003232 epoch total loss 1.40541077\n",
      "Trained batch 342 batch loss 1.04291868 epoch total loss 1.40435076\n",
      "Trained batch 343 batch loss 1.41313624 epoch total loss 1.40437639\n",
      "Trained batch 344 batch loss 1.46203041 epoch total loss 1.404544\n",
      "Trained batch 345 batch loss 1.65912652 epoch total loss 1.4052819\n",
      "Trained batch 346 batch loss 1.55538845 epoch total loss 1.40571582\n",
      "Trained batch 347 batch loss 1.38777018 epoch total loss 1.40566397\n",
      "Trained batch 348 batch loss 1.40791142 epoch total loss 1.4056704\n",
      "Trained batch 349 batch loss 1.3443017 epoch total loss 1.40549457\n",
      "Trained batch 350 batch loss 1.36238694 epoch total loss 1.40537143\n",
      "Trained batch 351 batch loss 1.46327972 epoch total loss 1.40553641\n",
      "Trained batch 352 batch loss 1.5518434 epoch total loss 1.4059521\n",
      "Trained batch 353 batch loss 1.51585793 epoch total loss 1.40626347\n",
      "Trained batch 354 batch loss 1.45912147 epoch total loss 1.40641272\n",
      "Trained batch 355 batch loss 1.42747474 epoch total loss 1.40647209\n",
      "Trained batch 356 batch loss 1.4245559 epoch total loss 1.40652287\n",
      "Trained batch 357 batch loss 1.41974711 epoch total loss 1.40655982\n",
      "Trained batch 358 batch loss 1.46167707 epoch total loss 1.40671384\n",
      "Trained batch 359 batch loss 1.43221116 epoch total loss 1.40678489\n",
      "Trained batch 360 batch loss 1.577739 epoch total loss 1.4072597\n",
      "Trained batch 361 batch loss 1.58510327 epoch total loss 1.40775239\n",
      "Trained batch 362 batch loss 1.59048533 epoch total loss 1.40825713\n",
      "Trained batch 363 batch loss 1.5135057 epoch total loss 1.40854716\n",
      "Trained batch 364 batch loss 1.45799327 epoch total loss 1.40868306\n",
      "Trained batch 365 batch loss 1.49907494 epoch total loss 1.40893066\n",
      "Trained batch 366 batch loss 1.52398121 epoch total loss 1.40924501\n",
      "Trained batch 367 batch loss 1.44330168 epoch total loss 1.40933788\n",
      "Trained batch 368 batch loss 1.33292031 epoch total loss 1.40913022\n",
      "Trained batch 369 batch loss 1.39959967 epoch total loss 1.40910447\n",
      "Trained batch 370 batch loss 1.43753362 epoch total loss 1.40918136\n",
      "Trained batch 371 batch loss 1.31694269 epoch total loss 1.40893281\n",
      "Trained batch 372 batch loss 1.31294286 epoch total loss 1.40867472\n",
      "Trained batch 373 batch loss 1.09480882 epoch total loss 1.4078331\n",
      "Trained batch 374 batch loss 1.32237077 epoch total loss 1.40760469\n",
      "Trained batch 375 batch loss 1.51157641 epoch total loss 1.40788198\n",
      "Trained batch 376 batch loss 1.58477914 epoch total loss 1.40835249\n",
      "Trained batch 377 batch loss 1.39819145 epoch total loss 1.40832555\n",
      "Trained batch 378 batch loss 1.41394675 epoch total loss 1.40834033\n",
      "Trained batch 379 batch loss 1.37614822 epoch total loss 1.40825546\n",
      "Trained batch 380 batch loss 1.3611021 epoch total loss 1.40813136\n",
      "Trained batch 381 batch loss 1.35462332 epoch total loss 1.40799081\n",
      "Trained batch 382 batch loss 1.35093653 epoch total loss 1.40784156\n",
      "Trained batch 383 batch loss 1.39926136 epoch total loss 1.40781903\n",
      "Trained batch 384 batch loss 1.38325799 epoch total loss 1.40775502\n",
      "Trained batch 385 batch loss 1.38811958 epoch total loss 1.40770411\n",
      "Trained batch 386 batch loss 1.42474508 epoch total loss 1.40774822\n",
      "Trained batch 387 batch loss 1.31669819 epoch total loss 1.40751302\n",
      "Trained batch 388 batch loss 1.36687672 epoch total loss 1.40740824\n",
      "Trained batch 389 batch loss 1.33791888 epoch total loss 1.40722954\n",
      "Trained batch 390 batch loss 1.25545979 epoch total loss 1.40684032\n",
      "Trained batch 391 batch loss 1.19000196 epoch total loss 1.40628576\n",
      "Trained batch 392 batch loss 1.18489742 epoch total loss 1.40572095\n",
      "Trained batch 393 batch loss 1.3102634 epoch total loss 1.405478\n",
      "Trained batch 394 batch loss 1.30494952 epoch total loss 1.40522277\n",
      "Trained batch 395 batch loss 1.37199879 epoch total loss 1.40513873\n",
      "Trained batch 396 batch loss 1.37346351 epoch total loss 1.40505874\n",
      "Trained batch 397 batch loss 1.3241564 epoch total loss 1.40485501\n",
      "Trained batch 398 batch loss 1.40541291 epoch total loss 1.40485632\n",
      "Trained batch 399 batch loss 1.42782307 epoch total loss 1.40491378\n",
      "Trained batch 400 batch loss 1.32706547 epoch total loss 1.40471923\n",
      "Trained batch 401 batch loss 1.40807271 epoch total loss 1.40472758\n",
      "Trained batch 402 batch loss 1.17721486 epoch total loss 1.40416157\n",
      "Trained batch 403 batch loss 1.33752143 epoch total loss 1.40399623\n",
      "Trained batch 404 batch loss 1.34869075 epoch total loss 1.40385938\n",
      "Trained batch 405 batch loss 1.32149911 epoch total loss 1.40365589\n",
      "Trained batch 406 batch loss 1.40918839 epoch total loss 1.40366948\n",
      "Trained batch 407 batch loss 1.52242088 epoch total loss 1.4039613\n",
      "Trained batch 408 batch loss 1.6203258 epoch total loss 1.40449154\n",
      "Trained batch 409 batch loss 1.46635485 epoch total loss 1.40464282\n",
      "Trained batch 410 batch loss 1.34194148 epoch total loss 1.40448976\n",
      "Trained batch 411 batch loss 1.35785341 epoch total loss 1.40437627\n",
      "Trained batch 412 batch loss 1.26466155 epoch total loss 1.40403724\n",
      "Trained batch 413 batch loss 1.40729249 epoch total loss 1.4040451\n",
      "Trained batch 414 batch loss 1.39647818 epoch total loss 1.40402675\n",
      "Trained batch 415 batch loss 1.40717363 epoch total loss 1.40403438\n",
      "Trained batch 416 batch loss 1.32203543 epoch total loss 1.4038372\n",
      "Trained batch 417 batch loss 1.37904191 epoch total loss 1.40377772\n",
      "Trained batch 418 batch loss 1.57917762 epoch total loss 1.40419734\n",
      "Trained batch 419 batch loss 1.55679965 epoch total loss 1.40456152\n",
      "Trained batch 420 batch loss 1.51632595 epoch total loss 1.40482759\n",
      "Trained batch 421 batch loss 1.43983471 epoch total loss 1.40491068\n",
      "Trained batch 422 batch loss 1.5016551 epoch total loss 1.40513992\n",
      "Trained batch 423 batch loss 1.4424361 epoch total loss 1.40522814\n",
      "Trained batch 424 batch loss 1.46981323 epoch total loss 1.40538037\n",
      "Trained batch 425 batch loss 1.42485833 epoch total loss 1.40542626\n",
      "Trained batch 426 batch loss 1.44840777 epoch total loss 1.40552723\n",
      "Trained batch 427 batch loss 1.34201872 epoch total loss 1.40537846\n",
      "Trained batch 428 batch loss 1.46437526 epoch total loss 1.40551627\n",
      "Trained batch 429 batch loss 1.4312489 epoch total loss 1.40557635\n",
      "Trained batch 430 batch loss 1.48327327 epoch total loss 1.40575707\n",
      "Trained batch 431 batch loss 1.36764312 epoch total loss 1.4056685\n",
      "Trained batch 432 batch loss 1.37051225 epoch total loss 1.40558708\n",
      "Trained batch 433 batch loss 1.33010793 epoch total loss 1.40541267\n",
      "Trained batch 434 batch loss 1.35543287 epoch total loss 1.40529752\n",
      "Trained batch 435 batch loss 1.43613601 epoch total loss 1.40536845\n",
      "Trained batch 436 batch loss 1.3620702 epoch total loss 1.40526915\n",
      "Trained batch 437 batch loss 1.36242187 epoch total loss 1.40517104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 438 batch loss 1.44607759 epoch total loss 1.4052645\n",
      "Trained batch 439 batch loss 1.24390721 epoch total loss 1.40489697\n",
      "Trained batch 440 batch loss 1.20349514 epoch total loss 1.40443921\n",
      "Trained batch 441 batch loss 1.364586 epoch total loss 1.40434873\n",
      "Trained batch 442 batch loss 1.41423428 epoch total loss 1.40437114\n",
      "Trained batch 443 batch loss 1.48098314 epoch total loss 1.404544\n",
      "Trained batch 444 batch loss 1.41873991 epoch total loss 1.40457606\n",
      "Trained batch 445 batch loss 1.52132297 epoch total loss 1.40483832\n",
      "Trained batch 446 batch loss 1.52516913 epoch total loss 1.40510809\n",
      "Trained batch 447 batch loss 1.42056823 epoch total loss 1.40514278\n",
      "Trained batch 448 batch loss 1.42347789 epoch total loss 1.40518367\n",
      "Trained batch 449 batch loss 1.37564635 epoch total loss 1.40511787\n",
      "Trained batch 450 batch loss 1.4958322 epoch total loss 1.40531957\n",
      "Trained batch 451 batch loss 1.3594141 epoch total loss 1.40521777\n",
      "Trained batch 452 batch loss 1.40230656 epoch total loss 1.40521133\n",
      "Trained batch 453 batch loss 1.57058334 epoch total loss 1.40557635\n",
      "Trained batch 454 batch loss 1.35236609 epoch total loss 1.40545905\n",
      "Trained batch 455 batch loss 1.43225443 epoch total loss 1.40551794\n",
      "Trained batch 456 batch loss 1.36695 epoch total loss 1.40543342\n",
      "Trained batch 457 batch loss 1.45420671 epoch total loss 1.40554011\n",
      "Trained batch 458 batch loss 1.33296919 epoch total loss 1.40538168\n",
      "Trained batch 459 batch loss 1.43001103 epoch total loss 1.40543532\n",
      "Trained batch 460 batch loss 1.37869 epoch total loss 1.40537703\n",
      "Trained batch 461 batch loss 1.50449443 epoch total loss 1.40559208\n",
      "Trained batch 462 batch loss 1.39377379 epoch total loss 1.40556657\n",
      "Trained batch 463 batch loss 1.37199974 epoch total loss 1.40549409\n",
      "Trained batch 464 batch loss 1.50504661 epoch total loss 1.40570867\n",
      "Trained batch 465 batch loss 1.49726868 epoch total loss 1.4059056\n",
      "Trained batch 466 batch loss 1.4468478 epoch total loss 1.40599346\n",
      "Trained batch 467 batch loss 1.35256481 epoch total loss 1.4058789\n",
      "Trained batch 468 batch loss 1.28678656 epoch total loss 1.40562451\n",
      "Trained batch 469 batch loss 1.38221943 epoch total loss 1.40557456\n",
      "Trained batch 470 batch loss 1.35302258 epoch total loss 1.40546274\n",
      "Trained batch 471 batch loss 1.39421296 epoch total loss 1.4054389\n",
      "Trained batch 472 batch loss 1.40752518 epoch total loss 1.40544331\n",
      "Trained batch 473 batch loss 1.30414879 epoch total loss 1.40522921\n",
      "Trained batch 474 batch loss 1.37188935 epoch total loss 1.40515888\n",
      "Trained batch 475 batch loss 1.26193178 epoch total loss 1.40485728\n",
      "Trained batch 476 batch loss 1.33760297 epoch total loss 1.4047159\n",
      "Trained batch 477 batch loss 1.37023067 epoch total loss 1.40464365\n",
      "Trained batch 478 batch loss 1.54863763 epoch total loss 1.4049449\n",
      "Trained batch 479 batch loss 1.53292489 epoch total loss 1.40521204\n",
      "Trained batch 480 batch loss 1.44665444 epoch total loss 1.40529835\n",
      "Trained batch 481 batch loss 1.53974986 epoch total loss 1.4055779\n",
      "Trained batch 482 batch loss 1.37887251 epoch total loss 1.40552235\n",
      "Trained batch 483 batch loss 1.45727265 epoch total loss 1.40562952\n",
      "Trained batch 484 batch loss 1.36510646 epoch total loss 1.40554583\n",
      "Trained batch 485 batch loss 1.30192816 epoch total loss 1.40533221\n",
      "Trained batch 486 batch loss 1.36630976 epoch total loss 1.40525198\n",
      "Trained batch 487 batch loss 1.40062821 epoch total loss 1.40524244\n",
      "Trained batch 488 batch loss 1.37519407 epoch total loss 1.40518093\n",
      "Trained batch 489 batch loss 1.49941909 epoch total loss 1.40537357\n",
      "Trained batch 490 batch loss 1.44813156 epoch total loss 1.40546083\n",
      "Trained batch 491 batch loss 1.43125439 epoch total loss 1.40551341\n",
      "Trained batch 492 batch loss 1.54077947 epoch total loss 1.4057883\n",
      "Trained batch 493 batch loss 1.3456111 epoch total loss 1.40566611\n",
      "Trained batch 494 batch loss 1.35999727 epoch total loss 1.40557373\n",
      "Trained batch 495 batch loss 1.29134095 epoch total loss 1.40534282\n",
      "Trained batch 496 batch loss 1.40875387 epoch total loss 1.40534973\n",
      "Trained batch 497 batch loss 1.40377724 epoch total loss 1.40534651\n",
      "Trained batch 498 batch loss 1.33550406 epoch total loss 1.40520632\n",
      "Trained batch 499 batch loss 1.29183555 epoch total loss 1.40497899\n",
      "Trained batch 500 batch loss 1.28353143 epoch total loss 1.40473604\n",
      "Trained batch 501 batch loss 1.29084253 epoch total loss 1.40450871\n",
      "Trained batch 502 batch loss 1.3704896 epoch total loss 1.404441\n",
      "Trained batch 503 batch loss 1.30131423 epoch total loss 1.40423596\n",
      "Trained batch 504 batch loss 1.30463731 epoch total loss 1.40403831\n",
      "Trained batch 505 batch loss 1.48430097 epoch total loss 1.40419734\n",
      "Trained batch 506 batch loss 1.46006668 epoch total loss 1.40430772\n",
      "Trained batch 507 batch loss 1.36028981 epoch total loss 1.40422094\n",
      "Trained batch 508 batch loss 1.45755863 epoch total loss 1.40432596\n",
      "Trained batch 509 batch loss 1.34564686 epoch total loss 1.40421069\n",
      "Trained batch 510 batch loss 1.37139201 epoch total loss 1.40414631\n",
      "Trained batch 511 batch loss 1.18026841 epoch total loss 1.40370822\n",
      "Trained batch 512 batch loss 1.2849164 epoch total loss 1.40347624\n",
      "Trained batch 513 batch loss 1.37863028 epoch total loss 1.40342772\n",
      "Trained batch 514 batch loss 1.4410404 epoch total loss 1.40350091\n",
      "Trained batch 515 batch loss 1.4208312 epoch total loss 1.40353453\n",
      "Trained batch 516 batch loss 1.34588444 epoch total loss 1.40342283\n",
      "Trained batch 517 batch loss 1.33383107 epoch total loss 1.40328825\n",
      "Trained batch 518 batch loss 1.33608794 epoch total loss 1.40315843\n",
      "Trained batch 519 batch loss 1.4042573 epoch total loss 1.40316045\n",
      "Trained batch 520 batch loss 1.09337711 epoch total loss 1.40256476\n",
      "Trained batch 521 batch loss 1.49211693 epoch total loss 1.40273666\n",
      "Trained batch 522 batch loss 1.43553615 epoch total loss 1.40279949\n",
      "Trained batch 523 batch loss 1.32303774 epoch total loss 1.40264702\n",
      "Trained batch 524 batch loss 1.15337968 epoch total loss 1.40217137\n",
      "Trained batch 525 batch loss 1.3144145 epoch total loss 1.40200412\n",
      "Trained batch 526 batch loss 1.40369058 epoch total loss 1.40200734\n",
      "Trained batch 527 batch loss 1.43838668 epoch total loss 1.40207648\n",
      "Trained batch 528 batch loss 1.39181399 epoch total loss 1.40205693\n",
      "Trained batch 529 batch loss 1.30277729 epoch total loss 1.4018693\n",
      "Trained batch 530 batch loss 1.40828204 epoch total loss 1.40188134\n",
      "Trained batch 531 batch loss 1.41091502 epoch total loss 1.40189838\n",
      "Trained batch 532 batch loss 1.44021559 epoch total loss 1.40197027\n",
      "Trained batch 533 batch loss 1.25645101 epoch total loss 1.40169728\n",
      "Trained batch 534 batch loss 1.30610597 epoch total loss 1.40151834\n",
      "Trained batch 535 batch loss 1.40768623 epoch total loss 1.40152991\n",
      "Trained batch 536 batch loss 1.32868755 epoch total loss 1.40139389\n",
      "Trained batch 537 batch loss 1.34147835 epoch total loss 1.40128243\n",
      "Trained batch 538 batch loss 1.43760538 epoch total loss 1.4013499\n",
      "Trained batch 539 batch loss 1.32482541 epoch total loss 1.40120792\n",
      "Trained batch 540 batch loss 1.41869295 epoch total loss 1.40124035\n",
      "Trained batch 541 batch loss 1.39761186 epoch total loss 1.40123355\n",
      "Trained batch 542 batch loss 1.35576081 epoch total loss 1.40114975\n",
      "Trained batch 543 batch loss 1.31914985 epoch total loss 1.40099871\n",
      "Trained batch 544 batch loss 1.50473106 epoch total loss 1.40118945\n",
      "Trained batch 545 batch loss 1.43886137 epoch total loss 1.40125859\n",
      "Trained batch 546 batch loss 1.48497772 epoch total loss 1.40141189\n",
      "Trained batch 547 batch loss 1.43571329 epoch total loss 1.4014746\n",
      "Trained batch 548 batch loss 1.4735744 epoch total loss 1.4016062\n",
      "Trained batch 549 batch loss 1.40141308 epoch total loss 1.40160584\n",
      "Trained batch 550 batch loss 1.3013382 epoch total loss 1.40142357\n",
      "Trained batch 551 batch loss 1.39671326 epoch total loss 1.40141499\n",
      "Trained batch 552 batch loss 1.46132541 epoch total loss 1.40152359\n",
      "Trained batch 553 batch loss 1.34445143 epoch total loss 1.40142024\n",
      "Trained batch 554 batch loss 1.33978844 epoch total loss 1.40130901\n",
      "Trained batch 555 batch loss 1.34497416 epoch total loss 1.40120745\n",
      "Trained batch 556 batch loss 1.25230122 epoch total loss 1.4009397\n",
      "Trained batch 557 batch loss 1.38618898 epoch total loss 1.40091324\n",
      "Trained batch 558 batch loss 1.22474241 epoch total loss 1.40059745\n",
      "Trained batch 559 batch loss 1.18267488 epoch total loss 1.40020764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 560 batch loss 1.34100699 epoch total loss 1.4001019\n",
      "Trained batch 561 batch loss 1.39483082 epoch total loss 1.40009248\n",
      "Trained batch 562 batch loss 1.50001669 epoch total loss 1.40027034\n",
      "Trained batch 563 batch loss 1.51558471 epoch total loss 1.40047503\n",
      "Trained batch 564 batch loss 1.58660626 epoch total loss 1.40080512\n",
      "Trained batch 565 batch loss 1.14730299 epoch total loss 1.40035641\n",
      "Trained batch 566 batch loss 1.55280817 epoch total loss 1.40062571\n",
      "Trained batch 567 batch loss 1.4768039 epoch total loss 1.40076\n",
      "Trained batch 568 batch loss 1.35626984 epoch total loss 1.40068173\n",
      "Trained batch 569 batch loss 1.56032395 epoch total loss 1.40096223\n",
      "Trained batch 570 batch loss 1.54207063 epoch total loss 1.40120983\n",
      "Trained batch 571 batch loss 1.36608601 epoch total loss 1.40114832\n",
      "Trained batch 572 batch loss 1.3453393 epoch total loss 1.40105069\n",
      "Trained batch 573 batch loss 1.32283521 epoch total loss 1.40091419\n",
      "Trained batch 574 batch loss 1.3586936 epoch total loss 1.40084064\n",
      "Trained batch 575 batch loss 1.38180542 epoch total loss 1.40080762\n",
      "Trained batch 576 batch loss 1.39292598 epoch total loss 1.40079391\n",
      "Trained batch 577 batch loss 1.49944389 epoch total loss 1.40096486\n",
      "Trained batch 578 batch loss 1.49330211 epoch total loss 1.4011246\n",
      "Trained batch 579 batch loss 1.5576005 epoch total loss 1.40139496\n",
      "Trained batch 580 batch loss 1.5390048 epoch total loss 1.40163219\n",
      "Trained batch 581 batch loss 1.49223411 epoch total loss 1.40178812\n",
      "Trained batch 582 batch loss 1.50312185 epoch total loss 1.40196228\n",
      "Trained batch 583 batch loss 1.43183541 epoch total loss 1.40201342\n",
      "Trained batch 584 batch loss 1.47997046 epoch total loss 1.40214694\n",
      "Trained batch 585 batch loss 1.45462394 epoch total loss 1.4022367\n",
      "Trained batch 586 batch loss 1.36284685 epoch total loss 1.40216947\n",
      "Trained batch 587 batch loss 1.41809976 epoch total loss 1.40219665\n",
      "Trained batch 588 batch loss 1.40576124 epoch total loss 1.40220273\n",
      "Trained batch 589 batch loss 1.32738793 epoch total loss 1.40207565\n",
      "Trained batch 590 batch loss 1.34298205 epoch total loss 1.40197551\n",
      "Trained batch 591 batch loss 1.41873717 epoch total loss 1.40200388\n",
      "Trained batch 592 batch loss 1.38633311 epoch total loss 1.40197742\n",
      "Trained batch 593 batch loss 1.49533868 epoch total loss 1.4021349\n",
      "Trained batch 594 batch loss 1.30564737 epoch total loss 1.40197253\n",
      "Trained batch 595 batch loss 1.33443761 epoch total loss 1.40185893\n",
      "Trained batch 596 batch loss 1.37756157 epoch total loss 1.40181816\n",
      "Trained batch 597 batch loss 1.32730889 epoch total loss 1.40169346\n",
      "Trained batch 598 batch loss 1.30804455 epoch total loss 1.40153682\n",
      "Trained batch 599 batch loss 1.3284862 epoch total loss 1.40141487\n",
      "Trained batch 600 batch loss 1.40955794 epoch total loss 1.40142846\n",
      "Trained batch 601 batch loss 1.47631049 epoch total loss 1.40155303\n",
      "Trained batch 602 batch loss 1.43446159 epoch total loss 1.40160763\n",
      "Trained batch 603 batch loss 1.38333809 epoch total loss 1.40157747\n",
      "Trained batch 604 batch loss 1.3880527 epoch total loss 1.40155506\n",
      "Trained batch 605 batch loss 1.42911971 epoch total loss 1.4016006\n",
      "Trained batch 606 batch loss 1.46089673 epoch total loss 1.40169847\n",
      "Trained batch 607 batch loss 1.27568007 epoch total loss 1.40149081\n",
      "Trained batch 608 batch loss 1.32141757 epoch total loss 1.4013592\n",
      "Trained batch 609 batch loss 1.37695849 epoch total loss 1.40131903\n",
      "Trained batch 610 batch loss 1.42218852 epoch total loss 1.40135324\n",
      "Trained batch 611 batch loss 1.38563979 epoch total loss 1.40132749\n",
      "Trained batch 612 batch loss 1.35219145 epoch total loss 1.40124726\n",
      "Trained batch 613 batch loss 1.2621727 epoch total loss 1.40102029\n",
      "Trained batch 614 batch loss 1.36911833 epoch total loss 1.40096831\n",
      "Trained batch 615 batch loss 1.40646768 epoch total loss 1.40097737\n",
      "Trained batch 616 batch loss 1.35740042 epoch total loss 1.40090668\n",
      "Trained batch 617 batch loss 1.32234025 epoch total loss 1.40077925\n",
      "Trained batch 618 batch loss 1.37693965 epoch total loss 1.40074074\n",
      "Trained batch 619 batch loss 1.37624335 epoch total loss 1.40070117\n",
      "Trained batch 620 batch loss 1.42345095 epoch total loss 1.40073788\n",
      "Trained batch 621 batch loss 1.3299334 epoch total loss 1.4006238\n",
      "Trained batch 622 batch loss 1.39231968 epoch total loss 1.40061057\n",
      "Trained batch 623 batch loss 1.32993221 epoch total loss 1.40049708\n",
      "Trained batch 624 batch loss 1.50657701 epoch total loss 1.40066719\n",
      "Trained batch 625 batch loss 1.41345429 epoch total loss 1.40068758\n",
      "Trained batch 626 batch loss 1.37903559 epoch total loss 1.400653\n",
      "Trained batch 627 batch loss 1.29422927 epoch total loss 1.40048325\n",
      "Trained batch 628 batch loss 1.37267423 epoch total loss 1.40043902\n",
      "Trained batch 629 batch loss 1.3621223 epoch total loss 1.40037811\n",
      "Trained batch 630 batch loss 1.35602272 epoch total loss 1.40030766\n",
      "Trained batch 631 batch loss 1.3679986 epoch total loss 1.40025651\n",
      "Trained batch 632 batch loss 1.40180933 epoch total loss 1.4002589\n",
      "Trained batch 633 batch loss 1.41180956 epoch total loss 1.40027714\n",
      "Trained batch 634 batch loss 1.31573284 epoch total loss 1.40014374\n",
      "Trained batch 635 batch loss 1.34922564 epoch total loss 1.40006363\n",
      "Trained batch 636 batch loss 1.27155197 epoch total loss 1.39986157\n",
      "Trained batch 637 batch loss 1.39727068 epoch total loss 1.39985752\n",
      "Trained batch 638 batch loss 1.29444218 epoch total loss 1.3996923\n",
      "Trained batch 639 batch loss 1.29456151 epoch total loss 1.39952767\n",
      "Trained batch 640 batch loss 1.36988139 epoch total loss 1.39948142\n",
      "Trained batch 641 batch loss 1.29672933 epoch total loss 1.39932108\n",
      "Trained batch 642 batch loss 1.30256522 epoch total loss 1.3991704\n",
      "Trained batch 643 batch loss 1.27898526 epoch total loss 1.39898348\n",
      "Trained batch 644 batch loss 1.29585648 epoch total loss 1.39882338\n",
      "Trained batch 645 batch loss 1.33482695 epoch total loss 1.39872408\n",
      "Trained batch 646 batch loss 1.25816417 epoch total loss 1.39850652\n",
      "Trained batch 647 batch loss 1.34184027 epoch total loss 1.39841902\n",
      "Trained batch 648 batch loss 1.3156023 epoch total loss 1.39829123\n",
      "Trained batch 649 batch loss 1.28765118 epoch total loss 1.39812076\n",
      "Trained batch 650 batch loss 1.43543279 epoch total loss 1.3981781\n",
      "Trained batch 651 batch loss 1.45851791 epoch total loss 1.39827085\n",
      "Trained batch 652 batch loss 1.44122744 epoch total loss 1.39833665\n",
      "Trained batch 653 batch loss 1.37067473 epoch total loss 1.39829433\n",
      "Trained batch 654 batch loss 1.19093251 epoch total loss 1.39797723\n",
      "Trained batch 655 batch loss 1.27805388 epoch total loss 1.39779413\n",
      "Trained batch 656 batch loss 1.37041688 epoch total loss 1.3977524\n",
      "Trained batch 657 batch loss 1.23512721 epoch total loss 1.39750493\n",
      "Trained batch 658 batch loss 1.34932506 epoch total loss 1.39743161\n",
      "Trained batch 659 batch loss 1.44868934 epoch total loss 1.39750934\n",
      "Trained batch 660 batch loss 1.35208249 epoch total loss 1.39744055\n",
      "Trained batch 661 batch loss 1.3499732 epoch total loss 1.39736879\n",
      "Trained batch 662 batch loss 1.32979679 epoch total loss 1.39726663\n",
      "Trained batch 663 batch loss 1.27818668 epoch total loss 1.3970871\n",
      "Trained batch 664 batch loss 1.31967163 epoch total loss 1.39697039\n",
      "Trained batch 665 batch loss 1.34384775 epoch total loss 1.39689064\n",
      "Trained batch 666 batch loss 1.24537814 epoch total loss 1.39666307\n",
      "Trained batch 667 batch loss 1.40498888 epoch total loss 1.39667559\n",
      "Trained batch 668 batch loss 1.33754897 epoch total loss 1.39658701\n",
      "Trained batch 669 batch loss 1.35103965 epoch total loss 1.39651883\n",
      "Trained batch 670 batch loss 1.34437346 epoch total loss 1.39644098\n",
      "Trained batch 671 batch loss 1.2941798 epoch total loss 1.39628863\n",
      "Trained batch 672 batch loss 1.22413516 epoch total loss 1.39603245\n",
      "Trained batch 673 batch loss 1.15000165 epoch total loss 1.39566684\n",
      "Trained batch 674 batch loss 1.28593254 epoch total loss 1.39550412\n",
      "Trained batch 675 batch loss 1.56481647 epoch total loss 1.39575493\n",
      "Trained batch 676 batch loss 1.47003317 epoch total loss 1.39586484\n",
      "Trained batch 677 batch loss 1.49019766 epoch total loss 1.39600408\n",
      "Trained batch 678 batch loss 1.4620651 epoch total loss 1.39610147\n",
      "Trained batch 679 batch loss 1.43458724 epoch total loss 1.39615822\n",
      "Trained batch 680 batch loss 1.35682666 epoch total loss 1.39610028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 681 batch loss 1.29089594 epoch total loss 1.39594579\n",
      "Trained batch 682 batch loss 1.33420801 epoch total loss 1.39585531\n",
      "Trained batch 683 batch loss 1.42771304 epoch total loss 1.39590204\n",
      "Trained batch 684 batch loss 1.39974642 epoch total loss 1.39590752\n",
      "Trained batch 685 batch loss 1.38257432 epoch total loss 1.39588809\n",
      "Trained batch 686 batch loss 1.41669536 epoch total loss 1.39591837\n",
      "Trained batch 687 batch loss 1.35604358 epoch total loss 1.39586031\n",
      "Trained batch 688 batch loss 1.43432283 epoch total loss 1.39591622\n",
      "Trained batch 689 batch loss 1.47789502 epoch total loss 1.39603519\n",
      "Trained batch 690 batch loss 1.54190397 epoch total loss 1.39624667\n",
      "Trained batch 691 batch loss 1.45189023 epoch total loss 1.39632726\n",
      "Trained batch 692 batch loss 1.47651601 epoch total loss 1.39644313\n",
      "Trained batch 693 batch loss 1.47877824 epoch total loss 1.39656186\n",
      "Trained batch 694 batch loss 1.45110059 epoch total loss 1.39664054\n",
      "Trained batch 695 batch loss 1.38773644 epoch total loss 1.39662766\n",
      "Trained batch 696 batch loss 1.4055357 epoch total loss 1.39664042\n",
      "Trained batch 697 batch loss 1.38939309 epoch total loss 1.39663\n",
      "Trained batch 698 batch loss 1.39021087 epoch total loss 1.39662087\n",
      "Trained batch 699 batch loss 1.30448461 epoch total loss 1.39648914\n",
      "Trained batch 700 batch loss 1.41623032 epoch total loss 1.39651728\n",
      "Trained batch 701 batch loss 1.37754536 epoch total loss 1.39649034\n",
      "Trained batch 702 batch loss 1.43989539 epoch total loss 1.39655209\n",
      "Trained batch 703 batch loss 1.36610138 epoch total loss 1.39650881\n",
      "Trained batch 704 batch loss 1.30014467 epoch total loss 1.39637196\n",
      "Trained batch 705 batch loss 1.35862958 epoch total loss 1.39631844\n",
      "Trained batch 706 batch loss 1.42125428 epoch total loss 1.39635372\n",
      "Trained batch 707 batch loss 1.45856857 epoch total loss 1.3964417\n",
      "Trained batch 708 batch loss 1.42116165 epoch total loss 1.39647663\n",
      "Trained batch 709 batch loss 1.33694649 epoch total loss 1.3963927\n",
      "Trained batch 710 batch loss 1.27724361 epoch total loss 1.39622486\n",
      "Trained batch 711 batch loss 1.26310205 epoch total loss 1.3960377\n",
      "Trained batch 712 batch loss 1.39291692 epoch total loss 1.39603329\n",
      "Trained batch 713 batch loss 1.24772644 epoch total loss 1.39582527\n",
      "Trained batch 714 batch loss 1.37661529 epoch total loss 1.39579833\n",
      "Trained batch 715 batch loss 1.45178056 epoch total loss 1.39587665\n",
      "Trained batch 716 batch loss 1.44479513 epoch total loss 1.39594507\n",
      "Trained batch 717 batch loss 1.4625169 epoch total loss 1.39603794\n",
      "Trained batch 718 batch loss 1.36663818 epoch total loss 1.39599693\n",
      "Trained batch 719 batch loss 1.37302756 epoch total loss 1.39596498\n",
      "Trained batch 720 batch loss 1.36778212 epoch total loss 1.39592588\n",
      "Trained batch 721 batch loss 1.46684074 epoch total loss 1.39602423\n",
      "Trained batch 722 batch loss 1.44951367 epoch total loss 1.39609838\n",
      "Trained batch 723 batch loss 1.31372976 epoch total loss 1.39598441\n",
      "Trained batch 724 batch loss 1.26408088 epoch total loss 1.39580226\n",
      "Trained batch 725 batch loss 1.27018464 epoch total loss 1.39562905\n",
      "Trained batch 726 batch loss 1.25949836 epoch total loss 1.39544153\n",
      "Trained batch 727 batch loss 1.32506037 epoch total loss 1.39534473\n",
      "Trained batch 728 batch loss 1.35429251 epoch total loss 1.39528835\n",
      "Trained batch 729 batch loss 1.45846581 epoch total loss 1.39537513\n",
      "Trained batch 730 batch loss 1.68441272 epoch total loss 1.39577103\n",
      "Trained batch 731 batch loss 1.46566248 epoch total loss 1.39586663\n",
      "Trained batch 732 batch loss 1.52438283 epoch total loss 1.39604211\n",
      "Trained batch 733 batch loss 1.35199499 epoch total loss 1.39598203\n",
      "Trained batch 734 batch loss 1.34595835 epoch total loss 1.39591396\n",
      "Trained batch 735 batch loss 1.10719395 epoch total loss 1.39552104\n",
      "Trained batch 736 batch loss 1.09103644 epoch total loss 1.39510739\n",
      "Trained batch 737 batch loss 1.19230747 epoch total loss 1.39483225\n",
      "Trained batch 738 batch loss 1.19269609 epoch total loss 1.39455843\n",
      "Trained batch 739 batch loss 1.31441832 epoch total loss 1.39445\n",
      "Trained batch 740 batch loss 1.48673451 epoch total loss 1.39457464\n",
      "Trained batch 741 batch loss 1.32443047 epoch total loss 1.39448\n",
      "Trained batch 742 batch loss 1.28683138 epoch total loss 1.39433503\n",
      "Trained batch 743 batch loss 1.17695379 epoch total loss 1.39404249\n",
      "Trained batch 744 batch loss 1.36179209 epoch total loss 1.39399922\n",
      "Trained batch 745 batch loss 1.35203087 epoch total loss 1.39394283\n",
      "Trained batch 746 batch loss 1.47457862 epoch total loss 1.39405096\n",
      "Trained batch 747 batch loss 1.38435745 epoch total loss 1.39403808\n",
      "Trained batch 748 batch loss 1.34731901 epoch total loss 1.39397562\n",
      "Trained batch 749 batch loss 1.37800264 epoch total loss 1.39395428\n",
      "Trained batch 750 batch loss 1.33370519 epoch total loss 1.39387405\n",
      "Trained batch 751 batch loss 1.27112746 epoch total loss 1.39371061\n",
      "Trained batch 752 batch loss 1.42141056 epoch total loss 1.39374733\n",
      "Trained batch 753 batch loss 1.4479903 epoch total loss 1.39381945\n",
      "Trained batch 754 batch loss 1.48018789 epoch total loss 1.39393401\n",
      "Trained batch 755 batch loss 1.42336726 epoch total loss 1.39397299\n",
      "Trained batch 756 batch loss 1.2684288 epoch total loss 1.39380693\n",
      "Trained batch 757 batch loss 1.30170858 epoch total loss 1.39368534\n",
      "Trained batch 758 batch loss 1.28640413 epoch total loss 1.39354372\n",
      "Trained batch 759 batch loss 1.36175334 epoch total loss 1.39350176\n",
      "Trained batch 760 batch loss 1.30439353 epoch total loss 1.39338458\n",
      "Trained batch 761 batch loss 1.35302949 epoch total loss 1.39333153\n",
      "Trained batch 762 batch loss 1.27467883 epoch total loss 1.39317584\n",
      "Trained batch 763 batch loss 1.45969152 epoch total loss 1.39326298\n",
      "Trained batch 764 batch loss 1.41307652 epoch total loss 1.39328897\n",
      "Trained batch 765 batch loss 1.37411726 epoch total loss 1.39326394\n",
      "Trained batch 766 batch loss 1.49232936 epoch total loss 1.39339328\n",
      "Trained batch 767 batch loss 1.40532482 epoch total loss 1.39340878\n",
      "Trained batch 768 batch loss 1.35266006 epoch total loss 1.39335573\n",
      "Trained batch 769 batch loss 1.46228147 epoch total loss 1.39344537\n",
      "Trained batch 770 batch loss 1.42410231 epoch total loss 1.39348507\n",
      "Trained batch 771 batch loss 1.37018359 epoch total loss 1.39345491\n",
      "Trained batch 772 batch loss 1.34830058 epoch total loss 1.39339638\n",
      "Trained batch 773 batch loss 1.30005217 epoch total loss 1.39327562\n",
      "Trained batch 774 batch loss 1.26267874 epoch total loss 1.39310694\n",
      "Trained batch 775 batch loss 1.26811326 epoch total loss 1.39294565\n",
      "Trained batch 776 batch loss 1.35984182 epoch total loss 1.39290297\n",
      "Trained batch 777 batch loss 1.33310604 epoch total loss 1.39282608\n",
      "Trained batch 778 batch loss 1.35993052 epoch total loss 1.39278388\n",
      "Trained batch 779 batch loss 1.21597934 epoch total loss 1.39255679\n",
      "Trained batch 780 batch loss 1.36711478 epoch total loss 1.39252412\n",
      "Trained batch 781 batch loss 1.39677477 epoch total loss 1.39252949\n",
      "Trained batch 782 batch loss 1.39390504 epoch total loss 1.39253128\n",
      "Trained batch 783 batch loss 1.2629621 epoch total loss 1.39236581\n",
      "Trained batch 784 batch loss 1.35737789 epoch total loss 1.39232123\n",
      "Trained batch 785 batch loss 1.50280571 epoch total loss 1.39246202\n",
      "Trained batch 786 batch loss 1.29641509 epoch total loss 1.39233971\n",
      "Trained batch 787 batch loss 1.26721871 epoch total loss 1.39218068\n",
      "Trained batch 788 batch loss 1.34991884 epoch total loss 1.39212716\n",
      "Trained batch 789 batch loss 1.36309087 epoch total loss 1.39209032\n",
      "Trained batch 790 batch loss 1.33945549 epoch total loss 1.39202368\n",
      "Trained batch 791 batch loss 1.2789824 epoch total loss 1.39188075\n",
      "Trained batch 792 batch loss 1.35415673 epoch total loss 1.39183307\n",
      "Trained batch 793 batch loss 1.36423016 epoch total loss 1.39179826\n",
      "Trained batch 794 batch loss 1.29355621 epoch total loss 1.39167464\n",
      "Trained batch 795 batch loss 1.32517695 epoch total loss 1.39159095\n",
      "Trained batch 796 batch loss 1.3736285 epoch total loss 1.39156842\n",
      "Trained batch 797 batch loss 1.38735235 epoch total loss 1.39156318\n",
      "Trained batch 798 batch loss 1.37395859 epoch total loss 1.391541\n",
      "Trained batch 799 batch loss 1.38985956 epoch total loss 1.39153898\n",
      "Trained batch 800 batch loss 1.22792494 epoch total loss 1.39133441\n",
      "Trained batch 801 batch loss 1.32938707 epoch total loss 1.39125705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 802 batch loss 1.34054589 epoch total loss 1.39119375\n",
      "Trained batch 803 batch loss 1.42030025 epoch total loss 1.39123\n",
      "Trained batch 804 batch loss 1.38024497 epoch total loss 1.3912164\n",
      "Trained batch 805 batch loss 1.39743865 epoch total loss 1.39122415\n",
      "Trained batch 806 batch loss 1.38287747 epoch total loss 1.39121389\n",
      "Trained batch 807 batch loss 1.43591809 epoch total loss 1.39126921\n",
      "Trained batch 808 batch loss 1.36422396 epoch total loss 1.39123583\n",
      "Trained batch 809 batch loss 1.34660864 epoch total loss 1.39118063\n",
      "Trained batch 810 batch loss 1.44390583 epoch total loss 1.3912456\n",
      "Trained batch 811 batch loss 1.38029134 epoch total loss 1.39123201\n",
      "Trained batch 812 batch loss 1.42968941 epoch total loss 1.39127934\n",
      "Trained batch 813 batch loss 1.40852189 epoch total loss 1.39130068\n",
      "Trained batch 814 batch loss 1.39576042 epoch total loss 1.39130616\n",
      "Trained batch 815 batch loss 1.35955572 epoch total loss 1.39126706\n",
      "Trained batch 816 batch loss 1.53012323 epoch total loss 1.39143729\n",
      "Trained batch 817 batch loss 1.33794069 epoch total loss 1.39137173\n",
      "Trained batch 818 batch loss 1.40164661 epoch total loss 1.39138424\n",
      "Trained batch 819 batch loss 1.39794683 epoch total loss 1.39139235\n",
      "Trained batch 820 batch loss 1.39449131 epoch total loss 1.39139616\n",
      "Trained batch 821 batch loss 1.32402062 epoch total loss 1.39131403\n",
      "Trained batch 822 batch loss 1.32743824 epoch total loss 1.39123619\n",
      "Trained batch 823 batch loss 1.23463035 epoch total loss 1.39104593\n",
      "Trained batch 824 batch loss 1.21866298 epoch total loss 1.39083672\n",
      "Trained batch 825 batch loss 1.20751953 epoch total loss 1.39061451\n",
      "Trained batch 826 batch loss 1.26782846 epoch total loss 1.39046586\n",
      "Trained batch 827 batch loss 1.39373636 epoch total loss 1.39046967\n",
      "Trained batch 828 batch loss 1.36920667 epoch total loss 1.39044416\n",
      "Trained batch 829 batch loss 1.36422443 epoch total loss 1.39041257\n",
      "Trained batch 830 batch loss 1.33127689 epoch total loss 1.39034128\n",
      "Trained batch 831 batch loss 1.28715646 epoch total loss 1.39021707\n",
      "Trained batch 832 batch loss 1.33009672 epoch total loss 1.39014482\n",
      "Trained batch 833 batch loss 1.35874164 epoch total loss 1.39010715\n",
      "Trained batch 834 batch loss 1.38832688 epoch total loss 1.39010501\n",
      "Trained batch 835 batch loss 1.27593648 epoch total loss 1.38996816\n",
      "Trained batch 836 batch loss 1.25289297 epoch total loss 1.38980424\n",
      "Trained batch 837 batch loss 1.27379346 epoch total loss 1.3896656\n",
      "Trained batch 838 batch loss 1.38958979 epoch total loss 1.3896656\n",
      "Trained batch 839 batch loss 1.49086964 epoch total loss 1.38978624\n",
      "Trained batch 840 batch loss 1.32192636 epoch total loss 1.38970542\n",
      "Trained batch 841 batch loss 1.32502532 epoch total loss 1.38962853\n",
      "Trained batch 842 batch loss 1.26141536 epoch total loss 1.3894763\n",
      "Trained batch 843 batch loss 1.23053074 epoch total loss 1.38928783\n",
      "Trained batch 844 batch loss 1.16599154 epoch total loss 1.3890233\n",
      "Trained batch 845 batch loss 1.20717359 epoch total loss 1.38880813\n",
      "Trained batch 846 batch loss 1.37920582 epoch total loss 1.38879669\n",
      "Trained batch 847 batch loss 1.2107836 epoch total loss 1.38858652\n",
      "Trained batch 848 batch loss 1.36032844 epoch total loss 1.38855326\n",
      "Trained batch 849 batch loss 1.43965852 epoch total loss 1.38861346\n",
      "Trained batch 850 batch loss 1.25992298 epoch total loss 1.38846207\n",
      "Trained batch 851 batch loss 1.30965185 epoch total loss 1.38836944\n",
      "Trained batch 852 batch loss 1.29472327 epoch total loss 1.38825953\n",
      "Trained batch 853 batch loss 1.32309806 epoch total loss 1.38818312\n",
      "Trained batch 854 batch loss 1.52390146 epoch total loss 1.38834214\n",
      "Trained batch 855 batch loss 1.31253624 epoch total loss 1.38825345\n",
      "Trained batch 856 batch loss 1.08629012 epoch total loss 1.38790071\n",
      "Trained batch 857 batch loss 1.1325444 epoch total loss 1.38760269\n",
      "Trained batch 858 batch loss 1.35402966 epoch total loss 1.38756359\n",
      "Trained batch 859 batch loss 1.31292152 epoch total loss 1.38747656\n",
      "Trained batch 860 batch loss 1.22987199 epoch total loss 1.38729334\n",
      "Trained batch 861 batch loss 1.29105294 epoch total loss 1.38718152\n",
      "Trained batch 862 batch loss 1.31022251 epoch total loss 1.38709223\n",
      "Trained batch 863 batch loss 1.44739509 epoch total loss 1.38716209\n",
      "Trained batch 864 batch loss 1.35570312 epoch total loss 1.38712561\n",
      "Trained batch 865 batch loss 1.47799933 epoch total loss 1.38723075\n",
      "Trained batch 866 batch loss 1.54649746 epoch total loss 1.38741469\n",
      "Trained batch 867 batch loss 1.57440948 epoch total loss 1.38763046\n",
      "Trained batch 868 batch loss 1.45359778 epoch total loss 1.3877064\n",
      "Trained batch 869 batch loss 1.41124463 epoch total loss 1.38773358\n",
      "Trained batch 870 batch loss 1.4091351 epoch total loss 1.38775814\n",
      "Trained batch 871 batch loss 1.4476037 epoch total loss 1.38782692\n",
      "Trained batch 872 batch loss 1.43687904 epoch total loss 1.38788319\n",
      "Trained batch 873 batch loss 1.39444351 epoch total loss 1.3878907\n",
      "Trained batch 874 batch loss 1.42667568 epoch total loss 1.38793504\n",
      "Trained batch 875 batch loss 1.39853644 epoch total loss 1.38794708\n",
      "Trained batch 876 batch loss 1.40159297 epoch total loss 1.3879627\n",
      "Trained batch 877 batch loss 1.35088706 epoch total loss 1.38792038\n",
      "Trained batch 878 batch loss 1.31145585 epoch total loss 1.38783324\n",
      "Trained batch 879 batch loss 1.35652685 epoch total loss 1.38779771\n",
      "Trained batch 880 batch loss 1.32288516 epoch total loss 1.38772392\n",
      "Trained batch 881 batch loss 1.34228671 epoch total loss 1.38767231\n",
      "Trained batch 882 batch loss 1.24525094 epoch total loss 1.38751078\n",
      "Trained batch 883 batch loss 1.29619694 epoch total loss 1.3874073\n",
      "Trained batch 884 batch loss 1.29706824 epoch total loss 1.38730526\n",
      "Trained batch 885 batch loss 1.41526341 epoch total loss 1.38733685\n",
      "Trained batch 886 batch loss 1.29721236 epoch total loss 1.38723516\n",
      "Trained batch 887 batch loss 1.39958143 epoch total loss 1.38724899\n",
      "Trained batch 888 batch loss 1.3308599 epoch total loss 1.38718545\n",
      "Trained batch 889 batch loss 1.4840641 epoch total loss 1.38729441\n",
      "Trained batch 890 batch loss 1.42188239 epoch total loss 1.38733327\n",
      "Trained batch 891 batch loss 1.44325662 epoch total loss 1.38739598\n",
      "Trained batch 892 batch loss 1.31051564 epoch total loss 1.38730979\n",
      "Trained batch 893 batch loss 1.43687069 epoch total loss 1.38736534\n",
      "Trained batch 894 batch loss 1.3594712 epoch total loss 1.38733411\n",
      "Trained batch 895 batch loss 1.32089984 epoch total loss 1.38726\n",
      "Trained batch 896 batch loss 1.34960139 epoch total loss 1.38721788\n",
      "Trained batch 897 batch loss 1.40343595 epoch total loss 1.387236\n",
      "Trained batch 898 batch loss 1.4973371 epoch total loss 1.38735855\n",
      "Trained batch 899 batch loss 1.50054 epoch total loss 1.38748443\n",
      "Trained batch 900 batch loss 1.41159201 epoch total loss 1.38751125\n",
      "Trained batch 901 batch loss 1.41934657 epoch total loss 1.38754654\n",
      "Trained batch 902 batch loss 1.44946146 epoch total loss 1.3876152\n",
      "Trained batch 903 batch loss 1.29151154 epoch total loss 1.38750875\n",
      "Trained batch 904 batch loss 1.360484 epoch total loss 1.38747883\n",
      "Trained batch 905 batch loss 1.3057766 epoch total loss 1.38738859\n",
      "Trained batch 906 batch loss 1.36246288 epoch total loss 1.38736105\n",
      "Trained batch 907 batch loss 1.30122626 epoch total loss 1.38726616\n",
      "Trained batch 908 batch loss 1.37222993 epoch total loss 1.38724947\n",
      "Trained batch 909 batch loss 1.35005307 epoch total loss 1.3872087\n",
      "Trained batch 910 batch loss 1.3518436 epoch total loss 1.38716972\n",
      "Trained batch 911 batch loss 1.347242 epoch total loss 1.38712597\n",
      "Trained batch 912 batch loss 1.39837313 epoch total loss 1.38713825\n",
      "Trained batch 913 batch loss 1.39407468 epoch total loss 1.38714576\n",
      "Trained batch 914 batch loss 1.34611571 epoch total loss 1.38710082\n",
      "Trained batch 915 batch loss 1.2369231 epoch total loss 1.38693678\n",
      "Trained batch 916 batch loss 1.24856746 epoch total loss 1.38678563\n",
      "Trained batch 917 batch loss 1.19641161 epoch total loss 1.38657808\n",
      "Trained batch 918 batch loss 1.43871427 epoch total loss 1.38663483\n",
      "Trained batch 919 batch loss 1.41424918 epoch total loss 1.38666499\n",
      "Trained batch 920 batch loss 1.58652139 epoch total loss 1.38688219\n",
      "Trained batch 921 batch loss 1.57659733 epoch total loss 1.38708818\n",
      "Trained batch 922 batch loss 1.42978406 epoch total loss 1.38713443\n",
      "Trained batch 923 batch loss 1.38845849 epoch total loss 1.38713586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 924 batch loss 1.35138226 epoch total loss 1.38709724\n",
      "Trained batch 925 batch loss 1.27472639 epoch total loss 1.38697577\n",
      "Trained batch 926 batch loss 1.24593675 epoch total loss 1.38682353\n",
      "Trained batch 927 batch loss 1.41355586 epoch total loss 1.38685238\n",
      "Trained batch 928 batch loss 1.44410264 epoch total loss 1.38691413\n",
      "Trained batch 929 batch loss 1.33226204 epoch total loss 1.38685524\n",
      "Trained batch 930 batch loss 1.34355116 epoch total loss 1.38680863\n",
      "Trained batch 931 batch loss 1.34556699 epoch total loss 1.38676441\n",
      "Trained batch 932 batch loss 1.33049309 epoch total loss 1.38670397\n",
      "Trained batch 933 batch loss 1.35993278 epoch total loss 1.38667536\n",
      "Trained batch 934 batch loss 1.39817953 epoch total loss 1.38668764\n",
      "Trained batch 935 batch loss 1.45345581 epoch total loss 1.38675904\n",
      "Trained batch 936 batch loss 1.30716431 epoch total loss 1.38667405\n",
      "Trained batch 937 batch loss 1.52325845 epoch total loss 1.38681984\n",
      "Trained batch 938 batch loss 1.53360069 epoch total loss 1.38697624\n",
      "Trained batch 939 batch loss 1.52084684 epoch total loss 1.38711894\n",
      "Trained batch 940 batch loss 1.32780683 epoch total loss 1.38705575\n",
      "Trained batch 941 batch loss 1.19486308 epoch total loss 1.38685143\n",
      "Trained batch 942 batch loss 1.18163431 epoch total loss 1.38663363\n",
      "Trained batch 943 batch loss 1.23100734 epoch total loss 1.38646853\n",
      "Trained batch 944 batch loss 1.28936255 epoch total loss 1.38636565\n",
      "Trained batch 945 batch loss 1.16046429 epoch total loss 1.38612664\n",
      "Trained batch 946 batch loss 1.08593297 epoch total loss 1.3858093\n",
      "Trained batch 947 batch loss 1.11827993 epoch total loss 1.38552678\n",
      "Trained batch 948 batch loss 1.12079382 epoch total loss 1.38524759\n",
      "Trained batch 949 batch loss 1.28527164 epoch total loss 1.38514221\n",
      "Trained batch 950 batch loss 1.31256521 epoch total loss 1.38506591\n",
      "Trained batch 951 batch loss 1.39291334 epoch total loss 1.38507426\n",
      "Trained batch 952 batch loss 1.48003328 epoch total loss 1.38517392\n",
      "Trained batch 953 batch loss 1.30854285 epoch total loss 1.38509357\n",
      "Trained batch 954 batch loss 1.43594587 epoch total loss 1.38514686\n",
      "Trained batch 955 batch loss 1.31777835 epoch total loss 1.38507628\n",
      "Trained batch 956 batch loss 1.25735354 epoch total loss 1.38494265\n",
      "Trained batch 957 batch loss 1.28886592 epoch total loss 1.38484216\n",
      "Trained batch 958 batch loss 1.39930987 epoch total loss 1.3848573\n",
      "Trained batch 959 batch loss 1.39882565 epoch total loss 1.38487184\n",
      "Trained batch 960 batch loss 1.41075683 epoch total loss 1.38489878\n",
      "Trained batch 961 batch loss 1.30360007 epoch total loss 1.38481414\n",
      "Trained batch 962 batch loss 1.307019 epoch total loss 1.38473332\n",
      "Trained batch 963 batch loss 1.19671535 epoch total loss 1.38453794\n",
      "Trained batch 964 batch loss 1.31952822 epoch total loss 1.38447058\n",
      "Trained batch 965 batch loss 1.28673458 epoch total loss 1.38436925\n",
      "Trained batch 966 batch loss 1.30904424 epoch total loss 1.38429141\n",
      "Trained batch 967 batch loss 1.33642554 epoch total loss 1.38424182\n",
      "Trained batch 968 batch loss 1.40153 epoch total loss 1.3842597\n",
      "Trained batch 969 batch loss 1.4452 epoch total loss 1.38432252\n",
      "Trained batch 970 batch loss 1.28788292 epoch total loss 1.3842231\n",
      "Trained batch 971 batch loss 1.29101264 epoch total loss 1.38412714\n",
      "Trained batch 972 batch loss 1.40505838 epoch total loss 1.3841486\n",
      "Trained batch 973 batch loss 1.29937136 epoch total loss 1.38406146\n",
      "Trained batch 974 batch loss 1.27826214 epoch total loss 1.38395286\n",
      "Trained batch 975 batch loss 1.4703753 epoch total loss 1.38404143\n",
      "Trained batch 976 batch loss 1.47389567 epoch total loss 1.38413346\n",
      "Trained batch 977 batch loss 1.28935087 epoch total loss 1.38403642\n",
      "Trained batch 978 batch loss 1.29230344 epoch total loss 1.38394272\n",
      "Trained batch 979 batch loss 1.37011755 epoch total loss 1.38392854\n",
      "Trained batch 980 batch loss 1.38759553 epoch total loss 1.38393235\n",
      "Trained batch 981 batch loss 1.39724779 epoch total loss 1.38394582\n",
      "Trained batch 982 batch loss 1.29997826 epoch total loss 1.38386035\n",
      "Trained batch 983 batch loss 1.30828071 epoch total loss 1.38378334\n",
      "Trained batch 984 batch loss 1.25150514 epoch total loss 1.38364887\n",
      "Trained batch 985 batch loss 1.25464582 epoch total loss 1.38351786\n",
      "Trained batch 986 batch loss 1.20063758 epoch total loss 1.38333249\n",
      "Trained batch 987 batch loss 1.3082093 epoch total loss 1.38325644\n",
      "Trained batch 988 batch loss 1.27169561 epoch total loss 1.38314354\n",
      "Trained batch 989 batch loss 1.25764084 epoch total loss 1.38301671\n",
      "Trained batch 990 batch loss 1.1484704 epoch total loss 1.38277972\n",
      "Trained batch 991 batch loss 1.24469066 epoch total loss 1.38264036\n",
      "Trained batch 992 batch loss 1.30564284 epoch total loss 1.38256288\n",
      "Trained batch 993 batch loss 1.31497145 epoch total loss 1.38249469\n",
      "Trained batch 994 batch loss 1.357548 epoch total loss 1.38246965\n",
      "Trained batch 995 batch loss 1.3889246 epoch total loss 1.38247609\n",
      "Trained batch 996 batch loss 1.3577044 epoch total loss 1.38245118\n",
      "Trained batch 997 batch loss 1.35397971 epoch total loss 1.38242269\n",
      "Trained batch 998 batch loss 1.36978042 epoch total loss 1.38240993\n",
      "Trained batch 999 batch loss 1.27006078 epoch total loss 1.38229752\n",
      "Trained batch 1000 batch loss 1.23334301 epoch total loss 1.3821485\n",
      "Trained batch 1001 batch loss 1.27364337 epoch total loss 1.38204014\n",
      "Trained batch 1002 batch loss 1.25591338 epoch total loss 1.38191426\n",
      "Trained batch 1003 batch loss 1.29369497 epoch total loss 1.38182628\n",
      "Trained batch 1004 batch loss 1.388484 epoch total loss 1.38183296\n",
      "Trained batch 1005 batch loss 1.27087343 epoch total loss 1.38172245\n",
      "Trained batch 1006 batch loss 1.16873908 epoch total loss 1.38151073\n",
      "Trained batch 1007 batch loss 1.14099407 epoch total loss 1.38127184\n",
      "Trained batch 1008 batch loss 1.0437603 epoch total loss 1.38093698\n",
      "Trained batch 1009 batch loss 1.33368993 epoch total loss 1.38089025\n",
      "Trained batch 1010 batch loss 1.34111035 epoch total loss 1.38085079\n",
      "Trained batch 1011 batch loss 1.40592217 epoch total loss 1.38087559\n",
      "Trained batch 1012 batch loss 1.48230898 epoch total loss 1.38097572\n",
      "Trained batch 1013 batch loss 1.32981038 epoch total loss 1.3809253\n",
      "Trained batch 1014 batch loss 1.30846035 epoch total loss 1.38085389\n",
      "Trained batch 1015 batch loss 1.16954648 epoch total loss 1.38064563\n",
      "Trained batch 1016 batch loss 1.28135657 epoch total loss 1.380548\n",
      "Trained batch 1017 batch loss 1.31149197 epoch total loss 1.38048\n",
      "Trained batch 1018 batch loss 1.33172083 epoch total loss 1.38043213\n",
      "Trained batch 1019 batch loss 1.28154206 epoch total loss 1.38033509\n",
      "Trained batch 1020 batch loss 1.39181852 epoch total loss 1.3803463\n",
      "Trained batch 1021 batch loss 1.27487397 epoch total loss 1.38024306\n",
      "Trained batch 1022 batch loss 1.333395 epoch total loss 1.38019717\n",
      "Trained batch 1023 batch loss 1.43913198 epoch total loss 1.38025475\n",
      "Trained batch 1024 batch loss 1.37930822 epoch total loss 1.38025379\n",
      "Trained batch 1025 batch loss 1.23470128 epoch total loss 1.38011181\n",
      "Trained batch 1026 batch loss 1.24307585 epoch total loss 1.37997818\n",
      "Trained batch 1027 batch loss 1.22037339 epoch total loss 1.37982273\n",
      "Trained batch 1028 batch loss 1.21755028 epoch total loss 1.3796649\n",
      "Trained batch 1029 batch loss 1.31857514 epoch total loss 1.37960553\n",
      "Trained batch 1030 batch loss 1.42864156 epoch total loss 1.3796531\n",
      "Trained batch 1031 batch loss 1.30834591 epoch total loss 1.37958395\n",
      "Trained batch 1032 batch loss 1.31070781 epoch total loss 1.3795172\n",
      "Trained batch 1033 batch loss 1.25316274 epoch total loss 1.37939489\n",
      "Trained batch 1034 batch loss 1.43261862 epoch total loss 1.37944639\n",
      "Trained batch 1035 batch loss 1.36887062 epoch total loss 1.37943614\n",
      "Trained batch 1036 batch loss 1.40285635 epoch total loss 1.37945879\n",
      "Trained batch 1037 batch loss 1.35799849 epoch total loss 1.37943804\n",
      "Trained batch 1038 batch loss 1.23836851 epoch total loss 1.37930226\n",
      "Trained batch 1039 batch loss 1.13849425 epoch total loss 1.37907052\n",
      "Trained batch 1040 batch loss 1.1054399 epoch total loss 1.37880743\n",
      "Trained batch 1041 batch loss 1.23166704 epoch total loss 1.37866604\n",
      "Trained batch 1042 batch loss 1.22394204 epoch total loss 1.37851763\n",
      "Trained batch 1043 batch loss 1.21333706 epoch total loss 1.37835932\n",
      "Trained batch 1044 batch loss 1.23845923 epoch total loss 1.37822533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1045 batch loss 1.22416615 epoch total loss 1.37807786\n",
      "Trained batch 1046 batch loss 1.28254056 epoch total loss 1.37798655\n",
      "Trained batch 1047 batch loss 1.17529547 epoch total loss 1.37779295\n",
      "Trained batch 1048 batch loss 1.21942556 epoch total loss 1.37764192\n",
      "Trained batch 1049 batch loss 1.28310466 epoch total loss 1.37755167\n",
      "Trained batch 1050 batch loss 1.19566655 epoch total loss 1.37737846\n",
      "Trained batch 1051 batch loss 1.22791934 epoch total loss 1.37723625\n",
      "Trained batch 1052 batch loss 1.20529222 epoch total loss 1.37707293\n",
      "Trained batch 1053 batch loss 1.27394223 epoch total loss 1.37697494\n",
      "Trained batch 1054 batch loss 1.38860583 epoch total loss 1.37698591\n",
      "Trained batch 1055 batch loss 1.53209519 epoch total loss 1.37713289\n",
      "Trained batch 1056 batch loss 1.4729898 epoch total loss 1.37722373\n",
      "Trained batch 1057 batch loss 1.37457609 epoch total loss 1.37722135\n",
      "Trained batch 1058 batch loss 1.469046 epoch total loss 1.37730801\n",
      "Trained batch 1059 batch loss 1.54195142 epoch total loss 1.37746358\n",
      "Trained batch 1060 batch loss 1.49797797 epoch total loss 1.37757719\n",
      "Trained batch 1061 batch loss 1.4148035 epoch total loss 1.37761223\n",
      "Trained batch 1062 batch loss 1.26533699 epoch total loss 1.37750661\n",
      "Trained batch 1063 batch loss 1.16326869 epoch total loss 1.37730503\n",
      "Trained batch 1064 batch loss 1.11212385 epoch total loss 1.37705576\n",
      "Trained batch 1065 batch loss 1.22918844 epoch total loss 1.376917\n",
      "Trained batch 1066 batch loss 1.53781462 epoch total loss 1.37706792\n",
      "Trained batch 1067 batch loss 1.45832503 epoch total loss 1.37714422\n",
      "Trained batch 1068 batch loss 1.51336622 epoch total loss 1.37727165\n",
      "Trained batch 1069 batch loss 1.38870418 epoch total loss 1.37728238\n",
      "Trained batch 1070 batch loss 1.47856331 epoch total loss 1.37737691\n",
      "Trained batch 1071 batch loss 1.33340192 epoch total loss 1.37733591\n",
      "Trained batch 1072 batch loss 1.45772421 epoch total loss 1.37741089\n",
      "Trained batch 1073 batch loss 1.36374283 epoch total loss 1.37739813\n",
      "Trained batch 1074 batch loss 1.4175849 epoch total loss 1.37743556\n",
      "Trained batch 1075 batch loss 1.36343348 epoch total loss 1.37742257\n",
      "Trained batch 1076 batch loss 1.2906729 epoch total loss 1.37734187\n",
      "Trained batch 1077 batch loss 1.16026735 epoch total loss 1.3771404\n",
      "Trained batch 1078 batch loss 1.26589966 epoch total loss 1.37703717\n",
      "Trained batch 1079 batch loss 1.24006379 epoch total loss 1.37691021\n",
      "Trained batch 1080 batch loss 1.42975903 epoch total loss 1.3769592\n",
      "Trained batch 1081 batch loss 1.4463774 epoch total loss 1.37702346\n",
      "Trained batch 1082 batch loss 1.39201522 epoch total loss 1.37703729\n",
      "Trained batch 1083 batch loss 1.46891069 epoch total loss 1.37712204\n",
      "Trained batch 1084 batch loss 1.28050649 epoch total loss 1.377033\n",
      "Trained batch 1085 batch loss 1.37854958 epoch total loss 1.37703431\n",
      "Trained batch 1086 batch loss 1.36153805 epoch total loss 1.37702012\n",
      "Trained batch 1087 batch loss 1.32762563 epoch total loss 1.3769747\n",
      "Trained batch 1088 batch loss 1.28145504 epoch total loss 1.37688696\n",
      "Trained batch 1089 batch loss 1.31199276 epoch total loss 1.37682736\n",
      "Trained batch 1090 batch loss 1.17465448 epoch total loss 1.37664187\n",
      "Trained batch 1091 batch loss 1.22435677 epoch total loss 1.37650228\n",
      "Trained batch 1092 batch loss 1.23950326 epoch total loss 1.37637687\n",
      "Trained batch 1093 batch loss 1.30915785 epoch total loss 1.37631536\n",
      "Trained batch 1094 batch loss 1.22625041 epoch total loss 1.37617815\n",
      "Trained batch 1095 batch loss 1.19749475 epoch total loss 1.37601507\n",
      "Trained batch 1096 batch loss 1.23070276 epoch total loss 1.37588251\n",
      "Trained batch 1097 batch loss 1.3090471 epoch total loss 1.37582159\n",
      "Trained batch 1098 batch loss 1.33388579 epoch total loss 1.37578332\n",
      "Trained batch 1099 batch loss 1.34712052 epoch total loss 1.37575734\n",
      "Trained batch 1100 batch loss 1.28667355 epoch total loss 1.37567627\n",
      "Trained batch 1101 batch loss 1.35374415 epoch total loss 1.37565637\n",
      "Trained batch 1102 batch loss 1.41844141 epoch total loss 1.37569523\n",
      "Trained batch 1103 batch loss 1.20902181 epoch total loss 1.37554407\n",
      "Trained batch 1104 batch loss 1.37499332 epoch total loss 1.37554359\n",
      "Trained batch 1105 batch loss 1.38805413 epoch total loss 1.37555492\n",
      "Trained batch 1106 batch loss 1.32808077 epoch total loss 1.375512\n",
      "Trained batch 1107 batch loss 1.3651228 epoch total loss 1.37550259\n",
      "Trained batch 1108 batch loss 1.39680135 epoch total loss 1.3755219\n",
      "Trained batch 1109 batch loss 1.54364622 epoch total loss 1.37567353\n",
      "Trained batch 1110 batch loss 1.46257913 epoch total loss 1.37575173\n",
      "Trained batch 1111 batch loss 1.54719639 epoch total loss 1.37590611\n",
      "Trained batch 1112 batch loss 1.31054616 epoch total loss 1.37584734\n",
      "Trained batch 1113 batch loss 1.34000409 epoch total loss 1.37581515\n",
      "Trained batch 1114 batch loss 1.27884912 epoch total loss 1.37572801\n",
      "Trained batch 1115 batch loss 1.33921242 epoch total loss 1.37569535\n",
      "Trained batch 1116 batch loss 1.35262203 epoch total loss 1.37567472\n",
      "Trained batch 1117 batch loss 1.22249603 epoch total loss 1.37553751\n",
      "Trained batch 1118 batch loss 1.33264327 epoch total loss 1.37549925\n",
      "Trained batch 1119 batch loss 1.26375556 epoch total loss 1.37539935\n",
      "Trained batch 1120 batch loss 1.21161866 epoch total loss 1.3752532\n",
      "Trained batch 1121 batch loss 1.28410733 epoch total loss 1.37517178\n",
      "Trained batch 1122 batch loss 1.46342969 epoch total loss 1.37525046\n",
      "Trained batch 1123 batch loss 1.62132764 epoch total loss 1.37546957\n",
      "Trained batch 1124 batch loss 1.56491685 epoch total loss 1.37563813\n",
      "Trained batch 1125 batch loss 1.35285652 epoch total loss 1.37561798\n",
      "Trained batch 1126 batch loss 1.47704625 epoch total loss 1.37570798\n",
      "Trained batch 1127 batch loss 1.49011755 epoch total loss 1.37580955\n",
      "Trained batch 1128 batch loss 1.57649398 epoch total loss 1.37598753\n",
      "Trained batch 1129 batch loss 1.37002993 epoch total loss 1.37598217\n",
      "Trained batch 1130 batch loss 1.30417633 epoch total loss 1.37591863\n",
      "Trained batch 1131 batch loss 1.39311409 epoch total loss 1.37593377\n",
      "Trained batch 1132 batch loss 1.31397879 epoch total loss 1.37587905\n",
      "Trained batch 1133 batch loss 1.27902269 epoch total loss 1.37579358\n",
      "Trained batch 1134 batch loss 1.30426407 epoch total loss 1.37573063\n",
      "Trained batch 1135 batch loss 1.22326863 epoch total loss 1.37559628\n",
      "Trained batch 1136 batch loss 1.32516074 epoch total loss 1.37555194\n",
      "Trained batch 1137 batch loss 1.2017436 epoch total loss 1.37539911\n",
      "Trained batch 1138 batch loss 1.20520759 epoch total loss 1.37524951\n",
      "Trained batch 1139 batch loss 1.16342294 epoch total loss 1.37506354\n",
      "Trained batch 1140 batch loss 1.28212142 epoch total loss 1.374982\n",
      "Trained batch 1141 batch loss 1.24874771 epoch total loss 1.37487137\n",
      "Trained batch 1142 batch loss 1.27198565 epoch total loss 1.37478125\n",
      "Trained batch 1143 batch loss 1.32927823 epoch total loss 1.37474144\n",
      "Trained batch 1144 batch loss 1.26667321 epoch total loss 1.37464702\n",
      "Trained batch 1145 batch loss 1.48367715 epoch total loss 1.37474227\n",
      "Trained batch 1146 batch loss 1.33390331 epoch total loss 1.37470651\n",
      "Trained batch 1147 batch loss 1.459396 epoch total loss 1.3747803\n",
      "Trained batch 1148 batch loss 1.36774993 epoch total loss 1.37477422\n",
      "Trained batch 1149 batch loss 1.21445513 epoch total loss 1.37463474\n",
      "Trained batch 1150 batch loss 1.19922709 epoch total loss 1.37448215\n",
      "Trained batch 1151 batch loss 1.08925807 epoch total loss 1.37423444\n",
      "Trained batch 1152 batch loss 1.20382214 epoch total loss 1.3740865\n",
      "Trained batch 1153 batch loss 1.314978 epoch total loss 1.37403524\n",
      "Trained batch 1154 batch loss 1.54822803 epoch total loss 1.37418616\n",
      "Trained batch 1155 batch loss 1.56142712 epoch total loss 1.37434816\n",
      "Trained batch 1156 batch loss 1.38124335 epoch total loss 1.37435412\n",
      "Trained batch 1157 batch loss 1.38604093 epoch total loss 1.37436426\n",
      "Trained batch 1158 batch loss 1.31090522 epoch total loss 1.37430942\n",
      "Trained batch 1159 batch loss 1.47302079 epoch total loss 1.37439466\n",
      "Trained batch 1160 batch loss 1.39436066 epoch total loss 1.37441182\n",
      "Trained batch 1161 batch loss 1.3771888 epoch total loss 1.37441421\n",
      "Trained batch 1162 batch loss 1.34501529 epoch total loss 1.37438893\n",
      "Trained batch 1163 batch loss 1.42501092 epoch total loss 1.37443244\n",
      "Trained batch 1164 batch loss 1.3393575 epoch total loss 1.37440228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1165 batch loss 1.36798418 epoch total loss 1.37439692\n",
      "Trained batch 1166 batch loss 1.25900745 epoch total loss 1.37429798\n",
      "Trained batch 1167 batch loss 1.2990315 epoch total loss 1.37423348\n",
      "Trained batch 1168 batch loss 1.27490842 epoch total loss 1.37414849\n",
      "Trained batch 1169 batch loss 1.43870127 epoch total loss 1.37420368\n",
      "Trained batch 1170 batch loss 1.25341153 epoch total loss 1.37410045\n",
      "Trained batch 1171 batch loss 1.16919971 epoch total loss 1.37392545\n",
      "Trained batch 1172 batch loss 1.41258216 epoch total loss 1.37395847\n",
      "Trained batch 1173 batch loss 1.26197863 epoch total loss 1.37386298\n",
      "Trained batch 1174 batch loss 1.33507144 epoch total loss 1.37383\n",
      "Trained batch 1175 batch loss 1.41583848 epoch total loss 1.37386572\n",
      "Trained batch 1176 batch loss 1.3388989 epoch total loss 1.37383592\n",
      "Trained batch 1177 batch loss 1.40374374 epoch total loss 1.37386131\n",
      "Trained batch 1178 batch loss 1.27660644 epoch total loss 1.37377882\n",
      "Trained batch 1179 batch loss 1.29352617 epoch total loss 1.37371075\n",
      "Trained batch 1180 batch loss 1.21628368 epoch total loss 1.37357736\n",
      "Trained batch 1181 batch loss 1.25766659 epoch total loss 1.37347925\n",
      "Trained batch 1182 batch loss 1.39898443 epoch total loss 1.37350082\n",
      "Trained batch 1183 batch loss 1.4123311 epoch total loss 1.37353361\n",
      "Trained batch 1184 batch loss 1.47756267 epoch total loss 1.37362146\n",
      "Trained batch 1185 batch loss 1.37478411 epoch total loss 1.37362242\n",
      "Trained batch 1186 batch loss 1.35844278 epoch total loss 1.37360954\n",
      "Trained batch 1187 batch loss 1.38421249 epoch total loss 1.37361848\n",
      "Trained batch 1188 batch loss 1.37234163 epoch total loss 1.37361741\n",
      "Trained batch 1189 batch loss 1.24877918 epoch total loss 1.37351239\n",
      "Trained batch 1190 batch loss 1.30725622 epoch total loss 1.37345672\n",
      "Trained batch 1191 batch loss 1.2984122 epoch total loss 1.37339377\n",
      "Trained batch 1192 batch loss 1.27826464 epoch total loss 1.3733139\n",
      "Trained batch 1193 batch loss 1.38055253 epoch total loss 1.37332\n",
      "Trained batch 1194 batch loss 1.36025548 epoch total loss 1.37330902\n",
      "Trained batch 1195 batch loss 1.3018899 epoch total loss 1.37324929\n",
      "Trained batch 1196 batch loss 1.34887505 epoch total loss 1.37322891\n",
      "Trained batch 1197 batch loss 1.33228111 epoch total loss 1.37319469\n",
      "Trained batch 1198 batch loss 1.23601937 epoch total loss 1.37308013\n",
      "Trained batch 1199 batch loss 1.26097655 epoch total loss 1.37298656\n",
      "Trained batch 1200 batch loss 1.41721189 epoch total loss 1.37302351\n",
      "Trained batch 1201 batch loss 1.46207619 epoch total loss 1.37309754\n",
      "Trained batch 1202 batch loss 1.27326095 epoch total loss 1.37301457\n",
      "Trained batch 1203 batch loss 1.18596601 epoch total loss 1.372859\n",
      "Trained batch 1204 batch loss 1.1654706 epoch total loss 1.37268686\n",
      "Trained batch 1205 batch loss 1.20462954 epoch total loss 1.37254739\n",
      "Trained batch 1206 batch loss 1.20677626 epoch total loss 1.37240994\n",
      "Trained batch 1207 batch loss 1.15654612 epoch total loss 1.37223101\n",
      "Trained batch 1208 batch loss 1.11417782 epoch total loss 1.37201738\n",
      "Trained batch 1209 batch loss 1.29715288 epoch total loss 1.37195539\n",
      "Trained batch 1210 batch loss 1.23646235 epoch total loss 1.37184346\n",
      "Trained batch 1211 batch loss 1.34141529 epoch total loss 1.3718183\n",
      "Trained batch 1212 batch loss 1.374143 epoch total loss 1.37182021\n",
      "Trained batch 1213 batch loss 1.41593874 epoch total loss 1.37185657\n",
      "Trained batch 1214 batch loss 1.39431572 epoch total loss 1.37187505\n",
      "Trained batch 1215 batch loss 1.44115829 epoch total loss 1.37193203\n",
      "Trained batch 1216 batch loss 1.45337582 epoch total loss 1.37199903\n",
      "Trained batch 1217 batch loss 1.49425554 epoch total loss 1.37209952\n",
      "Trained batch 1218 batch loss 1.38297009 epoch total loss 1.37210834\n",
      "Trained batch 1219 batch loss 1.38929427 epoch total loss 1.37212253\n",
      "Trained batch 1220 batch loss 1.31478667 epoch total loss 1.37207556\n",
      "Trained batch 1221 batch loss 1.30434418 epoch total loss 1.37202\n",
      "Trained batch 1222 batch loss 1.37233734 epoch total loss 1.37202024\n",
      "Trained batch 1223 batch loss 1.31917834 epoch total loss 1.37197709\n",
      "Trained batch 1224 batch loss 1.31539118 epoch total loss 1.37193084\n",
      "Trained batch 1225 batch loss 1.32482791 epoch total loss 1.37189245\n",
      "Trained batch 1226 batch loss 1.28369439 epoch total loss 1.37182045\n",
      "Trained batch 1227 batch loss 1.45510268 epoch total loss 1.3718884\n",
      "Trained batch 1228 batch loss 1.364923 epoch total loss 1.37188268\n",
      "Trained batch 1229 batch loss 1.22051263 epoch total loss 1.37175941\n",
      "Trained batch 1230 batch loss 1.3351413 epoch total loss 1.37172961\n",
      "Trained batch 1231 batch loss 1.27933013 epoch total loss 1.37165451\n",
      "Trained batch 1232 batch loss 1.17705536 epoch total loss 1.37149656\n",
      "Trained batch 1233 batch loss 1.30355322 epoch total loss 1.37144148\n",
      "Trained batch 1234 batch loss 1.22130942 epoch total loss 1.37131977\n",
      "Trained batch 1235 batch loss 1.2995553 epoch total loss 1.37126172\n",
      "Trained batch 1236 batch loss 1.38992071 epoch total loss 1.37127674\n",
      "Trained batch 1237 batch loss 1.37878215 epoch total loss 1.37128282\n",
      "Trained batch 1238 batch loss 1.38899016 epoch total loss 1.37129712\n",
      "Trained batch 1239 batch loss 1.18898034 epoch total loss 1.37115\n",
      "Trained batch 1240 batch loss 1.16497016 epoch total loss 1.37098372\n",
      "Trained batch 1241 batch loss 1.14027286 epoch total loss 1.37079775\n",
      "Trained batch 1242 batch loss 1.19178545 epoch total loss 1.37065363\n",
      "Trained batch 1243 batch loss 1.2890923 epoch total loss 1.37058794\n",
      "Trained batch 1244 batch loss 1.20006108 epoch total loss 1.37045097\n",
      "Trained batch 1245 batch loss 1.27095127 epoch total loss 1.37037098\n",
      "Trained batch 1246 batch loss 1.28445697 epoch total loss 1.37030208\n",
      "Trained batch 1247 batch loss 1.36880422 epoch total loss 1.37030089\n",
      "Trained batch 1248 batch loss 1.40210509 epoch total loss 1.37032628\n",
      "Trained batch 1249 batch loss 1.46084881 epoch total loss 1.37039876\n",
      "Trained batch 1250 batch loss 1.18228626 epoch total loss 1.3702482\n",
      "Trained batch 1251 batch loss 1.19338059 epoch total loss 1.37010682\n",
      "Trained batch 1252 batch loss 1.29744959 epoch total loss 1.37004888\n",
      "Trained batch 1253 batch loss 1.40588379 epoch total loss 1.37007749\n",
      "Trained batch 1254 batch loss 1.51722121 epoch total loss 1.37019479\n",
      "Trained batch 1255 batch loss 1.36296272 epoch total loss 1.37018895\n",
      "Trained batch 1256 batch loss 1.33039165 epoch total loss 1.37015736\n",
      "Trained batch 1257 batch loss 1.23931444 epoch total loss 1.37005317\n",
      "Trained batch 1258 batch loss 1.31372952 epoch total loss 1.37000847\n",
      "Trained batch 1259 batch loss 1.22837663 epoch total loss 1.36989594\n",
      "Trained batch 1260 batch loss 1.25707793 epoch total loss 1.36980641\n",
      "Trained batch 1261 batch loss 1.2431941 epoch total loss 1.36970592\n",
      "Trained batch 1262 batch loss 1.22170246 epoch total loss 1.36958861\n",
      "Trained batch 1263 batch loss 1.31111968 epoch total loss 1.36954236\n",
      "Trained batch 1264 batch loss 1.21011066 epoch total loss 1.36941624\n",
      "Trained batch 1265 batch loss 1.26468813 epoch total loss 1.36933339\n",
      "Trained batch 1266 batch loss 1.29637444 epoch total loss 1.36927581\n",
      "Trained batch 1267 batch loss 1.21624458 epoch total loss 1.36915493\n",
      "Trained batch 1268 batch loss 1.23311555 epoch total loss 1.36904776\n",
      "Trained batch 1269 batch loss 1.16740716 epoch total loss 1.36888874\n",
      "Trained batch 1270 batch loss 1.19585109 epoch total loss 1.36875248\n",
      "Trained batch 1271 batch loss 1.177809 epoch total loss 1.36860228\n",
      "Trained batch 1272 batch loss 1.29669583 epoch total loss 1.36854577\n",
      "Trained batch 1273 batch loss 1.30020332 epoch total loss 1.36849213\n",
      "Trained batch 1274 batch loss 1.25008464 epoch total loss 1.36839914\n",
      "Trained batch 1275 batch loss 1.40477407 epoch total loss 1.36842775\n",
      "Trained batch 1276 batch loss 1.31688499 epoch total loss 1.36838734\n",
      "Trained batch 1277 batch loss 1.37327337 epoch total loss 1.36839116\n",
      "Trained batch 1278 batch loss 1.36501694 epoch total loss 1.36838853\n",
      "Trained batch 1279 batch loss 1.35821462 epoch total loss 1.36838055\n",
      "Trained batch 1280 batch loss 1.23499441 epoch total loss 1.36827636\n",
      "Trained batch 1281 batch loss 1.22313809 epoch total loss 1.36816299\n",
      "Trained batch 1282 batch loss 1.18334103 epoch total loss 1.36801887\n",
      "Trained batch 1283 batch loss 1.16810203 epoch total loss 1.36786306\n",
      "Trained batch 1284 batch loss 1.38715029 epoch total loss 1.36787808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1285 batch loss 1.35129201 epoch total loss 1.3678652\n",
      "Trained batch 1286 batch loss 1.32046366 epoch total loss 1.36782837\n",
      "Trained batch 1287 batch loss 1.2999146 epoch total loss 1.36777556\n",
      "Trained batch 1288 batch loss 1.34140468 epoch total loss 1.36775506\n",
      "Trained batch 1289 batch loss 1.39297915 epoch total loss 1.36777461\n",
      "Trained batch 1290 batch loss 1.43357468 epoch total loss 1.36782563\n",
      "Trained batch 1291 batch loss 1.33075571 epoch total loss 1.36779702\n",
      "Trained batch 1292 batch loss 1.253901 epoch total loss 1.3677088\n",
      "Trained batch 1293 batch loss 1.22543037 epoch total loss 1.36759889\n",
      "Trained batch 1294 batch loss 1.30552733 epoch total loss 1.36755085\n",
      "Trained batch 1295 batch loss 1.29944801 epoch total loss 1.36749828\n",
      "Trained batch 1296 batch loss 1.49130738 epoch total loss 1.36759388\n",
      "Trained batch 1297 batch loss 1.36709237 epoch total loss 1.36759341\n",
      "Trained batch 1298 batch loss 1.43881655 epoch total loss 1.36764836\n",
      "Trained batch 1299 batch loss 1.20319223 epoch total loss 1.36752176\n",
      "Trained batch 1300 batch loss 1.32316494 epoch total loss 1.36748755\n",
      "Trained batch 1301 batch loss 1.34400272 epoch total loss 1.36746955\n",
      "Trained batch 1302 batch loss 1.28901947 epoch total loss 1.36740935\n",
      "Trained batch 1303 batch loss 1.27419281 epoch total loss 1.36733782\n",
      "Trained batch 1304 batch loss 1.26073718 epoch total loss 1.36725605\n",
      "Trained batch 1305 batch loss 1.22380555 epoch total loss 1.36714602\n",
      "Trained batch 1306 batch loss 1.10884786 epoch total loss 1.36694837\n",
      "Trained batch 1307 batch loss 1.24127984 epoch total loss 1.36685216\n",
      "Trained batch 1308 batch loss 1.26009285 epoch total loss 1.36677063\n",
      "Trained batch 1309 batch loss 1.4204241 epoch total loss 1.36681163\n",
      "Trained batch 1310 batch loss 1.3106631 epoch total loss 1.36676872\n",
      "Trained batch 1311 batch loss 1.28089166 epoch total loss 1.36670327\n",
      "Trained batch 1312 batch loss 1.2423625 epoch total loss 1.36660838\n",
      "Trained batch 1313 batch loss 1.43382514 epoch total loss 1.36665964\n",
      "Trained batch 1314 batch loss 1.30543661 epoch total loss 1.36661303\n",
      "Trained batch 1315 batch loss 1.3040365 epoch total loss 1.36656547\n",
      "Trained batch 1316 batch loss 1.36517954 epoch total loss 1.36656439\n",
      "Trained batch 1317 batch loss 1.28013015 epoch total loss 1.36649883\n",
      "Trained batch 1318 batch loss 1.25476027 epoch total loss 1.36641407\n",
      "Trained batch 1319 batch loss 1.24543726 epoch total loss 1.3663224\n",
      "Trained batch 1320 batch loss 1.23592353 epoch total loss 1.36622357\n",
      "Trained batch 1321 batch loss 1.29398012 epoch total loss 1.36616886\n",
      "Trained batch 1322 batch loss 1.28061223 epoch total loss 1.36610425\n",
      "Trained batch 1323 batch loss 1.28716278 epoch total loss 1.36604452\n",
      "Trained batch 1324 batch loss 1.29178643 epoch total loss 1.36598837\n",
      "Trained batch 1325 batch loss 1.35972059 epoch total loss 1.36598361\n",
      "Trained batch 1326 batch loss 1.2789917 epoch total loss 1.36591804\n",
      "Trained batch 1327 batch loss 1.14419723 epoch total loss 1.36575091\n",
      "Trained batch 1328 batch loss 1.25901031 epoch total loss 1.36567056\n",
      "Trained batch 1329 batch loss 1.22454691 epoch total loss 1.36556435\n",
      "Trained batch 1330 batch loss 1.1738255 epoch total loss 1.3654201\n",
      "Trained batch 1331 batch loss 1.24415255 epoch total loss 1.36532903\n",
      "Trained batch 1332 batch loss 1.20310152 epoch total loss 1.3652072\n",
      "Trained batch 1333 batch loss 1.35588241 epoch total loss 1.36520016\n",
      "Trained batch 1334 batch loss 1.30069578 epoch total loss 1.36515188\n",
      "Trained batch 1335 batch loss 1.25510097 epoch total loss 1.36506939\n",
      "Trained batch 1336 batch loss 1.25887704 epoch total loss 1.36499\n",
      "Trained batch 1337 batch loss 1.35648954 epoch total loss 1.36498356\n",
      "Trained batch 1338 batch loss 1.32095563 epoch total loss 1.36495066\n",
      "Trained batch 1339 batch loss 1.40303731 epoch total loss 1.36497915\n",
      "Trained batch 1340 batch loss 1.4144069 epoch total loss 1.36501598\n",
      "Trained batch 1341 batch loss 1.28127313 epoch total loss 1.36495352\n",
      "Trained batch 1342 batch loss 1.30331433 epoch total loss 1.36490762\n",
      "Trained batch 1343 batch loss 1.3688122 epoch total loss 1.36491048\n",
      "Trained batch 1344 batch loss 1.29486096 epoch total loss 1.36485839\n",
      "Trained batch 1345 batch loss 1.41466463 epoch total loss 1.36489546\n",
      "Trained batch 1346 batch loss 1.32078576 epoch total loss 1.36486268\n",
      "Trained batch 1347 batch loss 1.28935289 epoch total loss 1.36480665\n",
      "Trained batch 1348 batch loss 1.1873033 epoch total loss 1.36467493\n",
      "Trained batch 1349 batch loss 1.15055859 epoch total loss 1.36451614\n",
      "Trained batch 1350 batch loss 1.2698617 epoch total loss 1.36444604\n",
      "Trained batch 1351 batch loss 1.26791835 epoch total loss 1.36437464\n",
      "Trained batch 1352 batch loss 1.33627927 epoch total loss 1.3643539\n",
      "Trained batch 1353 batch loss 1.37204969 epoch total loss 1.36435962\n",
      "Trained batch 1354 batch loss 1.30829597 epoch total loss 1.36431825\n",
      "Trained batch 1355 batch loss 1.16765118 epoch total loss 1.36417305\n",
      "Trained batch 1356 batch loss 1.33940089 epoch total loss 1.3641547\n",
      "Trained batch 1357 batch loss 1.36214638 epoch total loss 1.36415327\n",
      "Trained batch 1358 batch loss 1.41979027 epoch total loss 1.36419427\n",
      "Trained batch 1359 batch loss 1.53268456 epoch total loss 1.36431825\n",
      "Trained batch 1360 batch loss 1.51677489 epoch total loss 1.36443031\n",
      "Trained batch 1361 batch loss 1.41458559 epoch total loss 1.36446714\n",
      "Trained batch 1362 batch loss 1.13845944 epoch total loss 1.3643012\n",
      "Trained batch 1363 batch loss 1.39068365 epoch total loss 1.36432052\n",
      "Trained batch 1364 batch loss 1.37423158 epoch total loss 1.36432779\n",
      "Trained batch 1365 batch loss 1.60960627 epoch total loss 1.36450756\n",
      "Trained batch 1366 batch loss 1.3904407 epoch total loss 1.36452639\n",
      "Trained batch 1367 batch loss 1.30662096 epoch total loss 1.36448407\n",
      "Trained batch 1368 batch loss 1.35356748 epoch total loss 1.36447608\n",
      "Trained batch 1369 batch loss 1.33860922 epoch total loss 1.36445725\n",
      "Trained batch 1370 batch loss 1.39021671 epoch total loss 1.36447597\n",
      "Trained batch 1371 batch loss 1.42178082 epoch total loss 1.36451781\n",
      "Trained batch 1372 batch loss 1.49304688 epoch total loss 1.36461151\n",
      "Trained batch 1373 batch loss 1.54752207 epoch total loss 1.36474466\n",
      "Trained batch 1374 batch loss 1.35317135 epoch total loss 1.3647362\n",
      "Trained batch 1375 batch loss 1.28857374 epoch total loss 1.36468089\n",
      "Trained batch 1376 batch loss 1.32306886 epoch total loss 1.36465061\n",
      "Trained batch 1377 batch loss 1.23970187 epoch total loss 1.36455989\n",
      "Trained batch 1378 batch loss 1.21758354 epoch total loss 1.3644532\n",
      "Trained batch 1379 batch loss 1.26002681 epoch total loss 1.3643775\n",
      "Trained batch 1380 batch loss 1.1887902 epoch total loss 1.3642503\n",
      "Trained batch 1381 batch loss 1.28735805 epoch total loss 1.36419463\n",
      "Trained batch 1382 batch loss 1.18412662 epoch total loss 1.36406434\n",
      "Trained batch 1383 batch loss 1.16692781 epoch total loss 1.36392164\n",
      "Trained batch 1384 batch loss 1.30361748 epoch total loss 1.36387813\n",
      "Trained batch 1385 batch loss 1.26470256 epoch total loss 1.36380649\n",
      "Trained batch 1386 batch loss 1.28012812 epoch total loss 1.36374605\n",
      "Trained batch 1387 batch loss 1.15502048 epoch total loss 1.3635956\n",
      "Trained batch 1388 batch loss 1.12657821 epoch total loss 1.3634249\n",
      "Epoch 2 train loss 1.3634248971939087\n",
      "Validated batch 1 batch loss 1.31095862\n",
      "Validated batch 2 batch loss 1.19232714\n",
      "Validated batch 3 batch loss 1.31149518\n",
      "Validated batch 4 batch loss 1.24166512\n",
      "Validated batch 5 batch loss 1.32091439\n",
      "Validated batch 6 batch loss 1.36627841\n",
      "Validated batch 7 batch loss 1.28682256\n",
      "Validated batch 8 batch loss 1.47619224\n",
      "Validated batch 9 batch loss 1.42753541\n",
      "Validated batch 10 batch loss 1.32238567\n",
      "Validated batch 11 batch loss 1.33971298\n",
      "Validated batch 12 batch loss 1.42232072\n",
      "Validated batch 13 batch loss 1.40601897\n",
      "Validated batch 14 batch loss 1.38919938\n",
      "Validated batch 15 batch loss 1.40769076\n",
      "Validated batch 16 batch loss 1.46105671\n",
      "Validated batch 17 batch loss 1.42440557\n",
      "Validated batch 18 batch loss 1.21197104\n",
      "Validated batch 19 batch loss 1.30120814\n",
      "Validated batch 20 batch loss 1.40053117\n",
      "Validated batch 21 batch loss 1.32185245\n",
      "Validated batch 22 batch loss 1.37207532\n",
      "Validated batch 23 batch loss 1.30375469\n",
      "Validated batch 24 batch loss 1.40179348\n",
      "Validated batch 25 batch loss 1.34987509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 26 batch loss 1.23750019\n",
      "Validated batch 27 batch loss 1.2549938\n",
      "Validated batch 28 batch loss 1.27673721\n",
      "Validated batch 29 batch loss 1.3828243\n",
      "Validated batch 30 batch loss 1.29071093\n",
      "Validated batch 31 batch loss 1.31791878\n",
      "Validated batch 32 batch loss 1.38028181\n",
      "Validated batch 33 batch loss 1.36085498\n",
      "Validated batch 34 batch loss 1.27291417\n",
      "Validated batch 35 batch loss 1.19662511\n",
      "Validated batch 36 batch loss 1.3529954\n",
      "Validated batch 37 batch loss 1.38362849\n",
      "Validated batch 38 batch loss 1.44803166\n",
      "Validated batch 39 batch loss 1.40816975\n",
      "Validated batch 40 batch loss 1.28535974\n",
      "Validated batch 41 batch loss 1.49589586\n",
      "Validated batch 42 batch loss 1.31618619\n",
      "Validated batch 43 batch loss 1.37260187\n",
      "Validated batch 44 batch loss 1.36590719\n",
      "Validated batch 45 batch loss 1.12343609\n",
      "Validated batch 46 batch loss 1.42656744\n",
      "Validated batch 47 batch loss 1.32445669\n",
      "Validated batch 48 batch loss 1.35452914\n",
      "Validated batch 49 batch loss 1.23949742\n",
      "Validated batch 50 batch loss 1.38661146\n",
      "Validated batch 51 batch loss 1.33151889\n",
      "Validated batch 52 batch loss 1.34507787\n",
      "Validated batch 53 batch loss 1.4163816\n",
      "Validated batch 54 batch loss 1.39901423\n",
      "Validated batch 55 batch loss 1.34247231\n",
      "Validated batch 56 batch loss 1.31345963\n",
      "Validated batch 57 batch loss 1.38367307\n",
      "Validated batch 58 batch loss 1.33299661\n",
      "Validated batch 59 batch loss 1.30826044\n",
      "Validated batch 60 batch loss 1.42850149\n",
      "Validated batch 61 batch loss 1.38425207\n",
      "Validated batch 62 batch loss 1.35898399\n",
      "Validated batch 63 batch loss 1.51043606\n",
      "Validated batch 64 batch loss 1.1594348\n",
      "Validated batch 65 batch loss 1.39790416\n",
      "Validated batch 66 batch loss 1.14497304\n",
      "Validated batch 67 batch loss 1.32224166\n",
      "Validated batch 68 batch loss 1.46537733\n",
      "Validated batch 69 batch loss 1.25614154\n",
      "Validated batch 70 batch loss 1.26388025\n",
      "Validated batch 71 batch loss 1.32084239\n",
      "Validated batch 72 batch loss 1.28267026\n",
      "Validated batch 73 batch loss 1.17512202\n",
      "Validated batch 74 batch loss 1.28660059\n",
      "Validated batch 75 batch loss 1.41367435\n",
      "Validated batch 76 batch loss 1.26453447\n",
      "Validated batch 77 batch loss 1.19286644\n",
      "Validated batch 78 batch loss 1.2271663\n",
      "Validated batch 79 batch loss 1.3398385\n",
      "Validated batch 80 batch loss 1.19909596\n",
      "Validated batch 81 batch loss 1.34592247\n",
      "Validated batch 82 batch loss 1.31445408\n",
      "Validated batch 83 batch loss 1.28280258\n",
      "Validated batch 84 batch loss 1.36370778\n",
      "Validated batch 85 batch loss 1.42664766\n",
      "Validated batch 86 batch loss 1.26711166\n",
      "Validated batch 87 batch loss 1.38499188\n",
      "Validated batch 88 batch loss 1.06594753\n",
      "Validated batch 89 batch loss 1.19614565\n",
      "Validated batch 90 batch loss 1.25550711\n",
      "Validated batch 91 batch loss 1.25048041\n",
      "Validated batch 92 batch loss 1.47817445\n",
      "Validated batch 93 batch loss 1.36021793\n",
      "Validated batch 94 batch loss 1.32677794\n",
      "Validated batch 95 batch loss 1.29333115\n",
      "Validated batch 96 batch loss 1.2859354\n",
      "Validated batch 97 batch loss 1.34281588\n",
      "Validated batch 98 batch loss 1.44409561\n",
      "Validated batch 99 batch loss 1.24833035\n",
      "Validated batch 100 batch loss 1.36699963\n",
      "Validated batch 101 batch loss 1.26136756\n",
      "Validated batch 102 batch loss 1.34330726\n",
      "Validated batch 103 batch loss 1.34190202\n",
      "Validated batch 104 batch loss 1.25762439\n",
      "Validated batch 105 batch loss 1.44626\n",
      "Validated batch 106 batch loss 1.39678812\n",
      "Validated batch 107 batch loss 1.34120059\n",
      "Validated batch 108 batch loss 1.33345914\n",
      "Validated batch 109 batch loss 1.35911942\n",
      "Validated batch 110 batch loss 1.25668788\n",
      "Validated batch 111 batch loss 1.28761423\n",
      "Validated batch 112 batch loss 1.3093555\n",
      "Validated batch 113 batch loss 1.23191273\n",
      "Validated batch 114 batch loss 1.36084819\n",
      "Validated batch 115 batch loss 1.32566774\n",
      "Validated batch 116 batch loss 1.48091936\n",
      "Validated batch 117 batch loss 1.29798949\n",
      "Validated batch 118 batch loss 1.30545259\n",
      "Validated batch 119 batch loss 1.3093338\n",
      "Validated batch 120 batch loss 1.25570083\n",
      "Validated batch 121 batch loss 1.35925949\n",
      "Validated batch 122 batch loss 1.33262849\n",
      "Validated batch 123 batch loss 1.27582216\n",
      "Validated batch 124 batch loss 1.26602435\n",
      "Validated batch 125 batch loss 1.32754707\n",
      "Validated batch 126 batch loss 1.32202756\n",
      "Validated batch 127 batch loss 1.40151429\n",
      "Validated batch 128 batch loss 1.33549082\n",
      "Validated batch 129 batch loss 1.22015786\n",
      "Validated batch 130 batch loss 1.28339326\n",
      "Validated batch 131 batch loss 1.33608186\n",
      "Validated batch 132 batch loss 1.30442321\n",
      "Validated batch 133 batch loss 1.37951326\n",
      "Validated batch 134 batch loss 1.41780853\n",
      "Validated batch 135 batch loss 1.58630395\n",
      "Validated batch 136 batch loss 1.48058891\n",
      "Validated batch 137 batch loss 1.35994983\n",
      "Validated batch 138 batch loss 1.2475872\n",
      "Validated batch 139 batch loss 1.17917585\n",
      "Validated batch 140 batch loss 1.29813862\n",
      "Validated batch 141 batch loss 1.3838706\n",
      "Validated batch 142 batch loss 1.29629946\n",
      "Validated batch 143 batch loss 1.39747214\n",
      "Validated batch 144 batch loss 1.4877193\n",
      "Validated batch 145 batch loss 1.20064425\n",
      "Validated batch 146 batch loss 1.33120489\n",
      "Validated batch 147 batch loss 1.29274\n",
      "Validated batch 148 batch loss 1.3410151\n",
      "Validated batch 149 batch loss 1.38394904\n",
      "Validated batch 150 batch loss 1.30476069\n",
      "Validated batch 151 batch loss 1.15174103\n",
      "Validated batch 152 batch loss 1.31392765\n",
      "Validated batch 153 batch loss 1.28433251\n",
      "Validated batch 154 batch loss 1.29769444\n",
      "Validated batch 155 batch loss 1.36186433\n",
      "Validated batch 156 batch loss 1.19938684\n",
      "Validated batch 157 batch loss 1.36245584\n",
      "Validated batch 158 batch loss 1.41108775\n",
      "Validated batch 159 batch loss 1.34508777\n",
      "Validated batch 160 batch loss 1.31407046\n",
      "Validated batch 161 batch loss 1.26395309\n",
      "Validated batch 162 batch loss 1.28262711\n",
      "Validated batch 163 batch loss 1.23650646\n",
      "Validated batch 164 batch loss 1.30829287\n",
      "Validated batch 165 batch loss 1.26599324\n",
      "Validated batch 166 batch loss 1.25596976\n",
      "Validated batch 167 batch loss 1.31073213\n",
      "Validated batch 168 batch loss 1.27264524\n",
      "Validated batch 169 batch loss 1.31115067\n",
      "Validated batch 170 batch loss 1.40800428\n",
      "Validated batch 171 batch loss 1.1930449\n",
      "Validated batch 172 batch loss 1.40186954\n",
      "Validated batch 173 batch loss 1.38340414\n",
      "Validated batch 174 batch loss 1.19321096\n",
      "Validated batch 175 batch loss 1.33621776\n",
      "Validated batch 176 batch loss 1.32152581\n",
      "Validated batch 177 batch loss 1.2990669\n",
      "Validated batch 178 batch loss 1.42435706\n",
      "Validated batch 179 batch loss 1.37200189\n",
      "Validated batch 180 batch loss 1.37652469\n",
      "Validated batch 181 batch loss 1.28378713\n",
      "Validated batch 182 batch loss 1.3775202\n",
      "Validated batch 183 batch loss 1.33148646\n",
      "Validated batch 184 batch loss 1.23709726\n",
      "Validated batch 185 batch loss 1.44494939\n",
      "Epoch 2 val loss 1.3268232345581055\n",
      "Model /aiffel/aiffel/mpii/models/model_HG-epoch-2-loss-1.3268.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.46535194 epoch total loss 1.46535194\n",
      "Trained batch 2 batch loss 1.31442845 epoch total loss 1.38989019\n",
      "Trained batch 3 batch loss 1.30107498 epoch total loss 1.36028516\n",
      "Trained batch 4 batch loss 1.32141745 epoch total loss 1.35056818\n",
      "Trained batch 5 batch loss 1.49666095 epoch total loss 1.37978673\n",
      "Trained batch 6 batch loss 1.38120234 epoch total loss 1.38002264\n",
      "Trained batch 7 batch loss 1.2924484 epoch total loss 1.36751199\n",
      "Trained batch 8 batch loss 1.43553495 epoch total loss 1.37601495\n",
      "Trained batch 9 batch loss 1.48663723 epoch total loss 1.38830626\n",
      "Trained batch 10 batch loss 1.44609606 epoch total loss 1.39408529\n",
      "Trained batch 11 batch loss 1.34134448 epoch total loss 1.38929069\n",
      "Trained batch 12 batch loss 1.32942009 epoch total loss 1.38430154\n",
      "Trained batch 13 batch loss 1.24287271 epoch total loss 1.37342238\n",
      "Trained batch 14 batch loss 1.30141497 epoch total loss 1.36827886\n",
      "Trained batch 15 batch loss 1.33694398 epoch total loss 1.36619\n",
      "Trained batch 16 batch loss 1.31701303 epoch total loss 1.36311638\n",
      "Trained batch 17 batch loss 1.32512593 epoch total loss 1.36088169\n",
      "Trained batch 18 batch loss 1.30362034 epoch total loss 1.35770059\n",
      "Trained batch 19 batch loss 1.21512079 epoch total loss 1.35019636\n",
      "Trained batch 20 batch loss 1.28271532 epoch total loss 1.34682226\n",
      "Trained batch 21 batch loss 1.31118083 epoch total loss 1.34512496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 22 batch loss 1.3317337 epoch total loss 1.34451628\n",
      "Trained batch 23 batch loss 1.23279917 epoch total loss 1.33965909\n",
      "Trained batch 24 batch loss 1.29276299 epoch total loss 1.33770514\n",
      "Trained batch 25 batch loss 1.27705216 epoch total loss 1.33527911\n",
      "Trained batch 26 batch loss 1.39234447 epoch total loss 1.33747399\n",
      "Trained batch 27 batch loss 1.21375299 epoch total loss 1.3328917\n",
      "Trained batch 28 batch loss 1.2926755 epoch total loss 1.33145535\n",
      "Trained batch 29 batch loss 1.30212891 epoch total loss 1.3304441\n",
      "Trained batch 30 batch loss 1.44513643 epoch total loss 1.33426714\n",
      "Trained batch 31 batch loss 1.29248023 epoch total loss 1.33291924\n",
      "Trained batch 32 batch loss 1.03732204 epoch total loss 1.32368183\n",
      "Trained batch 33 batch loss 1.23158717 epoch total loss 1.32089102\n",
      "Trained batch 34 batch loss 1.27555931 epoch total loss 1.31955779\n",
      "Trained batch 35 batch loss 1.16900718 epoch total loss 1.31525624\n",
      "Trained batch 36 batch loss 1.13536644 epoch total loss 1.31025934\n",
      "Trained batch 37 batch loss 1.33293712 epoch total loss 1.3108722\n",
      "Trained batch 38 batch loss 1.3418889 epoch total loss 1.31168842\n",
      "Trained batch 39 batch loss 1.33031309 epoch total loss 1.31216598\n",
      "Trained batch 40 batch loss 1.33610225 epoch total loss 1.31276441\n",
      "Trained batch 41 batch loss 1.36492968 epoch total loss 1.31403673\n",
      "Trained batch 42 batch loss 1.49171126 epoch total loss 1.31826699\n",
      "Trained batch 43 batch loss 1.6296159 epoch total loss 1.32550764\n",
      "Trained batch 44 batch loss 1.30965757 epoch total loss 1.32514751\n",
      "Trained batch 45 batch loss 1.3012166 epoch total loss 1.3246156\n",
      "Trained batch 46 batch loss 1.27445638 epoch total loss 1.32352519\n",
      "Trained batch 47 batch loss 1.18190801 epoch total loss 1.32051206\n",
      "Trained batch 48 batch loss 1.2610451 epoch total loss 1.31927311\n",
      "Trained batch 49 batch loss 1.25130498 epoch total loss 1.31788599\n",
      "Trained batch 50 batch loss 1.2470572 epoch total loss 1.31646943\n",
      "Trained batch 51 batch loss 1.23457074 epoch total loss 1.31486356\n",
      "Trained batch 52 batch loss 1.2617085 epoch total loss 1.31384146\n",
      "Trained batch 53 batch loss 1.24790549 epoch total loss 1.31259727\n",
      "Trained batch 54 batch loss 1.09296048 epoch total loss 1.30853\n",
      "Trained batch 55 batch loss 1.18009901 epoch total loss 1.3061949\n",
      "Trained batch 56 batch loss 1.13137889 epoch total loss 1.30307317\n",
      "Trained batch 57 batch loss 1.24265087 epoch total loss 1.30201316\n",
      "Trained batch 58 batch loss 1.2448808 epoch total loss 1.30102813\n",
      "Trained batch 59 batch loss 1.29647279 epoch total loss 1.30095088\n",
      "Trained batch 60 batch loss 1.26235902 epoch total loss 1.30030775\n",
      "Trained batch 61 batch loss 1.3459053 epoch total loss 1.30105519\n",
      "Trained batch 62 batch loss 1.31119382 epoch total loss 1.30121875\n",
      "Trained batch 63 batch loss 1.38454139 epoch total loss 1.30254138\n",
      "Trained batch 64 batch loss 1.339697 epoch total loss 1.30312192\n",
      "Trained batch 65 batch loss 1.43483114 epoch total loss 1.30514824\n",
      "Trained batch 66 batch loss 1.41143811 epoch total loss 1.30675864\n",
      "Trained batch 67 batch loss 1.30250251 epoch total loss 1.30669522\n",
      "Trained batch 68 batch loss 1.26746035 epoch total loss 1.30611825\n",
      "Trained batch 69 batch loss 1.41166711 epoch total loss 1.30764794\n",
      "Trained batch 70 batch loss 1.35571265 epoch total loss 1.30833459\n",
      "Trained batch 71 batch loss 1.37791634 epoch total loss 1.30931461\n",
      "Trained batch 72 batch loss 1.32012737 epoch total loss 1.30946481\n",
      "Trained batch 73 batch loss 1.37832284 epoch total loss 1.31040812\n",
      "Trained batch 74 batch loss 1.37558961 epoch total loss 1.31128883\n",
      "Trained batch 75 batch loss 1.29552472 epoch total loss 1.31107867\n",
      "Trained batch 76 batch loss 1.36683202 epoch total loss 1.31181228\n",
      "Trained batch 77 batch loss 1.29486203 epoch total loss 1.3115921\n",
      "Trained batch 78 batch loss 1.33060396 epoch total loss 1.31183589\n",
      "Trained batch 79 batch loss 1.31231236 epoch total loss 1.31184185\n",
      "Trained batch 80 batch loss 1.23203981 epoch total loss 1.3108443\n",
      "Trained batch 81 batch loss 1.16517162 epoch total loss 1.30904591\n",
      "Trained batch 82 batch loss 1.21592879 epoch total loss 1.30791032\n",
      "Trained batch 83 batch loss 1.17676079 epoch total loss 1.30633008\n",
      "Trained batch 84 batch loss 1.2531563 epoch total loss 1.30569708\n",
      "Trained batch 85 batch loss 1.17828751 epoch total loss 1.30419815\n",
      "Trained batch 86 batch loss 1.15017128 epoch total loss 1.30240715\n",
      "Trained batch 87 batch loss 1.22174788 epoch total loss 1.30148\n",
      "Trained batch 88 batch loss 1.23113978 epoch total loss 1.30068064\n",
      "Trained batch 89 batch loss 1.34462953 epoch total loss 1.30117452\n",
      "Trained batch 90 batch loss 1.41031492 epoch total loss 1.30238712\n",
      "Trained batch 91 batch loss 1.39212525 epoch total loss 1.30337334\n",
      "Trained batch 92 batch loss 1.15843654 epoch total loss 1.30179799\n",
      "Trained batch 93 batch loss 1.18488526 epoch total loss 1.3005408\n",
      "Trained batch 94 batch loss 1.29181647 epoch total loss 1.30044794\n",
      "Trained batch 95 batch loss 1.37275934 epoch total loss 1.30120909\n",
      "Trained batch 96 batch loss 1.25068855 epoch total loss 1.3006829\n",
      "Trained batch 97 batch loss 1.35023189 epoch total loss 1.30119371\n",
      "Trained batch 98 batch loss 1.40965962 epoch total loss 1.30230045\n",
      "Trained batch 99 batch loss 1.17988443 epoch total loss 1.3010639\n",
      "Trained batch 100 batch loss 1.24506509 epoch total loss 1.30050397\n",
      "Trained batch 101 batch loss 1.31783473 epoch total loss 1.30067563\n",
      "Trained batch 102 batch loss 1.28553188 epoch total loss 1.30052722\n",
      "Trained batch 103 batch loss 1.49040616 epoch total loss 1.30237067\n",
      "Trained batch 104 batch loss 1.37818289 epoch total loss 1.30309975\n",
      "Trained batch 105 batch loss 1.22867346 epoch total loss 1.30239081\n",
      "Trained batch 106 batch loss 1.23824739 epoch total loss 1.30178571\n",
      "Trained batch 107 batch loss 1.16085935 epoch total loss 1.30046868\n",
      "Trained batch 108 batch loss 1.3536576 epoch total loss 1.30096114\n",
      "Trained batch 109 batch loss 1.43751919 epoch total loss 1.30221391\n",
      "Trained batch 110 batch loss 1.35651648 epoch total loss 1.30270755\n",
      "Trained batch 111 batch loss 1.41977787 epoch total loss 1.30376232\n",
      "Trained batch 112 batch loss 1.40163183 epoch total loss 1.30463612\n",
      "Trained batch 113 batch loss 1.33079076 epoch total loss 1.30486763\n",
      "Trained batch 114 batch loss 1.36977589 epoch total loss 1.30543709\n",
      "Trained batch 115 batch loss 1.29291356 epoch total loss 1.30532813\n",
      "Trained batch 116 batch loss 1.38548601 epoch total loss 1.30601907\n",
      "Trained batch 117 batch loss 1.30340803 epoch total loss 1.30599678\n",
      "Trained batch 118 batch loss 1.3697927 epoch total loss 1.30653739\n",
      "Trained batch 119 batch loss 1.49277437 epoch total loss 1.30810237\n",
      "Trained batch 120 batch loss 1.42544806 epoch total loss 1.30908024\n",
      "Trained batch 121 batch loss 1.29874945 epoch total loss 1.30899489\n",
      "Trained batch 122 batch loss 1.34619856 epoch total loss 1.30929983\n",
      "Trained batch 123 batch loss 1.34353864 epoch total loss 1.30957818\n",
      "Trained batch 124 batch loss 1.23547411 epoch total loss 1.30898046\n",
      "Trained batch 125 batch loss 1.46096301 epoch total loss 1.3101964\n",
      "Trained batch 126 batch loss 1.44162297 epoch total loss 1.31123948\n",
      "Trained batch 127 batch loss 1.40570736 epoch total loss 1.31198323\n",
      "Trained batch 128 batch loss 1.31230605 epoch total loss 1.31198573\n",
      "Trained batch 129 batch loss 1.23608029 epoch total loss 1.31139731\n",
      "Trained batch 130 batch loss 1.13213301 epoch total loss 1.3100183\n",
      "Trained batch 131 batch loss 1.25991738 epoch total loss 1.30963588\n",
      "Trained batch 132 batch loss 1.21905708 epoch total loss 1.30894971\n",
      "Trained batch 133 batch loss 1.12609661 epoch total loss 1.30757487\n",
      "Trained batch 134 batch loss 1.11835456 epoch total loss 1.30616271\n",
      "Trained batch 135 batch loss 1.20318627 epoch total loss 1.30539989\n",
      "Trained batch 136 batch loss 1.24405026 epoch total loss 1.30494881\n",
      "Trained batch 137 batch loss 1.27116489 epoch total loss 1.30470216\n",
      "Trained batch 138 batch loss 1.33607018 epoch total loss 1.30492949\n",
      "Trained batch 139 batch loss 1.3451314 epoch total loss 1.30521882\n",
      "Trained batch 140 batch loss 1.33918548 epoch total loss 1.30546141\n",
      "Trained batch 141 batch loss 1.33051372 epoch total loss 1.30563915\n",
      "Trained batch 142 batch loss 1.1946522 epoch total loss 1.30485761\n",
      "Trained batch 143 batch loss 1.30110717 epoch total loss 1.30483139\n",
      "Trained batch 144 batch loss 1.30535614 epoch total loss 1.30483496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 145 batch loss 1.09029412 epoch total loss 1.30335546\n",
      "Trained batch 146 batch loss 1.06521177 epoch total loss 1.30172431\n",
      "Trained batch 147 batch loss 1.07234025 epoch total loss 1.30016398\n",
      "Trained batch 148 batch loss 1.32354343 epoch total loss 1.30032194\n",
      "Trained batch 149 batch loss 1.40727985 epoch total loss 1.3010397\n",
      "Trained batch 150 batch loss 1.44130647 epoch total loss 1.30197477\n",
      "Trained batch 151 batch loss 1.46378124 epoch total loss 1.30304635\n",
      "Trained batch 152 batch loss 1.37366056 epoch total loss 1.3035109\n",
      "Trained batch 153 batch loss 1.36314893 epoch total loss 1.3039006\n",
      "Trained batch 154 batch loss 1.37455249 epoch total loss 1.30435944\n",
      "Trained batch 155 batch loss 1.33430505 epoch total loss 1.30455267\n",
      "Trained batch 156 batch loss 1.34331012 epoch total loss 1.30480111\n",
      "Trained batch 157 batch loss 1.48251224 epoch total loss 1.305933\n",
      "Trained batch 158 batch loss 1.4600265 epoch total loss 1.30690825\n",
      "Trained batch 159 batch loss 1.43112111 epoch total loss 1.30768943\n",
      "Trained batch 160 batch loss 1.31822205 epoch total loss 1.30775523\n",
      "Trained batch 161 batch loss 1.06396258 epoch total loss 1.30624104\n",
      "Trained batch 162 batch loss 1.12545681 epoch total loss 1.30512512\n",
      "Trained batch 163 batch loss 1.29824126 epoch total loss 1.30508292\n",
      "Trained batch 164 batch loss 1.13625026 epoch total loss 1.30405343\n",
      "Trained batch 165 batch loss 1.04317749 epoch total loss 1.30247235\n",
      "Trained batch 166 batch loss 1.02013493 epoch total loss 1.30077159\n",
      "Trained batch 167 batch loss 1.05180514 epoch total loss 1.29928076\n",
      "Trained batch 168 batch loss 1.17174721 epoch total loss 1.29852164\n",
      "Trained batch 169 batch loss 1.25336206 epoch total loss 1.29825449\n",
      "Trained batch 170 batch loss 1.31356454 epoch total loss 1.29834449\n",
      "Trained batch 171 batch loss 1.29440379 epoch total loss 1.29832149\n",
      "Trained batch 172 batch loss 1.36752367 epoch total loss 1.29872382\n",
      "Trained batch 173 batch loss 1.31564617 epoch total loss 1.29882157\n",
      "Trained batch 174 batch loss 1.32680535 epoch total loss 1.2989825\n",
      "Trained batch 175 batch loss 1.29730833 epoch total loss 1.29897285\n",
      "Trained batch 176 batch loss 1.46737754 epoch total loss 1.29992974\n",
      "Trained batch 177 batch loss 1.27210593 epoch total loss 1.2997725\n",
      "Trained batch 178 batch loss 1.32679582 epoch total loss 1.29992437\n",
      "Trained batch 179 batch loss 1.38684523 epoch total loss 1.30040991\n",
      "Trained batch 180 batch loss 1.36091781 epoch total loss 1.30074608\n",
      "Trained batch 181 batch loss 1.27296448 epoch total loss 1.30059254\n",
      "Trained batch 182 batch loss 1.37764728 epoch total loss 1.30101597\n",
      "Trained batch 183 batch loss 1.45507312 epoch total loss 1.30185783\n",
      "Trained batch 184 batch loss 1.22404194 epoch total loss 1.30143487\n",
      "Trained batch 185 batch loss 1.32527041 epoch total loss 1.30156374\n",
      "Trained batch 186 batch loss 1.34532607 epoch total loss 1.30179894\n",
      "Trained batch 187 batch loss 1.32028246 epoch total loss 1.30189788\n",
      "Trained batch 188 batch loss 1.41006827 epoch total loss 1.30247319\n",
      "Trained batch 189 batch loss 1.333915 epoch total loss 1.30263948\n",
      "Trained batch 190 batch loss 1.27246737 epoch total loss 1.3024807\n",
      "Trained batch 191 batch loss 1.31758785 epoch total loss 1.30255973\n",
      "Trained batch 192 batch loss 1.30690861 epoch total loss 1.30258238\n",
      "Trained batch 193 batch loss 1.3654902 epoch total loss 1.30290842\n",
      "Trained batch 194 batch loss 1.34527874 epoch total loss 1.30312681\n",
      "Trained batch 195 batch loss 1.16468775 epoch total loss 1.3024168\n",
      "Trained batch 196 batch loss 1.37903547 epoch total loss 1.30280769\n",
      "Trained batch 197 batch loss 1.34906816 epoch total loss 1.30304253\n",
      "Trained batch 198 batch loss 1.3367672 epoch total loss 1.30321276\n",
      "Trained batch 199 batch loss 1.4234395 epoch total loss 1.30381691\n",
      "Trained batch 200 batch loss 1.4274168 epoch total loss 1.30443501\n",
      "Trained batch 201 batch loss 1.26573563 epoch total loss 1.30424249\n",
      "Trained batch 202 batch loss 1.33120012 epoch total loss 1.30437601\n",
      "Trained batch 203 batch loss 1.30383587 epoch total loss 1.30437326\n",
      "Trained batch 204 batch loss 1.39185572 epoch total loss 1.30480206\n",
      "Trained batch 205 batch loss 1.34576821 epoch total loss 1.30500185\n",
      "Trained batch 206 batch loss 1.27647567 epoch total loss 1.30486345\n",
      "Trained batch 207 batch loss 1.33510888 epoch total loss 1.3050096\n",
      "Trained batch 208 batch loss 1.273296 epoch total loss 1.30485713\n",
      "Trained batch 209 batch loss 1.18700182 epoch total loss 1.30429327\n",
      "Trained batch 210 batch loss 1.22714 epoch total loss 1.30392587\n",
      "Trained batch 211 batch loss 1.16902041 epoch total loss 1.30328643\n",
      "Trained batch 212 batch loss 1.18723524 epoch total loss 1.30273902\n",
      "Trained batch 213 batch loss 1.2568419 epoch total loss 1.30252349\n",
      "Trained batch 214 batch loss 1.34329545 epoch total loss 1.30271399\n",
      "Trained batch 215 batch loss 1.51811302 epoch total loss 1.30371594\n",
      "Trained batch 216 batch loss 1.46277308 epoch total loss 1.3044523\n",
      "Trained batch 217 batch loss 1.25353122 epoch total loss 1.3042177\n",
      "Trained batch 218 batch loss 1.16513658 epoch total loss 1.30357957\n",
      "Trained batch 219 batch loss 1.25899351 epoch total loss 1.30337608\n",
      "Trained batch 220 batch loss 1.34790587 epoch total loss 1.3035785\n",
      "Trained batch 221 batch loss 1.36324859 epoch total loss 1.30384851\n",
      "Trained batch 222 batch loss 1.34404504 epoch total loss 1.30402958\n",
      "Trained batch 223 batch loss 1.30625832 epoch total loss 1.30403948\n",
      "Trained batch 224 batch loss 1.33509123 epoch total loss 1.30417812\n",
      "Trained batch 225 batch loss 1.20584309 epoch total loss 1.3037411\n",
      "Trained batch 226 batch loss 1.22486186 epoch total loss 1.30339193\n",
      "Trained batch 227 batch loss 1.25433517 epoch total loss 1.30317593\n",
      "Trained batch 228 batch loss 1.30518627 epoch total loss 1.30318463\n",
      "Trained batch 229 batch loss 1.29222274 epoch total loss 1.30313683\n",
      "Trained batch 230 batch loss 1.38879418 epoch total loss 1.30350924\n",
      "Trained batch 231 batch loss 1.34004474 epoch total loss 1.30366743\n",
      "Trained batch 232 batch loss 1.33392322 epoch total loss 1.30379784\n",
      "Trained batch 233 batch loss 1.43521881 epoch total loss 1.30436194\n",
      "Trained batch 234 batch loss 1.43720293 epoch total loss 1.30492949\n",
      "Trained batch 235 batch loss 1.42320395 epoch total loss 1.30543292\n",
      "Trained batch 236 batch loss 1.3744235 epoch total loss 1.30572522\n",
      "Trained batch 237 batch loss 1.44378507 epoch total loss 1.30630779\n",
      "Trained batch 238 batch loss 1.38126516 epoch total loss 1.30662262\n",
      "Trained batch 239 batch loss 1.31776273 epoch total loss 1.30666924\n",
      "Trained batch 240 batch loss 1.33151269 epoch total loss 1.30677271\n",
      "Trained batch 241 batch loss 1.23747253 epoch total loss 1.30648518\n",
      "Trained batch 242 batch loss 1.39164281 epoch total loss 1.30683696\n",
      "Trained batch 243 batch loss 1.3686471 epoch total loss 1.30709136\n",
      "Trained batch 244 batch loss 1.35161531 epoch total loss 1.30727386\n",
      "Trained batch 245 batch loss 1.33254552 epoch total loss 1.30737698\n",
      "Trained batch 246 batch loss 1.3627162 epoch total loss 1.30760193\n",
      "Trained batch 247 batch loss 1.39866579 epoch total loss 1.30797052\n",
      "Trained batch 248 batch loss 1.30472481 epoch total loss 1.30795741\n",
      "Trained batch 249 batch loss 1.36641455 epoch total loss 1.30819225\n",
      "Trained batch 250 batch loss 1.3600421 epoch total loss 1.30839968\n",
      "Trained batch 251 batch loss 1.35553837 epoch total loss 1.30858743\n",
      "Trained batch 252 batch loss 1.39849114 epoch total loss 1.30894423\n",
      "Trained batch 253 batch loss 1.31399953 epoch total loss 1.30896413\n",
      "Trained batch 254 batch loss 1.29966617 epoch total loss 1.30892754\n",
      "Trained batch 255 batch loss 1.30630636 epoch total loss 1.30891728\n",
      "Trained batch 256 batch loss 1.25183368 epoch total loss 1.30869424\n",
      "Trained batch 257 batch loss 1.32717133 epoch total loss 1.30876613\n",
      "Trained batch 258 batch loss 1.2770648 epoch total loss 1.30864334\n",
      "Trained batch 259 batch loss 1.27973151 epoch total loss 1.30853164\n",
      "Trained batch 260 batch loss 1.21969724 epoch total loss 1.30819\n",
      "Trained batch 261 batch loss 1.15254855 epoch total loss 1.3075937\n",
      "Trained batch 262 batch loss 1.38098979 epoch total loss 1.30787385\n",
      "Trained batch 263 batch loss 1.52403152 epoch total loss 1.30869567\n",
      "Trained batch 264 batch loss 1.4898591 epoch total loss 1.30938184\n",
      "Trained batch 265 batch loss 1.50625062 epoch total loss 1.31012475\n",
      "Trained batch 266 batch loss 1.25798726 epoch total loss 1.30992889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 267 batch loss 1.32723808 epoch total loss 1.30999362\n",
      "Trained batch 268 batch loss 1.22555017 epoch total loss 1.30967855\n",
      "Trained batch 269 batch loss 1.24831378 epoch total loss 1.30945051\n",
      "Trained batch 270 batch loss 1.35930967 epoch total loss 1.30963516\n",
      "Trained batch 271 batch loss 1.26236308 epoch total loss 1.30946076\n",
      "Trained batch 272 batch loss 1.21449447 epoch total loss 1.3091116\n",
      "Trained batch 273 batch loss 1.23674464 epoch total loss 1.30884659\n",
      "Trained batch 274 batch loss 1.12574291 epoch total loss 1.30817831\n",
      "Trained batch 275 batch loss 1.13633668 epoch total loss 1.30755341\n",
      "Trained batch 276 batch loss 1.41364539 epoch total loss 1.30793774\n",
      "Trained batch 277 batch loss 1.5572505 epoch total loss 1.30883777\n",
      "Trained batch 278 batch loss 1.59407687 epoch total loss 1.30986381\n",
      "Trained batch 279 batch loss 1.45396888 epoch total loss 1.31038034\n",
      "Trained batch 280 batch loss 1.44478083 epoch total loss 1.3108604\n",
      "Trained batch 281 batch loss 1.48633623 epoch total loss 1.31148493\n",
      "Trained batch 282 batch loss 1.40923965 epoch total loss 1.31183159\n",
      "Trained batch 283 batch loss 1.37694657 epoch total loss 1.31206167\n",
      "Trained batch 284 batch loss 1.24884129 epoch total loss 1.31183898\n",
      "Trained batch 285 batch loss 1.25132501 epoch total loss 1.31162667\n",
      "Trained batch 286 batch loss 1.28856134 epoch total loss 1.31154609\n",
      "Trained batch 287 batch loss 1.27722788 epoch total loss 1.31142652\n",
      "Trained batch 288 batch loss 1.30021143 epoch total loss 1.31138754\n",
      "Trained batch 289 batch loss 1.24276733 epoch total loss 1.31115007\n",
      "Trained batch 290 batch loss 1.17028451 epoch total loss 1.3106643\n",
      "Trained batch 291 batch loss 1.08052337 epoch total loss 1.30987346\n",
      "Trained batch 292 batch loss 0.982609749 epoch total loss 1.30875266\n",
      "Trained batch 293 batch loss 1.22131765 epoch total loss 1.30845428\n",
      "Trained batch 294 batch loss 1.35152066 epoch total loss 1.30860078\n",
      "Trained batch 295 batch loss 1.35177779 epoch total loss 1.30874717\n",
      "Trained batch 296 batch loss 1.33220887 epoch total loss 1.30882645\n",
      "Trained batch 297 batch loss 1.36503851 epoch total loss 1.30901575\n",
      "Trained batch 298 batch loss 1.26642156 epoch total loss 1.30887282\n",
      "Trained batch 299 batch loss 1.18021059 epoch total loss 1.30844247\n",
      "Trained batch 300 batch loss 1.19393551 epoch total loss 1.30806077\n",
      "Trained batch 301 batch loss 1.19834399 epoch total loss 1.30769622\n",
      "Trained batch 302 batch loss 1.26819241 epoch total loss 1.30756545\n",
      "Trained batch 303 batch loss 1.30344522 epoch total loss 1.30755186\n",
      "Trained batch 304 batch loss 1.08089578 epoch total loss 1.30680621\n",
      "Trained batch 305 batch loss 1.01341343 epoch total loss 1.30584431\n",
      "Trained batch 306 batch loss 1.18852091 epoch total loss 1.30546093\n",
      "Trained batch 307 batch loss 1.18180692 epoch total loss 1.30505812\n",
      "Trained batch 308 batch loss 1.23462486 epoch total loss 1.30482936\n",
      "Trained batch 309 batch loss 1.26610899 epoch total loss 1.30470407\n",
      "Trained batch 310 batch loss 1.15456021 epoch total loss 1.30421984\n",
      "Trained batch 311 batch loss 1.23778033 epoch total loss 1.30400622\n",
      "Trained batch 312 batch loss 1.36838448 epoch total loss 1.30421257\n",
      "Trained batch 313 batch loss 1.20788658 epoch total loss 1.30390477\n",
      "Trained batch 314 batch loss 1.38500381 epoch total loss 1.3041631\n",
      "Trained batch 315 batch loss 1.19346321 epoch total loss 1.30381155\n",
      "Trained batch 316 batch loss 1.20847094 epoch total loss 1.30350983\n",
      "Trained batch 317 batch loss 1.14429057 epoch total loss 1.3030076\n",
      "Trained batch 318 batch loss 1.24021494 epoch total loss 1.30281007\n",
      "Trained batch 319 batch loss 1.3316319 epoch total loss 1.30290043\n",
      "Trained batch 320 batch loss 1.49982345 epoch total loss 1.30351579\n",
      "Trained batch 321 batch loss 1.49667776 epoch total loss 1.30411756\n",
      "Trained batch 322 batch loss 1.46326458 epoch total loss 1.3046118\n",
      "Trained batch 323 batch loss 1.25765347 epoch total loss 1.30446637\n",
      "Trained batch 324 batch loss 1.29515851 epoch total loss 1.30443776\n",
      "Trained batch 325 batch loss 1.08048606 epoch total loss 1.30374861\n",
      "Trained batch 326 batch loss 1.28782916 epoch total loss 1.30369985\n",
      "Trained batch 327 batch loss 1.30478525 epoch total loss 1.30370307\n",
      "Trained batch 328 batch loss 1.493294 epoch total loss 1.30428112\n",
      "Trained batch 329 batch loss 1.38248372 epoch total loss 1.30451882\n",
      "Trained batch 330 batch loss 1.30485868 epoch total loss 1.30451989\n",
      "Trained batch 331 batch loss 1.33562279 epoch total loss 1.30461383\n",
      "Trained batch 332 batch loss 1.36181557 epoch total loss 1.30478609\n",
      "Trained batch 333 batch loss 1.2765851 epoch total loss 1.30470145\n",
      "Trained batch 334 batch loss 1.30975342 epoch total loss 1.30471659\n",
      "Trained batch 335 batch loss 1.32215881 epoch total loss 1.30476856\n",
      "Trained batch 336 batch loss 1.26571512 epoch total loss 1.30465233\n",
      "Trained batch 337 batch loss 1.34378076 epoch total loss 1.30476844\n",
      "Trained batch 338 batch loss 1.3408556 epoch total loss 1.30487525\n",
      "Trained batch 339 batch loss 1.37240064 epoch total loss 1.30507445\n",
      "Trained batch 340 batch loss 1.37183261 epoch total loss 1.30527079\n",
      "Trained batch 341 batch loss 1.11952448 epoch total loss 1.30472612\n",
      "Trained batch 342 batch loss 1.15696216 epoch total loss 1.30429399\n",
      "Trained batch 343 batch loss 1.29078496 epoch total loss 1.30425453\n",
      "Trained batch 344 batch loss 1.23095012 epoch total loss 1.3040415\n",
      "Trained batch 345 batch loss 1.33140695 epoch total loss 1.3041209\n",
      "Trained batch 346 batch loss 1.43470597 epoch total loss 1.3044982\n",
      "Trained batch 347 batch loss 1.34211421 epoch total loss 1.30460656\n",
      "Trained batch 348 batch loss 1.37016606 epoch total loss 1.30479503\n",
      "Trained batch 349 batch loss 1.25137055 epoch total loss 1.30464196\n",
      "Trained batch 350 batch loss 1.26904178 epoch total loss 1.30454028\n",
      "Trained batch 351 batch loss 1.28476644 epoch total loss 1.30448389\n",
      "Trained batch 352 batch loss 1.22862554 epoch total loss 1.30426836\n",
      "Trained batch 353 batch loss 1.19230223 epoch total loss 1.30395114\n",
      "Trained batch 354 batch loss 1.19584775 epoch total loss 1.30364585\n",
      "Trained batch 355 batch loss 1.12793589 epoch total loss 1.30315089\n",
      "Trained batch 356 batch loss 1.21632576 epoch total loss 1.30290699\n",
      "Trained batch 357 batch loss 1.13605869 epoch total loss 1.30243957\n",
      "Trained batch 358 batch loss 1.25440264 epoch total loss 1.30230546\n",
      "Trained batch 359 batch loss 1.14499235 epoch total loss 1.30186725\n",
      "Trained batch 360 batch loss 1.32917 epoch total loss 1.30194306\n",
      "Trained batch 361 batch loss 1.49043322 epoch total loss 1.3024652\n",
      "Trained batch 362 batch loss 1.20958447 epoch total loss 1.30220866\n",
      "Trained batch 363 batch loss 1.22999477 epoch total loss 1.3020097\n",
      "Trained batch 364 batch loss 1.21223474 epoch total loss 1.30176306\n",
      "Trained batch 365 batch loss 1.27944958 epoch total loss 1.3017019\n",
      "Trained batch 366 batch loss 1.21252358 epoch total loss 1.30145836\n",
      "Trained batch 367 batch loss 1.28384566 epoch total loss 1.30141032\n",
      "Trained batch 368 batch loss 1.4097085 epoch total loss 1.30170453\n",
      "Trained batch 369 batch loss 1.290645 epoch total loss 1.3016746\n",
      "Trained batch 370 batch loss 1.21997023 epoch total loss 1.30145383\n",
      "Trained batch 371 batch loss 1.13832092 epoch total loss 1.30101407\n",
      "Trained batch 372 batch loss 1.25607777 epoch total loss 1.30089319\n",
      "Trained batch 373 batch loss 1.37483406 epoch total loss 1.30109155\n",
      "Trained batch 374 batch loss 1.28140914 epoch total loss 1.30103886\n",
      "Trained batch 375 batch loss 1.34940743 epoch total loss 1.30116785\n",
      "Trained batch 376 batch loss 1.36093473 epoch total loss 1.30132675\n",
      "Trained batch 377 batch loss 1.13704205 epoch total loss 1.30089104\n",
      "Trained batch 378 batch loss 1.21132112 epoch total loss 1.30065405\n",
      "Trained batch 379 batch loss 1.20747375 epoch total loss 1.30040824\n",
      "Trained batch 380 batch loss 1.22790337 epoch total loss 1.30021751\n",
      "Trained batch 381 batch loss 1.29041791 epoch total loss 1.30019176\n",
      "Trained batch 382 batch loss 1.29608583 epoch total loss 1.30018091\n",
      "Trained batch 383 batch loss 1.36125064 epoch total loss 1.30034041\n",
      "Trained batch 384 batch loss 1.31464875 epoch total loss 1.30037761\n",
      "Trained batch 385 batch loss 1.22869539 epoch total loss 1.3001914\n",
      "Trained batch 386 batch loss 1.25186729 epoch total loss 1.30006623\n",
      "Trained batch 387 batch loss 1.32925665 epoch total loss 1.30014169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 388 batch loss 1.29109383 epoch total loss 1.30011833\n",
      "Trained batch 389 batch loss 1.31186283 epoch total loss 1.30014849\n",
      "Trained batch 390 batch loss 1.32341564 epoch total loss 1.30020821\n",
      "Trained batch 391 batch loss 1.31590497 epoch total loss 1.30024838\n",
      "Trained batch 392 batch loss 1.24774384 epoch total loss 1.30011451\n",
      "Trained batch 393 batch loss 1.31339574 epoch total loss 1.30014825\n",
      "Trained batch 394 batch loss 1.3362149 epoch total loss 1.3002398\n",
      "Trained batch 395 batch loss 1.32643962 epoch total loss 1.30030608\n",
      "Trained batch 396 batch loss 1.29476285 epoch total loss 1.30029202\n",
      "Trained batch 397 batch loss 1.34500754 epoch total loss 1.30040479\n",
      "Trained batch 398 batch loss 1.27753687 epoch total loss 1.30034721\n",
      "Trained batch 399 batch loss 1.1467557 epoch total loss 1.29996228\n",
      "Trained batch 400 batch loss 1.29264855 epoch total loss 1.29994404\n",
      "Trained batch 401 batch loss 1.33955085 epoch total loss 1.30004275\n",
      "Trained batch 402 batch loss 1.42192495 epoch total loss 1.3003459\n",
      "Trained batch 403 batch loss 1.2968415 epoch total loss 1.3003372\n",
      "Trained batch 404 batch loss 1.338799 epoch total loss 1.30043244\n",
      "Trained batch 405 batch loss 1.30341232 epoch total loss 1.30043972\n",
      "Trained batch 406 batch loss 1.34408712 epoch total loss 1.30054736\n",
      "Trained batch 407 batch loss 1.24654984 epoch total loss 1.30041456\n",
      "Trained batch 408 batch loss 1.13779664 epoch total loss 1.30001605\n",
      "Trained batch 409 batch loss 1.31210279 epoch total loss 1.30004549\n",
      "Trained batch 410 batch loss 1.31064296 epoch total loss 1.30007148\n",
      "Trained batch 411 batch loss 1.31880927 epoch total loss 1.30011702\n",
      "Trained batch 412 batch loss 1.34349346 epoch total loss 1.30022228\n",
      "Trained batch 413 batch loss 1.32669294 epoch total loss 1.30028641\n",
      "Trained batch 414 batch loss 1.2542479 epoch total loss 1.30017531\n",
      "Trained batch 415 batch loss 1.32775211 epoch total loss 1.30024183\n",
      "Trained batch 416 batch loss 1.36761951 epoch total loss 1.30040371\n",
      "Trained batch 417 batch loss 1.24270415 epoch total loss 1.30026531\n",
      "Trained batch 418 batch loss 1.12203121 epoch total loss 1.2998389\n",
      "Trained batch 419 batch loss 1.35232687 epoch total loss 1.29996419\n",
      "Trained batch 420 batch loss 1.33973765 epoch total loss 1.30005884\n",
      "Trained batch 421 batch loss 1.25287628 epoch total loss 1.29994678\n",
      "Trained batch 422 batch loss 1.26342261 epoch total loss 1.29986024\n",
      "Trained batch 423 batch loss 1.31150532 epoch total loss 1.29988778\n",
      "Trained batch 424 batch loss 1.23964381 epoch total loss 1.29974568\n",
      "Trained batch 425 batch loss 1.19638598 epoch total loss 1.29950249\n",
      "Trained batch 426 batch loss 1.2506001 epoch total loss 1.29938781\n",
      "Trained batch 427 batch loss 1.2837106 epoch total loss 1.29935098\n",
      "Trained batch 428 batch loss 1.32547736 epoch total loss 1.29941213\n",
      "Trained batch 429 batch loss 1.29081595 epoch total loss 1.2993921\n",
      "Trained batch 430 batch loss 1.27105343 epoch total loss 1.29932618\n",
      "Trained batch 431 batch loss 1.1974194 epoch total loss 1.29908979\n",
      "Trained batch 432 batch loss 1.18476188 epoch total loss 1.29882514\n",
      "Trained batch 433 batch loss 1.26294374 epoch total loss 1.29874229\n",
      "Trained batch 434 batch loss 1.37637246 epoch total loss 1.29892111\n",
      "Trained batch 435 batch loss 1.39601064 epoch total loss 1.29914427\n",
      "Trained batch 436 batch loss 1.47886515 epoch total loss 1.29955649\n",
      "Trained batch 437 batch loss 1.3854661 epoch total loss 1.29975295\n",
      "Trained batch 438 batch loss 1.26323366 epoch total loss 1.29966962\n",
      "Trained batch 439 batch loss 1.28829134 epoch total loss 1.29964364\n",
      "Trained batch 440 batch loss 1.24975824 epoch total loss 1.29953027\n",
      "Trained batch 441 batch loss 1.23091483 epoch total loss 1.2993747\n",
      "Trained batch 442 batch loss 1.2349875 epoch total loss 1.29922903\n",
      "Trained batch 443 batch loss 1.3376174 epoch total loss 1.29931569\n",
      "Trained batch 444 batch loss 1.35748792 epoch total loss 1.2994467\n",
      "Trained batch 445 batch loss 1.22191274 epoch total loss 1.29927254\n",
      "Trained batch 446 batch loss 1.29157805 epoch total loss 1.29925525\n",
      "Trained batch 447 batch loss 1.24246645 epoch total loss 1.29912829\n",
      "Trained batch 448 batch loss 1.15131831 epoch total loss 1.29879832\n",
      "Trained batch 449 batch loss 1.28213739 epoch total loss 1.29876125\n",
      "Trained batch 450 batch loss 1.22878242 epoch total loss 1.29860568\n",
      "Trained batch 451 batch loss 1.195382 epoch total loss 1.2983768\n",
      "Trained batch 452 batch loss 1.19915354 epoch total loss 1.29815733\n",
      "Trained batch 453 batch loss 1.19747233 epoch total loss 1.29793501\n",
      "Trained batch 454 batch loss 1.19721019 epoch total loss 1.29771304\n",
      "Trained batch 455 batch loss 1.26333022 epoch total loss 1.29763746\n",
      "Trained batch 456 batch loss 1.18930709 epoch total loss 1.2974\n",
      "Trained batch 457 batch loss 1.27390385 epoch total loss 1.29734862\n",
      "Trained batch 458 batch loss 1.20124793 epoch total loss 1.29713869\n",
      "Trained batch 459 batch loss 1.22855055 epoch total loss 1.29698932\n",
      "Trained batch 460 batch loss 1.16557503 epoch total loss 1.2967037\n",
      "Trained batch 461 batch loss 1.11324537 epoch total loss 1.29630566\n",
      "Trained batch 462 batch loss 1.18825042 epoch total loss 1.29607177\n",
      "Trained batch 463 batch loss 1.18240666 epoch total loss 1.29582632\n",
      "Trained batch 464 batch loss 1.16618502 epoch total loss 1.29554701\n",
      "Trained batch 465 batch loss 1.17803454 epoch total loss 1.29529428\n",
      "Trained batch 466 batch loss 1.18436551 epoch total loss 1.29505622\n",
      "Trained batch 467 batch loss 1.1869396 epoch total loss 1.29482472\n",
      "Trained batch 468 batch loss 1.18158507 epoch total loss 1.29458284\n",
      "Trained batch 469 batch loss 1.45830536 epoch total loss 1.29493189\n",
      "Trained batch 470 batch loss 1.450454 epoch total loss 1.29526281\n",
      "Trained batch 471 batch loss 1.3586607 epoch total loss 1.29539728\n",
      "Trained batch 472 batch loss 1.3506875 epoch total loss 1.29551446\n",
      "Trained batch 473 batch loss 1.45902288 epoch total loss 1.29586029\n",
      "Trained batch 474 batch loss 1.39854741 epoch total loss 1.29607689\n",
      "Trained batch 475 batch loss 1.28250611 epoch total loss 1.2960484\n",
      "Trained batch 476 batch loss 1.21406651 epoch total loss 1.29587615\n",
      "Trained batch 477 batch loss 1.3591187 epoch total loss 1.29600871\n",
      "Trained batch 478 batch loss 1.42058396 epoch total loss 1.29626942\n",
      "Trained batch 479 batch loss 1.39138 epoch total loss 1.2964679\n",
      "Trained batch 480 batch loss 1.33939171 epoch total loss 1.29655731\n",
      "Trained batch 481 batch loss 1.21139896 epoch total loss 1.2963804\n",
      "Trained batch 482 batch loss 1.31840491 epoch total loss 1.29642606\n",
      "Trained batch 483 batch loss 1.37299514 epoch total loss 1.29658461\n",
      "Trained batch 484 batch loss 1.27638519 epoch total loss 1.29654288\n",
      "Trained batch 485 batch loss 1.15911925 epoch total loss 1.29625952\n",
      "Trained batch 486 batch loss 1.11013019 epoch total loss 1.2958765\n",
      "Trained batch 487 batch loss 1.16211867 epoch total loss 1.29560173\n",
      "Trained batch 488 batch loss 1.22533548 epoch total loss 1.29545784\n",
      "Trained batch 489 batch loss 1.14427543 epoch total loss 1.29514861\n",
      "Trained batch 490 batch loss 1.17359555 epoch total loss 1.29490054\n",
      "Trained batch 491 batch loss 1.22641754 epoch total loss 1.29476118\n",
      "Trained batch 492 batch loss 1.24634194 epoch total loss 1.29466271\n",
      "Trained batch 493 batch loss 1.29975939 epoch total loss 1.29467309\n",
      "Trained batch 494 batch loss 1.21180582 epoch total loss 1.29450524\n",
      "Trained batch 495 batch loss 1.19418645 epoch total loss 1.2943027\n",
      "Trained batch 496 batch loss 1.37630105 epoch total loss 1.29446793\n",
      "Trained batch 497 batch loss 1.33832932 epoch total loss 1.29455614\n",
      "Trained batch 498 batch loss 1.32972825 epoch total loss 1.29462671\n",
      "Trained batch 499 batch loss 1.30949569 epoch total loss 1.29465652\n",
      "Trained batch 500 batch loss 1.2982105 epoch total loss 1.29466367\n",
      "Trained batch 501 batch loss 1.29382253 epoch total loss 1.294662\n",
      "Trained batch 502 batch loss 1.2306844 epoch total loss 1.29453468\n",
      "Trained batch 503 batch loss 1.26464629 epoch total loss 1.2944752\n",
      "Trained batch 504 batch loss 1.21489429 epoch total loss 1.29431736\n",
      "Trained batch 505 batch loss 1.38483846 epoch total loss 1.29449654\n",
      "Trained batch 506 batch loss 1.41964638 epoch total loss 1.29474378\n",
      "Trained batch 507 batch loss 1.25628483 epoch total loss 1.29466796\n",
      "Trained batch 508 batch loss 1.2935431 epoch total loss 1.29466569\n",
      "Trained batch 509 batch loss 1.24790692 epoch total loss 1.2945739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 510 batch loss 1.34310353 epoch total loss 1.29466903\n",
      "Trained batch 511 batch loss 1.25855398 epoch total loss 1.29459834\n",
      "Trained batch 512 batch loss 1.27982187 epoch total loss 1.29456949\n",
      "Trained batch 513 batch loss 1.22746 epoch total loss 1.29443872\n",
      "Trained batch 514 batch loss 1.38440585 epoch total loss 1.29461372\n",
      "Trained batch 515 batch loss 1.38617468 epoch total loss 1.29479146\n",
      "Trained batch 516 batch loss 1.28665209 epoch total loss 1.29477584\n",
      "Trained batch 517 batch loss 1.34935164 epoch total loss 1.29488134\n",
      "Trained batch 518 batch loss 1.26500344 epoch total loss 1.29482377\n",
      "Trained batch 519 batch loss 1.34275126 epoch total loss 1.29491615\n",
      "Trained batch 520 batch loss 1.24542356 epoch total loss 1.2948209\n",
      "Trained batch 521 batch loss 1.36012936 epoch total loss 1.29494619\n",
      "Trained batch 522 batch loss 1.40276635 epoch total loss 1.29515278\n",
      "Trained batch 523 batch loss 1.25743818 epoch total loss 1.29508066\n",
      "Trained batch 524 batch loss 1.30662251 epoch total loss 1.29510272\n",
      "Trained batch 525 batch loss 1.28670216 epoch total loss 1.29508674\n",
      "Trained batch 526 batch loss 1.26027179 epoch total loss 1.29502046\n",
      "Trained batch 527 batch loss 1.35886562 epoch total loss 1.2951417\n",
      "Trained batch 528 batch loss 1.28777587 epoch total loss 1.29512775\n",
      "Trained batch 529 batch loss 1.26179028 epoch total loss 1.29506469\n",
      "Trained batch 530 batch loss 1.20619392 epoch total loss 1.29489696\n",
      "Trained batch 531 batch loss 1.27496934 epoch total loss 1.29485941\n",
      "Trained batch 532 batch loss 1.38828409 epoch total loss 1.29503512\n",
      "Trained batch 533 batch loss 1.39162922 epoch total loss 1.29521632\n",
      "Trained batch 534 batch loss 1.24264371 epoch total loss 1.29511774\n",
      "Trained batch 535 batch loss 1.35417354 epoch total loss 1.29522824\n",
      "Trained batch 536 batch loss 1.32124531 epoch total loss 1.29527664\n",
      "Trained batch 537 batch loss 1.34727621 epoch total loss 1.29537356\n",
      "Trained batch 538 batch loss 1.224769 epoch total loss 1.29524231\n",
      "Trained batch 539 batch loss 1.35986352 epoch total loss 1.29536223\n",
      "Trained batch 540 batch loss 1.2909497 epoch total loss 1.29535413\n",
      "Trained batch 541 batch loss 1.08757842 epoch total loss 1.29497\n",
      "Trained batch 542 batch loss 1.33204651 epoch total loss 1.29503846\n",
      "Trained batch 543 batch loss 1.39606071 epoch total loss 1.29522443\n",
      "Trained batch 544 batch loss 1.25225139 epoch total loss 1.29514551\n",
      "Trained batch 545 batch loss 1.29685092 epoch total loss 1.29514861\n",
      "Trained batch 546 batch loss 1.23791921 epoch total loss 1.29504383\n",
      "Trained batch 547 batch loss 1.25235498 epoch total loss 1.29496586\n",
      "Trained batch 548 batch loss 1.31693828 epoch total loss 1.29500592\n",
      "Trained batch 549 batch loss 1.28778422 epoch total loss 1.2949928\n",
      "Trained batch 550 batch loss 1.26480293 epoch total loss 1.29493797\n",
      "Trained batch 551 batch loss 1.30992258 epoch total loss 1.29496515\n",
      "Trained batch 552 batch loss 1.31085205 epoch total loss 1.294994\n",
      "Trained batch 553 batch loss 1.37676823 epoch total loss 1.29514182\n",
      "Trained batch 554 batch loss 1.34824538 epoch total loss 1.29523778\n",
      "Trained batch 555 batch loss 1.34900045 epoch total loss 1.29533458\n",
      "Trained batch 556 batch loss 1.47620821 epoch total loss 1.2956599\n",
      "Trained batch 557 batch loss 1.20568824 epoch total loss 1.29549837\n",
      "Trained batch 558 batch loss 1.27649438 epoch total loss 1.29546428\n",
      "Trained batch 559 batch loss 1.33304775 epoch total loss 1.29553163\n",
      "Trained batch 560 batch loss 1.19407654 epoch total loss 1.29535043\n",
      "Trained batch 561 batch loss 1.12658644 epoch total loss 1.29504955\n",
      "Trained batch 562 batch loss 1.15348446 epoch total loss 1.29479778\n",
      "Trained batch 563 batch loss 1.2760005 epoch total loss 1.2947644\n",
      "Trained batch 564 batch loss 1.37080097 epoch total loss 1.29489911\n",
      "Trained batch 565 batch loss 1.28022814 epoch total loss 1.29487312\n",
      "Trained batch 566 batch loss 1.24690247 epoch total loss 1.29478836\n",
      "Trained batch 567 batch loss 1.37252283 epoch total loss 1.29492545\n",
      "Trained batch 568 batch loss 1.26764548 epoch total loss 1.29487741\n",
      "Trained batch 569 batch loss 1.16382849 epoch total loss 1.2946471\n",
      "Trained batch 570 batch loss 1.21756327 epoch total loss 1.29451191\n",
      "Trained batch 571 batch loss 1.20091939 epoch total loss 1.294348\n",
      "Trained batch 572 batch loss 1.18690765 epoch total loss 1.29416013\n",
      "Trained batch 573 batch loss 1.14357829 epoch total loss 1.29389727\n",
      "Trained batch 574 batch loss 1.10631061 epoch total loss 1.29357052\n",
      "Trained batch 575 batch loss 1.11778355 epoch total loss 1.29326475\n",
      "Trained batch 576 batch loss 1.20081902 epoch total loss 1.29310429\n",
      "Trained batch 577 batch loss 1.19791508 epoch total loss 1.29293931\n",
      "Trained batch 578 batch loss 1.15044236 epoch total loss 1.29269278\n",
      "Trained batch 579 batch loss 1.21704817 epoch total loss 1.29256213\n",
      "Trained batch 580 batch loss 1.17052293 epoch total loss 1.29235172\n",
      "Trained batch 581 batch loss 1.45987415 epoch total loss 1.29264009\n",
      "Trained batch 582 batch loss 1.27894473 epoch total loss 1.29261661\n",
      "Trained batch 583 batch loss 1.33982432 epoch total loss 1.29269755\n",
      "Trained batch 584 batch loss 1.31031036 epoch total loss 1.29272771\n",
      "Trained batch 585 batch loss 1.34710205 epoch total loss 1.29282069\n",
      "Trained batch 586 batch loss 1.20196903 epoch total loss 1.2926656\n",
      "Trained batch 587 batch loss 1.25470471 epoch total loss 1.29260099\n",
      "Trained batch 588 batch loss 1.22657716 epoch total loss 1.29248869\n",
      "Trained batch 589 batch loss 1.2373656 epoch total loss 1.29239511\n",
      "Trained batch 590 batch loss 1.29831362 epoch total loss 1.29240513\n",
      "Trained batch 591 batch loss 1.26970375 epoch total loss 1.29236674\n",
      "Trained batch 592 batch loss 1.24023521 epoch total loss 1.29227865\n",
      "Trained batch 593 batch loss 1.24880373 epoch total loss 1.29220533\n",
      "Trained batch 594 batch loss 1.29719591 epoch total loss 1.29221368\n",
      "Trained batch 595 batch loss 1.25964665 epoch total loss 1.29215896\n",
      "Trained batch 596 batch loss 1.2606914 epoch total loss 1.29210615\n",
      "Trained batch 597 batch loss 1.33813608 epoch total loss 1.29218328\n",
      "Trained batch 598 batch loss 1.24192834 epoch total loss 1.29209924\n",
      "Trained batch 599 batch loss 1.3220315 epoch total loss 1.29214919\n",
      "Trained batch 600 batch loss 1.23466706 epoch total loss 1.29205346\n",
      "Trained batch 601 batch loss 1.40051782 epoch total loss 1.29223394\n",
      "Trained batch 602 batch loss 1.3250165 epoch total loss 1.2922883\n",
      "Trained batch 603 batch loss 1.42997599 epoch total loss 1.29251671\n",
      "Trained batch 604 batch loss 1.43317413 epoch total loss 1.29274952\n",
      "Trained batch 605 batch loss 1.36323905 epoch total loss 1.29286599\n",
      "Trained batch 606 batch loss 1.29975891 epoch total loss 1.29287744\n",
      "Trained batch 607 batch loss 1.25319719 epoch total loss 1.29281199\n",
      "Trained batch 608 batch loss 1.18921173 epoch total loss 1.29264164\n",
      "Trained batch 609 batch loss 1.1825974 epoch total loss 1.29246092\n",
      "Trained batch 610 batch loss 1.22542596 epoch total loss 1.29235101\n",
      "Trained batch 611 batch loss 1.14502549 epoch total loss 1.29210985\n",
      "Trained batch 612 batch loss 1.25180292 epoch total loss 1.29204404\n",
      "Trained batch 613 batch loss 1.24395406 epoch total loss 1.2919656\n",
      "Trained batch 614 batch loss 1.30032682 epoch total loss 1.29197931\n",
      "Trained batch 615 batch loss 1.39524746 epoch total loss 1.29214716\n",
      "Trained batch 616 batch loss 1.39778042 epoch total loss 1.2923187\n",
      "Trained batch 617 batch loss 1.309443 epoch total loss 1.29234648\n",
      "Trained batch 618 batch loss 1.26157391 epoch total loss 1.29229665\n",
      "Trained batch 619 batch loss 1.17122769 epoch total loss 1.29210103\n",
      "Trained batch 620 batch loss 1.12301481 epoch total loss 1.29182827\n",
      "Trained batch 621 batch loss 1.2293644 epoch total loss 1.29172766\n",
      "Trained batch 622 batch loss 1.3746264 epoch total loss 1.29186094\n",
      "Trained batch 623 batch loss 1.35430789 epoch total loss 1.29196119\n",
      "Trained batch 624 batch loss 1.28712356 epoch total loss 1.29195344\n",
      "Trained batch 625 batch loss 1.29004 epoch total loss 1.29195035\n",
      "Trained batch 626 batch loss 1.22853708 epoch total loss 1.29184902\n",
      "Trained batch 627 batch loss 1.28516316 epoch total loss 1.29183841\n",
      "Trained batch 628 batch loss 1.20365834 epoch total loss 1.29169798\n",
      "Trained batch 629 batch loss 1.21910274 epoch total loss 1.29158258\n",
      "Trained batch 630 batch loss 1.5135479 epoch total loss 1.29193497\n",
      "Trained batch 631 batch loss 1.4303124 epoch total loss 1.29215419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 632 batch loss 1.3313961 epoch total loss 1.2922163\n",
      "Trained batch 633 batch loss 1.46207261 epoch total loss 1.29248476\n",
      "Trained batch 634 batch loss 1.43346941 epoch total loss 1.29270709\n",
      "Trained batch 635 batch loss 1.31420362 epoch total loss 1.29274094\n",
      "Trained batch 636 batch loss 1.22818518 epoch total loss 1.29263949\n",
      "Trained batch 637 batch loss 1.25895166 epoch total loss 1.29258668\n",
      "Trained batch 638 batch loss 1.1878804 epoch total loss 1.29242253\n",
      "Trained batch 639 batch loss 1.45158315 epoch total loss 1.29267156\n",
      "Trained batch 640 batch loss 1.25618315 epoch total loss 1.29261458\n",
      "Trained batch 641 batch loss 1.34388304 epoch total loss 1.29269457\n",
      "Trained batch 642 batch loss 1.37782145 epoch total loss 1.29282713\n",
      "Trained batch 643 batch loss 1.33281422 epoch total loss 1.29288936\n",
      "Trained batch 644 batch loss 1.34485579 epoch total loss 1.29297\n",
      "Trained batch 645 batch loss 1.2716136 epoch total loss 1.2929368\n",
      "Trained batch 646 batch loss 1.2546742 epoch total loss 1.29287767\n",
      "Trained batch 647 batch loss 1.16663182 epoch total loss 1.29268253\n",
      "Trained batch 648 batch loss 1.28228915 epoch total loss 1.29266644\n",
      "Trained batch 649 batch loss 1.34092832 epoch total loss 1.29274082\n",
      "Trained batch 650 batch loss 1.23484063 epoch total loss 1.29265189\n",
      "Trained batch 651 batch loss 1.2438302 epoch total loss 1.29257691\n",
      "Trained batch 652 batch loss 1.28385687 epoch total loss 1.29256356\n",
      "Trained batch 653 batch loss 1.33760238 epoch total loss 1.29263246\n",
      "Trained batch 654 batch loss 1.27062809 epoch total loss 1.29259884\n",
      "Trained batch 655 batch loss 1.15569687 epoch total loss 1.29238975\n",
      "Trained batch 656 batch loss 1.26123953 epoch total loss 1.29234231\n",
      "Trained batch 657 batch loss 1.17063236 epoch total loss 1.29215705\n",
      "Trained batch 658 batch loss 1.20999849 epoch total loss 1.29203224\n",
      "Trained batch 659 batch loss 1.2905221 epoch total loss 1.29203\n",
      "Trained batch 660 batch loss 1.25857127 epoch total loss 1.29197919\n",
      "Trained batch 661 batch loss 1.26866078 epoch total loss 1.29194403\n",
      "Trained batch 662 batch loss 1.2104193 epoch total loss 1.29182088\n",
      "Trained batch 663 batch loss 1.19415903 epoch total loss 1.29167354\n",
      "Trained batch 664 batch loss 1.24000084 epoch total loss 1.2915957\n",
      "Trained batch 665 batch loss 1.2341187 epoch total loss 1.29150927\n",
      "Trained batch 666 batch loss 1.19892704 epoch total loss 1.29137027\n",
      "Trained batch 667 batch loss 1.28114831 epoch total loss 1.29135489\n",
      "Trained batch 668 batch loss 1.2472086 epoch total loss 1.29128885\n",
      "Trained batch 669 batch loss 1.21721339 epoch total loss 1.29117811\n",
      "Trained batch 670 batch loss 1.18709815 epoch total loss 1.29102278\n",
      "Trained batch 671 batch loss 1.28652668 epoch total loss 1.29101598\n",
      "Trained batch 672 batch loss 1.30799365 epoch total loss 1.29104125\n",
      "Trained batch 673 batch loss 1.37806249 epoch total loss 1.29117048\n",
      "Trained batch 674 batch loss 1.33116817 epoch total loss 1.29122984\n",
      "Trained batch 675 batch loss 1.24595106 epoch total loss 1.29116285\n",
      "Trained batch 676 batch loss 1.31756687 epoch total loss 1.29120195\n",
      "Trained batch 677 batch loss 1.32424521 epoch total loss 1.29125071\n",
      "Trained batch 678 batch loss 1.2910136 epoch total loss 1.29125035\n",
      "Trained batch 679 batch loss 1.29094815 epoch total loss 1.29124987\n",
      "Trained batch 680 batch loss 1.29871559 epoch total loss 1.29126084\n",
      "Trained batch 681 batch loss 1.31212425 epoch total loss 1.29129148\n",
      "Trained batch 682 batch loss 1.44157052 epoch total loss 1.29151189\n",
      "Trained batch 683 batch loss 1.35600615 epoch total loss 1.29160631\n",
      "Trained batch 684 batch loss 1.42559576 epoch total loss 1.29180217\n",
      "Trained batch 685 batch loss 1.25898707 epoch total loss 1.29175425\n",
      "Trained batch 686 batch loss 1.20870447 epoch total loss 1.29163313\n",
      "Trained batch 687 batch loss 1.34032369 epoch total loss 1.29170406\n",
      "Trained batch 688 batch loss 1.26623237 epoch total loss 1.2916671\n",
      "Trained batch 689 batch loss 1.27926 epoch total loss 1.29164898\n",
      "Trained batch 690 batch loss 1.23344672 epoch total loss 1.2915647\n",
      "Trained batch 691 batch loss 1.33080971 epoch total loss 1.29162145\n",
      "Trained batch 692 batch loss 1.21901369 epoch total loss 1.29151654\n",
      "Trained batch 693 batch loss 1.24279261 epoch total loss 1.29144621\n",
      "Trained batch 694 batch loss 1.26811695 epoch total loss 1.29141259\n",
      "Trained batch 695 batch loss 1.35396886 epoch total loss 1.29150259\n",
      "Trained batch 696 batch loss 1.24977338 epoch total loss 1.29144263\n",
      "Trained batch 697 batch loss 1.13629508 epoch total loss 1.29122007\n",
      "Trained batch 698 batch loss 1.20333755 epoch total loss 1.29109406\n",
      "Trained batch 699 batch loss 1.13024259 epoch total loss 1.29086399\n",
      "Trained batch 700 batch loss 1.11448598 epoch total loss 1.29061198\n",
      "Trained batch 701 batch loss 1.1986953 epoch total loss 1.29048085\n",
      "Trained batch 702 batch loss 1.17358983 epoch total loss 1.29031432\n",
      "Trained batch 703 batch loss 1.2335676 epoch total loss 1.29023361\n",
      "Trained batch 704 batch loss 1.37845 epoch total loss 1.29035902\n",
      "Trained batch 705 batch loss 1.3028456 epoch total loss 1.29037666\n",
      "Trained batch 706 batch loss 1.38294315 epoch total loss 1.29050779\n",
      "Trained batch 707 batch loss 1.2634629 epoch total loss 1.29046965\n",
      "Trained batch 708 batch loss 1.25035334 epoch total loss 1.2904129\n",
      "Trained batch 709 batch loss 1.15554583 epoch total loss 1.29022264\n",
      "Trained batch 710 batch loss 1.23936701 epoch total loss 1.29015112\n",
      "Trained batch 711 batch loss 1.34772444 epoch total loss 1.29023206\n",
      "Trained batch 712 batch loss 1.43589783 epoch total loss 1.29043663\n",
      "Trained batch 713 batch loss 1.33379209 epoch total loss 1.29049742\n",
      "Trained batch 714 batch loss 1.25561082 epoch total loss 1.29044867\n",
      "Trained batch 715 batch loss 1.16616869 epoch total loss 1.29027486\n",
      "Trained batch 716 batch loss 1.22060776 epoch total loss 1.29017746\n",
      "Trained batch 717 batch loss 1.19757581 epoch total loss 1.29004836\n",
      "Trained batch 718 batch loss 1.27313638 epoch total loss 1.29002476\n",
      "Trained batch 719 batch loss 1.24371505 epoch total loss 1.28996038\n",
      "Trained batch 720 batch loss 1.24504876 epoch total loss 1.28989804\n",
      "Trained batch 721 batch loss 1.31074011 epoch total loss 1.28992689\n",
      "Trained batch 722 batch loss 1.44846344 epoch total loss 1.29014647\n",
      "Trained batch 723 batch loss 1.56266952 epoch total loss 1.29052341\n",
      "Trained batch 724 batch loss 1.41465068 epoch total loss 1.29069495\n",
      "Trained batch 725 batch loss 1.35851681 epoch total loss 1.29078853\n",
      "Trained batch 726 batch loss 1.32916832 epoch total loss 1.29084134\n",
      "Trained batch 727 batch loss 1.4030447 epoch total loss 1.2909956\n",
      "Trained batch 728 batch loss 1.27805281 epoch total loss 1.29097795\n",
      "Trained batch 729 batch loss 1.31975627 epoch total loss 1.29101741\n",
      "Trained batch 730 batch loss 1.31170249 epoch total loss 1.29104567\n",
      "Trained batch 731 batch loss 1.19006181 epoch total loss 1.29090762\n",
      "Trained batch 732 batch loss 1.36134088 epoch total loss 1.29100382\n",
      "Trained batch 733 batch loss 1.36245656 epoch total loss 1.29110122\n",
      "Trained batch 734 batch loss 1.27687812 epoch total loss 1.29108179\n",
      "Trained batch 735 batch loss 1.21256173 epoch total loss 1.29097497\n",
      "Trained batch 736 batch loss 1.23166275 epoch total loss 1.29089451\n",
      "Trained batch 737 batch loss 1.25552034 epoch total loss 1.29084647\n",
      "Trained batch 738 batch loss 1.23985243 epoch total loss 1.29077733\n",
      "Trained batch 739 batch loss 1.19598436 epoch total loss 1.29064906\n",
      "Trained batch 740 batch loss 1.19252324 epoch total loss 1.2905165\n",
      "Trained batch 741 batch loss 1.31494749 epoch total loss 1.2905494\n",
      "Trained batch 742 batch loss 1.30276322 epoch total loss 1.29056585\n",
      "Trained batch 743 batch loss 1.25260735 epoch total loss 1.29051483\n",
      "Trained batch 744 batch loss 1.18540347 epoch total loss 1.29037356\n",
      "Trained batch 745 batch loss 1.23516798 epoch total loss 1.29029942\n",
      "Trained batch 746 batch loss 1.22823608 epoch total loss 1.29021621\n",
      "Trained batch 747 batch loss 1.25833797 epoch total loss 1.29017353\n",
      "Trained batch 748 batch loss 1.35570014 epoch total loss 1.29026115\n",
      "Trained batch 749 batch loss 1.25890064 epoch total loss 1.29021931\n",
      "Trained batch 750 batch loss 1.21509171 epoch total loss 1.29011917\n",
      "Trained batch 751 batch loss 1.23378098 epoch total loss 1.29004407\n",
      "Trained batch 752 batch loss 1.28652573 epoch total loss 1.29003942\n",
      "Trained batch 753 batch loss 1.29130447 epoch total loss 1.29004109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 754 batch loss 1.30481398 epoch total loss 1.29006064\n",
      "Trained batch 755 batch loss 1.26605964 epoch total loss 1.29002893\n",
      "Trained batch 756 batch loss 1.18726122 epoch total loss 1.28989291\n",
      "Trained batch 757 batch loss 1.24175453 epoch total loss 1.28982937\n",
      "Trained batch 758 batch loss 1.23688495 epoch total loss 1.28975952\n",
      "Trained batch 759 batch loss 1.11188817 epoch total loss 1.28952515\n",
      "Trained batch 760 batch loss 1.24168789 epoch total loss 1.28946221\n",
      "Trained batch 761 batch loss 1.33392239 epoch total loss 1.28952062\n",
      "Trained batch 762 batch loss 1.29341471 epoch total loss 1.28952575\n",
      "Trained batch 763 batch loss 1.35236466 epoch total loss 1.289608\n",
      "Trained batch 764 batch loss 1.31475413 epoch total loss 1.28964102\n",
      "Trained batch 765 batch loss 1.3041265 epoch total loss 1.28966\n",
      "Trained batch 766 batch loss 1.41540194 epoch total loss 1.28982413\n",
      "Trained batch 767 batch loss 1.32164669 epoch total loss 1.28986561\n",
      "Trained batch 768 batch loss 1.23076594 epoch total loss 1.2897886\n",
      "Trained batch 769 batch loss 1.30226028 epoch total loss 1.28980482\n",
      "Trained batch 770 batch loss 1.16819334 epoch total loss 1.28964698\n",
      "Trained batch 771 batch loss 1.29829252 epoch total loss 1.28965807\n",
      "Trained batch 772 batch loss 1.27221894 epoch total loss 1.28963554\n",
      "Trained batch 773 batch loss 1.15824604 epoch total loss 1.28946555\n",
      "Trained batch 774 batch loss 1.1614629 epoch total loss 1.2893002\n",
      "Trained batch 775 batch loss 1.19177139 epoch total loss 1.28917432\n",
      "Trained batch 776 batch loss 1.19309247 epoch total loss 1.28905058\n",
      "Trained batch 777 batch loss 1.29022956 epoch total loss 1.28905201\n",
      "Trained batch 778 batch loss 1.26333642 epoch total loss 1.28901899\n",
      "Trained batch 779 batch loss 1.26492703 epoch total loss 1.28898811\n",
      "Trained batch 780 batch loss 1.25257421 epoch total loss 1.2889415\n",
      "Trained batch 781 batch loss 1.34940946 epoch total loss 1.28901887\n",
      "Trained batch 782 batch loss 1.27404284 epoch total loss 1.2889998\n",
      "Trained batch 783 batch loss 1.30532432 epoch total loss 1.28902054\n",
      "Trained batch 784 batch loss 1.24497616 epoch total loss 1.28896439\n",
      "Trained batch 785 batch loss 1.18237042 epoch total loss 1.28882861\n",
      "Trained batch 786 batch loss 1.11845672 epoch total loss 1.28861189\n",
      "Trained batch 787 batch loss 1.25399888 epoch total loss 1.2885679\n",
      "Trained batch 788 batch loss 1.25018811 epoch total loss 1.28851926\n",
      "Trained batch 789 batch loss 1.39763963 epoch total loss 1.28865755\n",
      "Trained batch 790 batch loss 1.50851905 epoch total loss 1.2889359\n",
      "Trained batch 791 batch loss 1.52595901 epoch total loss 1.28923547\n",
      "Trained batch 792 batch loss 1.3859303 epoch total loss 1.28935754\n",
      "Trained batch 793 batch loss 1.34286714 epoch total loss 1.28942513\n",
      "Trained batch 794 batch loss 1.2829411 epoch total loss 1.28941691\n",
      "Trained batch 795 batch loss 1.14769351 epoch total loss 1.28923857\n",
      "Trained batch 796 batch loss 1.19525993 epoch total loss 1.28912067\n",
      "Trained batch 797 batch loss 1.31769931 epoch total loss 1.28915656\n",
      "Trained batch 798 batch loss 1.25508785 epoch total loss 1.28911388\n",
      "Trained batch 799 batch loss 1.34382367 epoch total loss 1.28918242\n",
      "Trained batch 800 batch loss 1.32128644 epoch total loss 1.2892226\n",
      "Trained batch 801 batch loss 1.25926232 epoch total loss 1.28918517\n",
      "Trained batch 802 batch loss 1.27968287 epoch total loss 1.28917336\n",
      "Trained batch 803 batch loss 1.28249335 epoch total loss 1.2891649\n",
      "Trained batch 804 batch loss 1.35520041 epoch total loss 1.28924716\n",
      "Trained batch 805 batch loss 1.28737426 epoch total loss 1.28924477\n",
      "Trained batch 806 batch loss 1.32533813 epoch total loss 1.28928947\n",
      "Trained batch 807 batch loss 1.32420206 epoch total loss 1.28933287\n",
      "Trained batch 808 batch loss 1.07778823 epoch total loss 1.28907096\n",
      "Trained batch 809 batch loss 1.10188866 epoch total loss 1.28883958\n",
      "Trained batch 810 batch loss 1.28516984 epoch total loss 1.28883505\n",
      "Trained batch 811 batch loss 1.27331495 epoch total loss 1.28881598\n",
      "Trained batch 812 batch loss 1.35792863 epoch total loss 1.28890109\n",
      "Trained batch 813 batch loss 1.36867142 epoch total loss 1.28899908\n",
      "Trained batch 814 batch loss 1.54371786 epoch total loss 1.28931201\n",
      "Trained batch 815 batch loss 1.32193077 epoch total loss 1.28935206\n",
      "Trained batch 816 batch loss 1.30338979 epoch total loss 1.28936923\n",
      "Trained batch 817 batch loss 1.38561773 epoch total loss 1.289487\n",
      "Trained batch 818 batch loss 1.27668524 epoch total loss 1.28947139\n",
      "Trained batch 819 batch loss 1.29273558 epoch total loss 1.28947532\n",
      "Trained batch 820 batch loss 1.32753611 epoch total loss 1.28952169\n",
      "Trained batch 821 batch loss 1.40297568 epoch total loss 1.28965986\n",
      "Trained batch 822 batch loss 1.28929949 epoch total loss 1.2896595\n",
      "Trained batch 823 batch loss 1.31406558 epoch total loss 1.28968918\n",
      "Trained batch 824 batch loss 1.27990687 epoch total loss 1.28967726\n",
      "Trained batch 825 batch loss 1.34656143 epoch total loss 1.28974628\n",
      "Trained batch 826 batch loss 1.33369863 epoch total loss 1.28979945\n",
      "Trained batch 827 batch loss 1.09647894 epoch total loss 1.28956568\n",
      "Trained batch 828 batch loss 1.22116852 epoch total loss 1.28948307\n",
      "Trained batch 829 batch loss 1.14607263 epoch total loss 1.28931022\n",
      "Trained batch 830 batch loss 1.1633004 epoch total loss 1.28915834\n",
      "Trained batch 831 batch loss 1.18002033 epoch total loss 1.28902709\n",
      "Trained batch 832 batch loss 1.19939733 epoch total loss 1.28891933\n",
      "Trained batch 833 batch loss 1.36817932 epoch total loss 1.28901446\n",
      "Trained batch 834 batch loss 1.25357223 epoch total loss 1.2889719\n",
      "Trained batch 835 batch loss 1.31984448 epoch total loss 1.28900886\n",
      "Trained batch 836 batch loss 1.27622926 epoch total loss 1.2889936\n",
      "Trained batch 837 batch loss 1.37268376 epoch total loss 1.28909361\n",
      "Trained batch 838 batch loss 1.46901059 epoch total loss 1.28930819\n",
      "Trained batch 839 batch loss 1.5129776 epoch total loss 1.28957474\n",
      "Trained batch 840 batch loss 1.51817513 epoch total loss 1.2898469\n",
      "Trained batch 841 batch loss 1.21014 epoch total loss 1.28975213\n",
      "Trained batch 842 batch loss 1.13721907 epoch total loss 1.28957093\n",
      "Trained batch 843 batch loss 1.36952901 epoch total loss 1.2896657\n",
      "Trained batch 852 batch loss 1.39655125 epoch total loss 1.2902137\n",
      "Trained batch 853 batch loss 1.31795979 epoch total loss 1.29024625\n",
      "Trained batch 854 batch loss 1.30980587 epoch total loss 1.29026914\n",
      "Trained batch 855 batch loss 1.25314903 epoch total loss 1.29022586\n",
      "Trained batch 856 batch loss 1.28783071 epoch total loss 1.290223\n",
      "Trained batch 857 batch loss 1.36895823 epoch total loss 1.29031491\n",
      "Trained batch 858 batch loss 1.40008223 epoch total loss 1.29044282\n",
      "Trained batch 859 batch loss 1.50108886 epoch total loss 1.29068804\n",
      "Trained batch 860 batch loss 1.3583777 epoch total loss 1.29076684\n",
      "Trained batch 861 batch loss 1.39365554 epoch total loss 1.29088628\n",
      "Trained batch 862 batch loss 1.34405529 epoch total loss 1.29094803\n",
      "Trained batch 863 batch loss 1.38816345 epoch total loss 1.29106081\n",
      "Trained batch 864 batch loss 1.46181595 epoch total loss 1.29125834\n",
      "Trained batch 865 batch loss 1.44935882 epoch total loss 1.29144108\n",
      "Trained batch 866 batch loss 1.31312656 epoch total loss 1.29146612\n",
      "Trained batch 867 batch loss 1.1666075 epoch total loss 1.29132211\n",
      "Trained batch 868 batch loss 1.48816264 epoch total loss 1.29154897\n",
      "Trained batch 869 batch loss 1.27094078 epoch total loss 1.29152524\n",
      "Trained batch 870 batch loss 1.23343062 epoch total loss 1.29145849\n",
      "Trained batch 871 batch loss 1.18346691 epoch total loss 1.29133451\n",
      "Trained batch 872 batch loss 1.260324 epoch total loss 1.29129899\n",
      "Trained batch 873 batch loss 1.38413119 epoch total loss 1.29140532\n",
      "Trained batch 874 batch loss 1.44505572 epoch total loss 1.29158115\n",
      "Trained batch 875 batch loss 1.30902696 epoch total loss 1.29160118\n",
      "Trained batch 876 batch loss 1.33668041 epoch total loss 1.29165256\n",
      "Trained batch 877 batch loss 1.31394744 epoch total loss 1.29167807\n",
      "Trained batch 878 batch loss 1.26466453 epoch total loss 1.2916472\n",
      "Trained batch 879 batch loss 1.30237103 epoch total loss 1.29165947\n",
      "Trained batch 880 batch loss 1.25432742 epoch total loss 1.29161692\n",
      "Trained batch 881 batch loss 1.2654686 epoch total loss 1.29158735\n",
      "Trained batch 882 batch loss 1.22622025 epoch total loss 1.2915132\n",
      "Trained batch 883 batch loss 1.21478689 epoch total loss 1.2914263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 884 batch loss 1.37494659 epoch total loss 1.29152083\n",
      "Trained batch 885 batch loss 1.35053527 epoch total loss 1.29158759\n",
      "Trained batch 886 batch loss 1.25388074 epoch total loss 1.29154515\n",
      "Trained batch 887 batch loss 1.34717822 epoch total loss 1.29160786\n",
      "Trained batch 888 batch loss 1.29691696 epoch total loss 1.2916137\n",
      "Trained batch 889 batch loss 1.34288692 epoch total loss 1.2916714\n",
      "Trained batch 890 batch loss 1.27392757 epoch total loss 1.29165149\n",
      "Trained batch 891 batch loss 1.35173368 epoch total loss 1.29171884\n",
      "Trained batch 892 batch loss 1.3082819 epoch total loss 1.29173732\n",
      "Trained batch 893 batch loss 1.29774225 epoch total loss 1.29174411\n",
      "Trained batch 894 batch loss 1.49692321 epoch total loss 1.29197359\n",
      "Trained batch 895 batch loss 1.44916344 epoch total loss 1.29214931\n",
      "Trained batch 896 batch loss 1.5874933 epoch total loss 1.29247892\n",
      "Trained batch 897 batch loss 1.31316519 epoch total loss 1.29250193\n",
      "Trained batch 898 batch loss 1.17384219 epoch total loss 1.29236984\n",
      "Trained batch 899 batch loss 1.18790603 epoch total loss 1.29225361\n",
      "Trained batch 900 batch loss 1.25830233 epoch total loss 1.29221582\n",
      "Trained batch 901 batch loss 1.18924022 epoch total loss 1.2921015\n",
      "Trained batch 902 batch loss 1.12754107 epoch total loss 1.29191911\n",
      "Trained batch 903 batch loss 1.21970296 epoch total loss 1.29183912\n",
      "Trained batch 904 batch loss 1.17297482 epoch total loss 1.29170763\n",
      "Trained batch 905 batch loss 1.09004 epoch total loss 1.29148483\n",
      "Trained batch 906 batch loss 1.11155379 epoch total loss 1.29128635\n",
      "Trained batch 907 batch loss 1.16753626 epoch total loss 1.29114985\n",
      "Trained batch 908 batch loss 1.27139783 epoch total loss 1.29112804\n",
      "Trained batch 909 batch loss 1.17152262 epoch total loss 1.29099643\n",
      "Trained batch 910 batch loss 1.084378 epoch total loss 1.29076934\n",
      "Trained batch 911 batch loss 1.13521755 epoch total loss 1.29059863\n",
      "Trained batch 912 batch loss 1.2533412 epoch total loss 1.29055774\n",
      "Trained batch 913 batch loss 1.33514595 epoch total loss 1.29060662\n",
      "Trained batch 914 batch loss 1.44652724 epoch total loss 1.29077721\n",
      "Trained batch 915 batch loss 1.61533535 epoch total loss 1.29113197\n",
      "Trained batch 916 batch loss 1.57958269 epoch total loss 1.29144692\n",
      "Trained batch 917 batch loss 1.42462277 epoch total loss 1.29159212\n",
      "Trained batch 918 batch loss 1.43751276 epoch total loss 1.29175115\n",
      "Trained batch 919 batch loss 1.22761154 epoch total loss 1.29168141\n",
      "Trained batch 920 batch loss 1.1330142 epoch total loss 1.29150891\n",
      "Trained batch 921 batch loss 1.2746172 epoch total loss 1.29149067\n",
      "Trained batch 922 batch loss 1.30623627 epoch total loss 1.29150665\n",
      "Trained batch 923 batch loss 1.40634727 epoch total loss 1.2916311\n",
      "Trained batch 924 batch loss 1.31654954 epoch total loss 1.29165804\n",
      "Trained batch 925 batch loss 1.25344598 epoch total loss 1.2916168\n",
      "Trained batch 926 batch loss 1.20473 epoch total loss 1.29152286\n",
      "Trained batch 927 batch loss 1.2100569 epoch total loss 1.291435\n",
      "Trained batch 928 batch loss 1.17353392 epoch total loss 1.29130805\n",
      "Trained batch 929 batch loss 1.22702169 epoch total loss 1.2912389\n",
      "Trained batch 930 batch loss 1.23814332 epoch total loss 1.2911818\n",
      "Trained batch 931 batch loss 1.18576324 epoch total loss 1.29106855\n",
      "Trained batch 932 batch loss 1.22284317 epoch total loss 1.29099548\n",
      "Trained batch 933 batch loss 1.41851783 epoch total loss 1.29113209\n",
      "Trained batch 934 batch loss 1.33475208 epoch total loss 1.2911787\n",
      "Trained batch 935 batch loss 1.38148856 epoch total loss 1.29127526\n",
      "Trained batch 936 batch loss 1.40000772 epoch total loss 1.29139149\n",
      "Trained batch 937 batch loss 1.3410666 epoch total loss 1.29144454\n",
      "Trained batch 938 batch loss 1.35355628 epoch total loss 1.2915107\n",
      "Trained batch 939 batch loss 1.2615788 epoch total loss 1.29147887\n",
      "Trained batch 940 batch loss 1.24793363 epoch total loss 1.2914325\n",
      "Trained batch 941 batch loss 1.23234653 epoch total loss 1.29136968\n",
      "Trained batch 942 batch loss 1.16672993 epoch total loss 1.29123735\n",
      "Trained batch 943 batch loss 1.09793866 epoch total loss 1.29103231\n",
      "Trained batch 944 batch loss 1.13955855 epoch total loss 1.29087186\n",
      "Trained batch 945 batch loss 1.17727542 epoch total loss 1.29075158\n",
      "Trained batch 946 batch loss 1.04217756 epoch total loss 1.29048884\n",
      "Trained batch 947 batch loss 1.06376147 epoch total loss 1.29024947\n",
      "Trained batch 948 batch loss 0.93165946 epoch total loss 1.2898711\n",
      "Trained batch 949 batch loss 1.19033 epoch total loss 1.28976619\n",
      "Trained batch 950 batch loss 1.33488297 epoch total loss 1.28981364\n",
      "Trained batch 951 batch loss 1.16081119 epoch total loss 1.28967798\n",
      "Trained batch 952 batch loss 1.32972908 epoch total loss 1.28972\n",
      "Trained batch 953 batch loss 1.08179426 epoch total loss 1.28950191\n",
      "Trained batch 954 batch loss 1.31003523 epoch total loss 1.28952336\n",
      "Trained batch 955 batch loss 1.33438 epoch total loss 1.28957033\n",
      "Trained batch 956 batch loss 1.16468179 epoch total loss 1.28943968\n",
      "Trained batch 957 batch loss 1.27331376 epoch total loss 1.28942287\n",
      "Trained batch 958 batch loss 1.35732138 epoch total loss 1.28949368\n",
      "Trained batch 959 batch loss 1.35328794 epoch total loss 1.2895602\n",
      "Trained batch 960 batch loss 1.35016394 epoch total loss 1.28962338\n",
      "Trained batch 961 batch loss 1.13236547 epoch total loss 1.28945971\n",
      "Trained batch 962 batch loss 1.3319726 epoch total loss 1.28950393\n",
      "Trained batch 963 batch loss 1.16931832 epoch total loss 1.28937912\n",
      "Trained batch 964 batch loss 1.15095592 epoch total loss 1.28923559\n",
      "Trained batch 965 batch loss 1.12917495 epoch total loss 1.28906965\n",
      "Trained batch 966 batch loss 1.20448363 epoch total loss 1.28898215\n",
      "Trained batch 967 batch loss 1.17396927 epoch total loss 1.28886318\n",
      "Trained batch 968 batch loss 1.18093908 epoch total loss 1.2887516\n",
      "Trained batch 969 batch loss 1.23408031 epoch total loss 1.28869534\n",
      "Trained batch 970 batch loss 1.23083687 epoch total loss 1.28863561\n",
      "Trained batch 971 batch loss 1.20775819 epoch total loss 1.2885524\n",
      "Trained batch 972 batch loss 1.30193937 epoch total loss 1.28856611\n",
      "Trained batch 973 batch loss 1.16628885 epoch total loss 1.28844035\n",
      "Trained batch 974 batch loss 1.35574234 epoch total loss 1.28850937\n",
      "Trained batch 975 batch loss 1.24008274 epoch total loss 1.28845978\n",
      "Trained batch 976 batch loss 1.35820282 epoch total loss 1.28853118\n",
      "Trained batch 977 batch loss 1.45537007 epoch total loss 1.28870189\n",
      "Trained batch 978 batch loss 1.28491724 epoch total loss 1.28869808\n",
      "Trained batch 979 batch loss 1.2502048 epoch total loss 1.28865874\n",
      "Trained batch 980 batch loss 1.20876122 epoch total loss 1.2885772\n",
      "Trained batch 981 batch loss 1.14825559 epoch total loss 1.28843427\n",
      "Trained batch 982 batch loss 1.22325397 epoch total loss 1.28836787\n",
      "Trained batch 983 batch loss 1.27103233 epoch total loss 1.28835022\n",
      "Trained batch 984 batch loss 1.2617346 epoch total loss 1.28832316\n",
      "Trained batch 985 batch loss 1.25099671 epoch total loss 1.28828526\n",
      "Trained batch 986 batch loss 1.23964167 epoch total loss 1.2882359\n",
      "Trained batch 987 batch loss 1.24285007 epoch total loss 1.28818977\n",
      "Trained batch 988 batch loss 1.1936388 epoch total loss 1.28809404\n",
      "Trained batch 989 batch loss 1.16588497 epoch total loss 1.28797054\n",
      "Trained batch 990 batch loss 1.2845161 epoch total loss 1.28796709\n",
      "Trained batch 991 batch loss 1.15531147 epoch total loss 1.28783321\n",
      "Trained batch 992 batch loss 1.29964638 epoch total loss 1.28784513\n",
      "Trained batch 993 batch loss 1.27202642 epoch total loss 1.28782916\n",
      "Trained batch 994 batch loss 1.23835564 epoch total loss 1.28777945\n",
      "Trained batch 995 batch loss 1.24839616 epoch total loss 1.28773987\n",
      "Trained batch 996 batch loss 1.22546935 epoch total loss 1.28767729\n",
      "Trained batch 997 batch loss 1.14723778 epoch total loss 1.28753638\n",
      "Trained batch 998 batch loss 1.22785091 epoch total loss 1.28747666\n",
      "Trained batch 999 batch loss 1.24104905 epoch total loss 1.28743029\n",
      "Trained batch 1000 batch loss 1.37196505 epoch total loss 1.28751481\n",
      "Trained batch 1001 batch loss 1.23416686 epoch total loss 1.2874614\n",
      "Trained batch 1002 batch loss 1.24395776 epoch total loss 1.28741813\n",
      "Trained batch 1003 batch loss 1.25882864 epoch total loss 1.28738952\n",
      "Trained batch 1004 batch loss 1.20009017 epoch total loss 1.28730261\n",
      "Trained batch 1005 batch loss 1.15434361 epoch total loss 1.28717017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1006 batch loss 1.30884981 epoch total loss 1.28719175\n",
      "Trained batch 1007 batch loss 1.43187499 epoch total loss 1.2873354\n",
      "Trained batch 1008 batch loss 1.35443163 epoch total loss 1.28740203\n",
      "Trained batch 1009 batch loss 1.4155395 epoch total loss 1.28752911\n",
      "Trained batch 1010 batch loss 1.40676546 epoch total loss 1.28764713\n",
      "Trained batch 1011 batch loss 1.22621608 epoch total loss 1.28758633\n",
      "Trained batch 1012 batch loss 1.32073402 epoch total loss 1.28761899\n",
      "Trained batch 1013 batch loss 1.27943707 epoch total loss 1.28761089\n",
      "Trained batch 1014 batch loss 1.38600707 epoch total loss 1.28770792\n",
      "Trained batch 1015 batch loss 1.30299 epoch total loss 1.28772295\n",
      "Trained batch 1016 batch loss 1.21904838 epoch total loss 1.28765535\n",
      "Trained batch 1017 batch loss 1.23831987 epoch total loss 1.28760684\n",
      "Trained batch 1018 batch loss 1.21656668 epoch total loss 1.28753698\n",
      "Trained batch 1019 batch loss 1.07206559 epoch total loss 1.2873255\n",
      "Trained batch 1020 batch loss 1.18045509 epoch total loss 1.28722072\n",
      "Trained batch 1021 batch loss 1.25204611 epoch total loss 1.28718626\n",
      "Trained batch 1022 batch loss 1.329741 epoch total loss 1.28722787\n",
      "Trained batch 1023 batch loss 1.15047061 epoch total loss 1.28709424\n",
      "Trained batch 1024 batch loss 1.20599818 epoch total loss 1.28701508\n",
      "Trained batch 1025 batch loss 1.14857209 epoch total loss 1.28688\n",
      "Trained batch 1026 batch loss 1.20591843 epoch total loss 1.2868011\n",
      "Trained batch 1027 batch loss 1.20958912 epoch total loss 1.28672588\n",
      "Trained batch 1028 batch loss 1.35717058 epoch total loss 1.28679442\n",
      "Trained batch 1029 batch loss 1.190449 epoch total loss 1.28670084\n",
      "Trained batch 1030 batch loss 1.34923017 epoch total loss 1.28676152\n",
      "Trained batch 1031 batch loss 1.21925831 epoch total loss 1.28669608\n",
      "Trained batch 1032 batch loss 1.28165734 epoch total loss 1.28669107\n",
      "Trained batch 1033 batch loss 1.25045729 epoch total loss 1.28665602\n",
      "Trained batch 1034 batch loss 1.26784706 epoch total loss 1.2866379\n",
      "Trained batch 1035 batch loss 1.22914052 epoch total loss 1.28658223\n",
      "Trained batch 1036 batch loss 1.37798536 epoch total loss 1.28667045\n",
      "Trained batch 1037 batch loss 1.38885689 epoch total loss 1.28676903\n",
      "Trained batch 1038 batch loss 1.25375676 epoch total loss 1.28673732\n",
      "Trained batch 1039 batch loss 1.22596169 epoch total loss 1.28667879\n",
      "Trained batch 1040 batch loss 1.21297169 epoch total loss 1.28660798\n",
      "Trained batch 1041 batch loss 1.22983754 epoch total loss 1.28655338\n",
      "Trained batch 1042 batch loss 1.29578364 epoch total loss 1.28656232\n",
      "Trained batch 1043 batch loss 1.35450506 epoch total loss 1.28662741\n",
      "Trained batch 1044 batch loss 1.18481863 epoch total loss 1.2865299\n",
      "Trained batch 1045 batch loss 1.23632765 epoch total loss 1.28648186\n",
      "Trained batch 1046 batch loss 1.25785172 epoch total loss 1.28645444\n",
      "Trained batch 1047 batch loss 1.22373235 epoch total loss 1.2863946\n",
      "Trained batch 1048 batch loss 1.25911307 epoch total loss 1.28636861\n",
      "Trained batch 1049 batch loss 1.39329088 epoch total loss 1.28647053\n",
      "Trained batch 1050 batch loss 1.31563723 epoch total loss 1.28649831\n",
      "Trained batch 1051 batch loss 1.09025431 epoch total loss 1.28631151\n",
      "Trained batch 1052 batch loss 1.12666941 epoch total loss 1.28615987\n",
      "Trained batch 1053 batch loss 1.05408895 epoch total loss 1.28593946\n",
      "Trained batch 1054 batch loss 1.16558909 epoch total loss 1.28582537\n",
      "Trained batch 1055 batch loss 1.08271432 epoch total loss 1.28563285\n",
      "Trained batch 1056 batch loss 1.04718173 epoch total loss 1.28540707\n",
      "Trained batch 1057 batch loss 1.13990462 epoch total loss 1.28526938\n",
      "Trained batch 1058 batch loss 1.22580802 epoch total loss 1.28521323\n",
      "Trained batch 1059 batch loss 1.19809866 epoch total loss 1.28513098\n",
      "Trained batch 1060 batch loss 1.32340765 epoch total loss 1.2851671\n",
      "Trained batch 1061 batch loss 1.33145642 epoch total loss 1.28521073\n",
      "Trained batch 1062 batch loss 1.25491166 epoch total loss 1.28518212\n",
      "Trained batch 1063 batch loss 1.18055475 epoch total loss 1.28508365\n",
      "Trained batch 1064 batch loss 1.22823119 epoch total loss 1.28503025\n",
      "Trained batch 1065 batch loss 1.18077791 epoch total loss 1.28493237\n",
      "Trained batch 1066 batch loss 1.07180941 epoch total loss 1.28473246\n",
      "Trained batch 1067 batch loss 1.2488544 epoch total loss 1.28469884\n",
      "Trained batch 1068 batch loss 1.35535097 epoch total loss 1.28476501\n",
      "Trained batch 1069 batch loss 1.31555557 epoch total loss 1.28479385\n",
      "Trained batch 1070 batch loss 1.24711573 epoch total loss 1.28475857\n",
      "Trained batch 1071 batch loss 1.36941671 epoch total loss 1.2848376\n",
      "Trained batch 1072 batch loss 1.40300643 epoch total loss 1.28494775\n",
      "Trained batch 1073 batch loss 1.22094345 epoch total loss 1.28488815\n",
      "Trained batch 1074 batch loss 1.31504 epoch total loss 1.28491616\n",
      "Trained batch 1075 batch loss 1.14930344 epoch total loss 1.28479\n",
      "Trained batch 1076 batch loss 1.267905 epoch total loss 1.28477442\n",
      "Trained batch 1077 batch loss 1.26445937 epoch total loss 1.28475547\n",
      "Trained batch 1078 batch loss 1.28599703 epoch total loss 1.28475666\n",
      "Trained batch 1079 batch loss 1.31706047 epoch total loss 1.28478658\n",
      "Trained batch 1080 batch loss 1.25939596 epoch total loss 1.28476298\n",
      "Trained batch 1081 batch loss 1.33846235 epoch total loss 1.28481269\n",
      "Trained batch 1082 batch loss 1.21093476 epoch total loss 1.2847445\n",
      "Trained batch 1083 batch loss 1.25053215 epoch total loss 1.28471279\n",
      "Trained batch 1084 batch loss 1.37039459 epoch total loss 1.28479183\n",
      "Trained batch 1085 batch loss 1.27867115 epoch total loss 1.28478622\n",
      "Trained batch 1086 batch loss 1.15334868 epoch total loss 1.28466511\n",
      "Trained batch 1087 batch loss 1.37344265 epoch total loss 1.28474677\n",
      "Trained batch 1088 batch loss 1.33546746 epoch total loss 1.28479338\n",
      "Trained batch 1089 batch loss 1.39275682 epoch total loss 1.28489244\n",
      "Trained batch 1090 batch loss 1.44502795 epoch total loss 1.28503942\n",
      "Trained batch 1091 batch loss 1.3472538 epoch total loss 1.28509653\n",
      "Trained batch 1092 batch loss 1.1683898 epoch total loss 1.2849896\n",
      "Trained batch 1093 batch loss 1.28407717 epoch total loss 1.28498876\n",
      "Trained batch 1094 batch loss 1.24365854 epoch total loss 1.28495097\n",
      "Trained batch 1095 batch loss 1.2871629 epoch total loss 1.28495288\n",
      "Trained batch 1096 batch loss 1.32652378 epoch total loss 1.28499091\n",
      "Trained batch 1097 batch loss 1.28065658 epoch total loss 1.28498685\n",
      "Trained batch 1098 batch loss 1.41352594 epoch total loss 1.28510404\n",
      "Trained batch 1099 batch loss 1.43536472 epoch total loss 1.28524077\n",
      "Trained batch 1100 batch loss 1.47469425 epoch total loss 1.28541303\n",
      "Trained batch 1101 batch loss 1.47464371 epoch total loss 1.28558493\n",
      "Trained batch 1102 batch loss 1.41579604 epoch total loss 1.28570306\n",
      "Trained batch 1103 batch loss 1.38540971 epoch total loss 1.28579342\n",
      "Trained batch 1104 batch loss 1.35438752 epoch total loss 1.28585553\n",
      "Trained batch 1105 batch loss 1.38506377 epoch total loss 1.28594518\n",
      "Trained batch 1106 batch loss 1.35467923 epoch total loss 1.2860074\n",
      "Trained batch 1107 batch loss 1.32683122 epoch total loss 1.28604424\n",
      "Trained batch 1108 batch loss 1.41903329 epoch total loss 1.28616428\n",
      "Trained batch 1109 batch loss 1.21861911 epoch total loss 1.28610337\n",
      "Trained batch 1110 batch loss 1.18751979 epoch total loss 1.28601456\n",
      "Trained batch 1111 batch loss 1.23669744 epoch total loss 1.28597021\n",
      "Trained batch 1112 batch loss 1.15095806 epoch total loss 1.28584886\n",
      "Trained batch 1113 batch loss 1.20884299 epoch total loss 1.2857796\n",
      "Trained batch 1114 batch loss 1.08003855 epoch total loss 1.28559506\n",
      "Trained batch 1115 batch loss 1.29255724 epoch total loss 1.28560126\n",
      "Trained batch 1116 batch loss 1.23688841 epoch total loss 1.28555763\n",
      "Trained batch 1117 batch loss 1.15770137 epoch total loss 1.28544319\n",
      "Trained batch 1118 batch loss 1.11705649 epoch total loss 1.28529263\n",
      "Trained batch 1119 batch loss 1.28809619 epoch total loss 1.28529513\n",
      "Trained batch 1120 batch loss 1.316679 epoch total loss 1.28532314\n",
      "Trained batch 1121 batch loss 1.31734073 epoch total loss 1.28535175\n",
      "Trained batch 1122 batch loss 1.40883708 epoch total loss 1.28546178\n",
      "Trained batch 1123 batch loss 1.31399262 epoch total loss 1.28548717\n",
      "Trained batch 1124 batch loss 1.27929986 epoch total loss 1.28548157\n",
      "Trained batch 1125 batch loss 1.28569448 epoch total loss 1.28548181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1126 batch loss 1.27008128 epoch total loss 1.2854681\n",
      "Trained batch 1127 batch loss 1.17522192 epoch total loss 1.28537023\n",
      "Trained batch 1128 batch loss 1.18986845 epoch total loss 1.28528559\n",
      "Trained batch 1129 batch loss 1.31278682 epoch total loss 1.28530991\n",
      "Trained batch 1130 batch loss 1.17527616 epoch total loss 1.28521252\n",
      "Trained batch 1131 batch loss 1.18092775 epoch total loss 1.28512037\n",
      "Trained batch 1132 batch loss 1.28377676 epoch total loss 1.28511918\n",
      "Trained batch 1133 batch loss 1.29877841 epoch total loss 1.28513122\n",
      "Trained batch 1134 batch loss 1.34502482 epoch total loss 1.28518403\n",
      "Trained batch 1135 batch loss 1.27033174 epoch total loss 1.28517103\n",
      "Trained batch 1136 batch loss 1.28208017 epoch total loss 1.28516829\n",
      "Trained batch 1137 batch loss 1.34906769 epoch total loss 1.28522456\n",
      "Trained batch 1138 batch loss 1.35732043 epoch total loss 1.28528786\n",
      "Trained batch 1139 batch loss 1.31624544 epoch total loss 1.28531504\n",
      "Trained batch 1140 batch loss 1.23339343 epoch total loss 1.2852695\n",
      "Trained batch 1141 batch loss 1.17619491 epoch total loss 1.28517389\n",
      "Trained batch 1142 batch loss 1.29827285 epoch total loss 1.28518534\n",
      "Trained batch 1143 batch loss 1.16830695 epoch total loss 1.28508306\n",
      "Trained batch 1144 batch loss 1.12050009 epoch total loss 1.28493917\n",
      "Trained batch 1145 batch loss 1.26504564 epoch total loss 1.28492177\n",
      "Trained batch 1146 batch loss 1.22231078 epoch total loss 1.28486717\n",
      "Trained batch 1147 batch loss 1.12824118 epoch total loss 1.28473067\n",
      "Trained batch 1148 batch loss 1.16496921 epoch total loss 1.28462625\n",
      "Trained batch 1149 batch loss 1.24707031 epoch total loss 1.28459358\n",
      "Trained batch 1150 batch loss 1.32773399 epoch total loss 1.28463113\n",
      "Trained batch 1151 batch loss 1.27346706 epoch total loss 1.28462136\n",
      "Trained batch 1152 batch loss 1.07392669 epoch total loss 1.28443861\n",
      "Trained batch 1153 batch loss 1.09451306 epoch total loss 1.28427386\n",
      "Trained batch 1154 batch loss 1.03568685 epoch total loss 1.28405833\n",
      "Trained batch 1155 batch loss 1.22396588 epoch total loss 1.28400636\n",
      "Trained batch 1156 batch loss 1.32357526 epoch total loss 1.28404057\n",
      "Trained batch 1157 batch loss 1.46080661 epoch total loss 1.2841934\n",
      "Trained batch 1158 batch loss 1.41877198 epoch total loss 1.28430963\n",
      "Trained batch 1159 batch loss 1.188133 epoch total loss 1.28422666\n",
      "Trained batch 1160 batch loss 1.17998981 epoch total loss 1.28413677\n",
      "Trained batch 1161 batch loss 1.19713 epoch total loss 1.28406179\n",
      "Trained batch 1162 batch loss 1.26873386 epoch total loss 1.28404856\n",
      "Trained batch 1163 batch loss 1.3710928 epoch total loss 1.28412342\n",
      "Trained batch 1164 batch loss 1.3689971 epoch total loss 1.28419638\n",
      "Trained batch 1165 batch loss 1.28254271 epoch total loss 1.28419495\n",
      "Trained batch 1166 batch loss 1.4157536 epoch total loss 1.28430784\n",
      "Trained batch 1167 batch loss 1.39736772 epoch total loss 1.28440464\n",
      "Trained batch 1168 batch loss 1.33363199 epoch total loss 1.28444684\n",
      "Trained batch 1169 batch loss 1.25745296 epoch total loss 1.28442371\n",
      "Trained batch 1170 batch loss 1.15651584 epoch total loss 1.28431439\n",
      "Trained batch 1171 batch loss 1.19112456 epoch total loss 1.28423488\n",
      "Trained batch 1172 batch loss 1.17068 epoch total loss 1.28413796\n",
      "Trained batch 1173 batch loss 1.16000283 epoch total loss 1.28403211\n",
      "Trained batch 1174 batch loss 1.24765825 epoch total loss 1.28400111\n",
      "Trained batch 1175 batch loss 1.29016 epoch total loss 1.28400636\n",
      "Trained batch 1176 batch loss 1.34143233 epoch total loss 1.28405523\n",
      "Trained batch 1177 batch loss 1.39657176 epoch total loss 1.28415084\n",
      "Trained batch 1178 batch loss 1.20490181 epoch total loss 1.2840836\n",
      "Trained batch 1179 batch loss 1.18167973 epoch total loss 1.2839967\n",
      "Trained batch 1180 batch loss 1.2353543 epoch total loss 1.28395545\n",
      "Trained batch 1181 batch loss 1.25383449 epoch total loss 1.28393\n",
      "Trained batch 1182 batch loss 1.12893391 epoch total loss 1.28379881\n",
      "Trained batch 1183 batch loss 1.11935568 epoch total loss 1.28365982\n",
      "Trained batch 1184 batch loss 1.20672154 epoch total loss 1.28359485\n",
      "Trained batch 1185 batch loss 1.23426318 epoch total loss 1.28355312\n",
      "Trained batch 1186 batch loss 1.30824018 epoch total loss 1.28357399\n",
      "Trained batch 1187 batch loss 1.31241131 epoch total loss 1.28359818\n",
      "Trained batch 1188 batch loss 1.28367734 epoch total loss 1.2835983\n",
      "Trained batch 1189 batch loss 1.1056143 epoch total loss 1.28344858\n",
      "Trained batch 1190 batch loss 1.04429984 epoch total loss 1.28324759\n",
      "Trained batch 1191 batch loss 1.02356827 epoch total loss 1.28302956\n",
      "Trained batch 1192 batch loss 1.10745692 epoch total loss 1.28288233\n",
      "Trained batch 1193 batch loss 1.21024609 epoch total loss 1.28282142\n",
      "Trained batch 1194 batch loss 1.24587154 epoch total loss 1.28279042\n",
      "Trained batch 1195 batch loss 1.30070043 epoch total loss 1.28280532\n",
      "Trained batch 1196 batch loss 1.30869818 epoch total loss 1.28282702\n",
      "Trained batch 1197 batch loss 1.29662228 epoch total loss 1.28283858\n",
      "Trained batch 1198 batch loss 1.35823607 epoch total loss 1.28290153\n",
      "Trained batch 1199 batch loss 1.27558625 epoch total loss 1.28289545\n",
      "Trained batch 1200 batch loss 1.26370716 epoch total loss 1.28287947\n",
      "Trained batch 1201 batch loss 1.13147664 epoch total loss 1.28275335\n",
      "Trained batch 1202 batch loss 1.30068207 epoch total loss 1.28276825\n",
      "Trained batch 1203 batch loss 1.17375875 epoch total loss 1.28267765\n",
      "Trained batch 1204 batch loss 1.13064575 epoch total loss 1.28255129\n",
      "Trained batch 1205 batch loss 1.21758652 epoch total loss 1.28249729\n",
      "Trained batch 1206 batch loss 1.43263412 epoch total loss 1.28262186\n",
      "Trained batch 1207 batch loss 1.53654945 epoch total loss 1.28283215\n",
      "Trained batch 1208 batch loss 1.40196884 epoch total loss 1.28293073\n",
      "Trained batch 1209 batch loss 1.40564907 epoch total loss 1.2830323\n",
      "Trained batch 1210 batch loss 1.42705 epoch total loss 1.28315127\n",
      "Trained batch 1211 batch loss 1.11529171 epoch total loss 1.28301263\n",
      "Trained batch 1212 batch loss 1.02055264 epoch total loss 1.28279603\n",
      "Trained batch 1213 batch loss 1.06637442 epoch total loss 1.28261769\n",
      "Trained batch 1214 batch loss 0.998795629 epoch total loss 1.2823838\n",
      "Trained batch 1215 batch loss 1.10269237 epoch total loss 1.28223586\n",
      "Trained batch 1216 batch loss 1.13737464 epoch total loss 1.28211677\n",
      "Trained batch 1217 batch loss 1.11506629 epoch total loss 1.28197956\n",
      "Trained batch 1218 batch loss 1.18167663 epoch total loss 1.28189719\n",
      "Trained batch 1219 batch loss 1.26067603 epoch total loss 1.28187966\n",
      "Trained batch 1220 batch loss 1.30992162 epoch total loss 1.28190267\n",
      "Trained batch 1221 batch loss 1.32938361 epoch total loss 1.28194153\n",
      "Trained batch 1222 batch loss 1.31896043 epoch total loss 1.28197181\n",
      "Trained batch 1223 batch loss 1.21060598 epoch total loss 1.2819134\n",
      "Trained batch 1224 batch loss 1.32223237 epoch total loss 1.28194642\n",
      "Trained batch 1225 batch loss 1.37219739 epoch total loss 1.28202009\n",
      "Trained batch 1226 batch loss 1.23872471 epoch total loss 1.28198481\n",
      "Trained batch 1227 batch loss 1.25786483 epoch total loss 1.28196514\n",
      "Trained batch 1228 batch loss 1.3586669 epoch total loss 1.2820276\n",
      "Trained batch 1229 batch loss 1.34021699 epoch total loss 1.28207493\n",
      "Trained batch 1230 batch loss 1.31919432 epoch total loss 1.28210509\n",
      "Trained batch 1231 batch loss 1.23317921 epoch total loss 1.28206527\n",
      "Trained batch 1232 batch loss 1.39033914 epoch total loss 1.28215325\n",
      "Trained batch 1233 batch loss 1.40829229 epoch total loss 1.28225553\n",
      "Trained batch 1234 batch loss 1.41394198 epoch total loss 1.28236234\n",
      "Trained batch 1235 batch loss 1.2320652 epoch total loss 1.28232157\n",
      "Trained batch 1236 batch loss 1.39280379 epoch total loss 1.28241098\n",
      "Trained batch 1237 batch loss 1.46855021 epoch total loss 1.28256142\n",
      "Trained batch 1238 batch loss 1.49824691 epoch total loss 1.28273571\n",
      "Trained batch 1239 batch loss 1.32441568 epoch total loss 1.28276932\n",
      "Trained batch 1240 batch loss 1.22287917 epoch total loss 1.28272104\n",
      "Trained batch 1241 batch loss 1.15516376 epoch total loss 1.28261828\n",
      "Trained batch 1242 batch loss 1.24083281 epoch total loss 1.28258467\n",
      "Trained batch 1243 batch loss 1.26334167 epoch total loss 1.28256905\n",
      "Trained batch 1244 batch loss 1.27527153 epoch total loss 1.28256321\n",
      "Trained batch 1245 batch loss 1.26698637 epoch total loss 1.28255069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1246 batch loss 1.31450725 epoch total loss 1.28257632\n",
      "Trained batch 1247 batch loss 1.39971924 epoch total loss 1.28267026\n",
      "Trained batch 1248 batch loss 1.41153097 epoch total loss 1.28277338\n",
      "Trained batch 1249 batch loss 1.28116989 epoch total loss 1.28277206\n",
      "Trained batch 1250 batch loss 1.31248152 epoch total loss 1.28279591\n",
      "Trained batch 1251 batch loss 1.19383383 epoch total loss 1.28272474\n",
      "Trained batch 1252 batch loss 1.30278111 epoch total loss 1.28274083\n",
      "Trained batch 1253 batch loss 1.33843887 epoch total loss 1.28278518\n",
      "Trained batch 1254 batch loss 1.20424473 epoch total loss 1.28272259\n",
      "Trained batch 1255 batch loss 1.19825745 epoch total loss 1.28265524\n",
      "Trained batch 1256 batch loss 1.18002439 epoch total loss 1.28257358\n",
      "Trained batch 1257 batch loss 1.15855241 epoch total loss 1.28247488\n",
      "Trained batch 1258 batch loss 1.24594581 epoch total loss 1.28244591\n",
      "Trained batch 1259 batch loss 1.28999317 epoch total loss 1.28245187\n",
      "Trained batch 1260 batch loss 1.25998449 epoch total loss 1.28243411\n",
      "Trained batch 1261 batch loss 1.17920017 epoch total loss 1.28235221\n",
      "Trained batch 1262 batch loss 1.16846478 epoch total loss 1.28226197\n",
      "Trained batch 1263 batch loss 1.1542871 epoch total loss 1.28216064\n",
      "Trained batch 1264 batch loss 1.11682725 epoch total loss 1.28202987\n",
      "Trained batch 1265 batch loss 1.27088964 epoch total loss 1.28202105\n",
      "Trained batch 1266 batch loss 1.30976427 epoch total loss 1.28204298\n",
      "Trained batch 1267 batch loss 1.22900474 epoch total loss 1.28200114\n",
      "Trained batch 1268 batch loss 1.18252957 epoch total loss 1.28192258\n",
      "Trained batch 1269 batch loss 1.25719428 epoch total loss 1.28190315\n",
      "Trained batch 1270 batch loss 1.28595686 epoch total loss 1.28190637\n",
      "Trained batch 1271 batch loss 1.3660481 epoch total loss 1.28197265\n",
      "Trained batch 1272 batch loss 1.27457583 epoch total loss 1.28196681\n",
      "Trained batch 1273 batch loss 1.25520468 epoch total loss 1.28194582\n",
      "Trained batch 1274 batch loss 1.26362503 epoch total loss 1.2819314\n",
      "Trained batch 1275 batch loss 1.11031806 epoch total loss 1.28179693\n",
      "Trained batch 1276 batch loss 1.19573867 epoch total loss 1.28172934\n",
      "Trained batch 1277 batch loss 1.21841431 epoch total loss 1.28167975\n",
      "Trained batch 1278 batch loss 1.29428828 epoch total loss 1.28168964\n",
      "Trained batch 1279 batch loss 1.16122043 epoch total loss 1.28159547\n",
      "Trained batch 1280 batch loss 1.07909703 epoch total loss 1.28143728\n",
      "Trained batch 1281 batch loss 1.09095609 epoch total loss 1.28128862\n",
      "Trained batch 1282 batch loss 1.23052311 epoch total loss 1.28124893\n",
      "Trained batch 1283 batch loss 1.29525292 epoch total loss 1.28125989\n",
      "Trained batch 1284 batch loss 1.32207739 epoch total loss 1.2812916\n",
      "Trained batch 1285 batch loss 1.35859191 epoch total loss 1.2813518\n",
      "Trained batch 1286 batch loss 1.397174 epoch total loss 1.28144193\n",
      "Trained batch 1287 batch loss 1.31391895 epoch total loss 1.2814672\n",
      "Trained batch 1288 batch loss 1.28459775 epoch total loss 1.28146958\n",
      "Trained batch 1289 batch loss 1.20486 epoch total loss 1.2814101\n",
      "Trained batch 1290 batch loss 1.46331799 epoch total loss 1.28155124\n",
      "Trained batch 1291 batch loss 1.33314562 epoch total loss 1.28159118\n",
      "Trained batch 1292 batch loss 1.15877175 epoch total loss 1.28149617\n",
      "Trained batch 1293 batch loss 1.21594894 epoch total loss 1.28144538\n",
      "Trained batch 1294 batch loss 1.10827231 epoch total loss 1.28131163\n",
      "Trained batch 1295 batch loss 1.12667871 epoch total loss 1.28119218\n",
      "Trained batch 1296 batch loss 1.16147602 epoch total loss 1.28109992\n",
      "Trained batch 1297 batch loss 1.33794498 epoch total loss 1.28114367\n",
      "Trained batch 1298 batch loss 1.23283362 epoch total loss 1.28110635\n",
      "Trained batch 1299 batch loss 1.37647355 epoch total loss 1.28117979\n",
      "Trained batch 1300 batch loss 1.29818094 epoch total loss 1.2811929\n",
      "Trained batch 1301 batch loss 1.1994561 epoch total loss 1.28113008\n",
      "Trained batch 1302 batch loss 1.01000094 epoch total loss 1.28092182\n",
      "Trained batch 1303 batch loss 0.984166741 epoch total loss 1.28069413\n",
      "Trained batch 1304 batch loss 1.20886648 epoch total loss 1.28063905\n",
      "Trained batch 1305 batch loss 1.21749485 epoch total loss 1.28059065\n",
      "Trained batch 1306 batch loss 1.51127434 epoch total loss 1.2807672\n",
      "Trained batch 1307 batch loss 1.51837981 epoch total loss 1.28094912\n",
      "Trained batch 1308 batch loss 1.31856096 epoch total loss 1.28097785\n",
      "Trained batch 1309 batch loss 1.22515404 epoch total loss 1.28093517\n",
      "Trained batch 1310 batch loss 1.24463606 epoch total loss 1.28090751\n",
      "Trained batch 1311 batch loss 1.24158311 epoch total loss 1.28087747\n",
      "Trained batch 1312 batch loss 1.30122733 epoch total loss 1.28089297\n",
      "Trained batch 1313 batch loss 1.26191735 epoch total loss 1.28087854\n",
      "Trained batch 1314 batch loss 1.29976368 epoch total loss 1.28089297\n",
      "Trained batch 1315 batch loss 1.2556603 epoch total loss 1.28087378\n",
      "Trained batch 1316 batch loss 1.29338145 epoch total loss 1.28088319\n",
      "Trained batch 1317 batch loss 1.2605449 epoch total loss 1.2808677\n",
      "Trained batch 1318 batch loss 1.20010257 epoch total loss 1.28080642\n",
      "Trained batch 1319 batch loss 1.18456626 epoch total loss 1.28073347\n",
      "Trained batch 1320 batch loss 1.16910207 epoch total loss 1.28064895\n",
      "Trained batch 1321 batch loss 1.22552156 epoch total loss 1.2806071\n",
      "Trained batch 1322 batch loss 1.24366784 epoch total loss 1.28057921\n",
      "Trained batch 1323 batch loss 1.17843699 epoch total loss 1.28050196\n",
      "Trained batch 1324 batch loss 1.20624065 epoch total loss 1.28044593\n",
      "Trained batch 1325 batch loss 1.0753243 epoch total loss 1.28029108\n",
      "Trained batch 1326 batch loss 1.21032786 epoch total loss 1.28023839\n",
      "Trained batch 1327 batch loss 1.0692302 epoch total loss 1.28007936\n",
      "Trained batch 1328 batch loss 1.01389372 epoch total loss 1.27987897\n",
      "Trained batch 1329 batch loss 1.37473536 epoch total loss 1.27995026\n",
      "Trained batch 1330 batch loss 1.49229729 epoch total loss 1.28011\n",
      "Trained batch 1331 batch loss 1.40713716 epoch total loss 1.28020537\n",
      "Trained batch 1332 batch loss 1.14365232 epoch total loss 1.28010285\n",
      "Trained batch 1333 batch loss 1.21873415 epoch total loss 1.28005683\n",
      "Trained batch 1334 batch loss 1.29170179 epoch total loss 1.28006566\n",
      "Trained batch 1335 batch loss 1.18786991 epoch total loss 1.27999651\n",
      "Trained batch 1336 batch loss 1.17851734 epoch total loss 1.27992058\n",
      "Trained batch 1337 batch loss 1.2533145 epoch total loss 1.27990067\n",
      "Trained batch 1338 batch loss 1.25579536 epoch total loss 1.27988255\n",
      "Trained batch 1339 batch loss 1.27079666 epoch total loss 1.27987576\n",
      "Trained batch 1340 batch loss 1.24318528 epoch total loss 1.27984834\n",
      "Trained batch 1341 batch loss 1.27237725 epoch total loss 1.27984273\n",
      "Trained batch 1342 batch loss 1.26610649 epoch total loss 1.2798326\n",
      "Trained batch 1343 batch loss 1.21604919 epoch total loss 1.27978504\n",
      "Trained batch 1344 batch loss 1.15002215 epoch total loss 1.27968848\n",
      "Trained batch 1345 batch loss 1.08181691 epoch total loss 1.27954137\n",
      "Trained batch 1346 batch loss 1.13369095 epoch total loss 1.27943301\n",
      "Trained batch 1347 batch loss 1.143893 epoch total loss 1.2793324\n",
      "Trained batch 1348 batch loss 1.22799575 epoch total loss 1.27929437\n",
      "Trained batch 1349 batch loss 1.25956905 epoch total loss 1.27927971\n",
      "Trained batch 1350 batch loss 1.34563541 epoch total loss 1.27932882\n",
      "Trained batch 1351 batch loss 1.28778744 epoch total loss 1.27933514\n",
      "Trained batch 1352 batch loss 1.35888696 epoch total loss 1.27939391\n",
      "Trained batch 1353 batch loss 1.34432447 epoch total loss 1.27944195\n",
      "Trained batch 1354 batch loss 1.30873966 epoch total loss 1.27946353\n",
      "Trained batch 1355 batch loss 1.36388397 epoch total loss 1.27952588\n",
      "Trained batch 1356 batch loss 1.24170363 epoch total loss 1.27949798\n",
      "Trained batch 1357 batch loss 1.21738625 epoch total loss 1.2794522\n",
      "Trained batch 1358 batch loss 1.31684828 epoch total loss 1.27947974\n",
      "Trained batch 1359 batch loss 1.21951389 epoch total loss 1.27943563\n",
      "Trained batch 1360 batch loss 1.21919668 epoch total loss 1.27939141\n",
      "Trained batch 1361 batch loss 1.18183386 epoch total loss 1.27931976\n",
      "Trained batch 1362 batch loss 1.22128892 epoch total loss 1.27927721\n",
      "Trained batch 1363 batch loss 1.19640338 epoch total loss 1.27921641\n",
      "Trained batch 1364 batch loss 1.13340545 epoch total loss 1.27910948\n",
      "Trained batch 1365 batch loss 1.20340502 epoch total loss 1.27905405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1366 batch loss 1.25852597 epoch total loss 1.27903903\n",
      "Trained batch 1367 batch loss 1.28446269 epoch total loss 1.27904296\n",
      "Trained batch 1368 batch loss 1.22324097 epoch total loss 1.27900219\n",
      "Trained batch 1369 batch loss 1.29398286 epoch total loss 1.27901304\n",
      "Trained batch 1370 batch loss 1.39198613 epoch total loss 1.27909553\n",
      "Trained batch 1371 batch loss 1.32227349 epoch total loss 1.279127\n",
      "Trained batch 1372 batch loss 1.18981528 epoch total loss 1.27906191\n",
      "Trained batch 1373 batch loss 1.22744322 epoch total loss 1.27902424\n",
      "Trained batch 1374 batch loss 1.10386908 epoch total loss 1.27889681\n",
      "Trained batch 1375 batch loss 1.21780109 epoch total loss 1.27885234\n",
      "Trained batch 1376 batch loss 1.23877621 epoch total loss 1.27882326\n",
      "Trained batch 1377 batch loss 1.31577706 epoch total loss 1.27885008\n",
      "Trained batch 1378 batch loss 1.20145655 epoch total loss 1.27879393\n",
      "Trained batch 1379 batch loss 1.14981937 epoch total loss 1.27870035\n",
      "Trained batch 1380 batch loss 1.19356418 epoch total loss 1.27863872\n",
      "Trained batch 1381 batch loss 1.26157379 epoch total loss 1.27862632\n",
      "Trained batch 1382 batch loss 1.48224878 epoch total loss 1.27877367\n",
      "Trained batch 1383 batch loss 1.22180665 epoch total loss 1.27873254\n",
      "Trained batch 1384 batch loss 1.27823424 epoch total loss 1.27873218\n",
      "Trained batch 1385 batch loss 1.10794723 epoch total loss 1.2786088\n",
      "Trained batch 1386 batch loss 0.982735515 epoch total loss 1.2783953\n",
      "Trained batch 1387 batch loss 0.977306724 epoch total loss 1.27817822\n",
      "Trained batch 1388 batch loss 0.959340334 epoch total loss 1.27794862\n",
      "Epoch 3 train loss 1.2779486179351807\n",
      "Validated batch 1 batch loss 1.26793194\n",
      "Validated batch 2 batch loss 1.17702174\n",
      "Validated batch 3 batch loss 1.30085015\n",
      "Validated batch 4 batch loss 1.16234326\n",
      "Validated batch 5 batch loss 1.2540015\n",
      "Validated batch 6 batch loss 1.27402782\n",
      "Validated batch 7 batch loss 1.26434267\n",
      "Validated batch 8 batch loss 1.39179063\n",
      "Validated batch 9 batch loss 1.37560499\n",
      "Validated batch 10 batch loss 1.275437\n",
      "Validated batch 11 batch loss 1.28500795\n",
      "Validated batch 12 batch loss 1.35843849\n",
      "Validated batch 13 batch loss 1.34267068\n",
      "Validated batch 14 batch loss 1.33615637\n",
      "Validated batch 15 batch loss 1.34106183\n",
      "Validated batch 16 batch loss 1.36671722\n",
      "Validated batch 17 batch loss 1.33071899\n",
      "Validated batch 18 batch loss 1.17959857\n",
      "Validated batch 19 batch loss 1.24587166\n",
      "Validated batch 20 batch loss 1.33224607\n",
      "Validated batch 21 batch loss 1.27906179\n",
      "Validated batch 22 batch loss 1.2791307\n",
      "Validated batch 23 batch loss 1.243536\n",
      "Validated batch 24 batch loss 1.15803301\n",
      "Validated batch 25 batch loss 1.28417695\n",
      "Validated batch 26 batch loss 1.31390047\n",
      "Validated batch 27 batch loss 1.21998918\n",
      "Validated batch 28 batch loss 1.36257076\n",
      "Validated batch 29 batch loss 1.41599751\n",
      "Validated batch 30 batch loss 1.15183437\n",
      "Validated batch 31 batch loss 1.2967931\n",
      "Validated batch 32 batch loss 1.27178824\n",
      "Validated batch 33 batch loss 1.34822404\n",
      "Validated batch 34 batch loss 1.29620862\n",
      "Validated batch 35 batch loss 1.14040232\n",
      "Validated batch 36 batch loss 1.15063739\n",
      "Validated batch 37 batch loss 1.24685502\n",
      "Validated batch 38 batch loss 1.23018479\n",
      "Validated batch 39 batch loss 1.23498547\n",
      "Validated batch 40 batch loss 1.22920752\n",
      "Validated batch 41 batch loss 1.16834927\n",
      "Validated batch 42 batch loss 1.32345438\n",
      "Validated batch 43 batch loss 1.31845713\n",
      "Validated batch 44 batch loss 1.32925045\n",
      "Validated batch 45 batch loss 1.2612983\n",
      "Validated batch 46 batch loss 1.17948055\n",
      "Validated batch 47 batch loss 1.20307577\n",
      "Validated batch 48 batch loss 1.24274468\n",
      "Validated batch 49 batch loss 1.22051358\n",
      "Validated batch 50 batch loss 1.15211606\n",
      "Validated batch 51 batch loss 1.19683611\n",
      "Validated batch 52 batch loss 1.31840086\n",
      "Validated batch 53 batch loss 1.18671238\n",
      "Validated batch 54 batch loss 1.05706835\n",
      "Validated batch 55 batch loss 1.16698587\n",
      "Validated batch 56 batch loss 1.20203221\n",
      "Validated batch 57 batch loss 1.18455267\n",
      "Validated batch 58 batch loss 1.23485875\n",
      "Validated batch 59 batch loss 1.24835575\n",
      "Validated batch 60 batch loss 1.20540309\n",
      "Validated batch 61 batch loss 1.28569579\n",
      "Validated batch 62 batch loss 1.32403326\n",
      "Validated batch 63 batch loss 1.20039022\n",
      "Validated batch 64 batch loss 1.33570695\n",
      "Validated batch 65 batch loss 1.01529491\n",
      "Validated batch 66 batch loss 1.2143414\n",
      "Validated batch 67 batch loss 1.17363799\n",
      "Validated batch 68 batch loss 1.23873591\n",
      "Validated batch 69 batch loss 1.43999529\n",
      "Validated batch 70 batch loss 1.24890685\n",
      "Validated batch 71 batch loss 1.31806183\n",
      "Validated batch 72 batch loss 1.14264143\n",
      "Validated batch 73 batch loss 1.21292734\n",
      "Validated batch 74 batch loss 1.27956605\n",
      "Validated batch 75 batch loss 1.22726464\n",
      "Validated batch 76 batch loss 1.26541662\n",
      "Validated batch 77 batch loss 1.23594749\n",
      "Validated batch 78 batch loss 1.22339618\n",
      "Validated batch 79 batch loss 1.35757279\n",
      "Validated batch 80 batch loss 1.14411342\n",
      "Validated batch 81 batch loss 1.11254811\n",
      "Validated batch 82 batch loss 1.31477308\n",
      "Validated batch 83 batch loss 1.36599481\n",
      "Validated batch 84 batch loss 1.37218058\n",
      "Validated batch 85 batch loss 1.41314435\n",
      "Validated batch 86 batch loss 1.18780255\n",
      "Validated batch 87 batch loss 1.42145848\n",
      "Validated batch 88 batch loss 1.19693792\n",
      "Validated batch 89 batch loss 1.33026135\n",
      "Validated batch 90 batch loss 1.28522038\n",
      "Validated batch 91 batch loss 1.03784764\n",
      "Validated batch 92 batch loss 1.27946401\n",
      "Validated batch 93 batch loss 1.2467742\n",
      "Validated batch 94 batch loss 1.30638933\n",
      "Validated batch 95 batch loss 1.20637119\n",
      "Validated batch 96 batch loss 1.18597507\n",
      "Validated batch 97 batch loss 1.262959\n",
      "Validated batch 98 batch loss 1.29620123\n",
      "Validated batch 99 batch loss 1.35268056\n",
      "Validated batch 100 batch loss 1.35482264\n",
      "Validated batch 101 batch loss 1.2999512\n",
      "Validated batch 102 batch loss 1.22724342\n",
      "Validated batch 103 batch loss 1.2944895\n",
      "Validated batch 104 batch loss 1.27179241\n",
      "Validated batch 105 batch loss 1.25829351\n",
      "Validated batch 106 batch loss 1.35526514\n",
      "Validated batch 107 batch loss 1.36735702\n",
      "Validated batch 108 batch loss 1.33678746\n",
      "Validated batch 109 batch loss 1.36207891\n",
      "Validated batch 110 batch loss 1.12692094\n",
      "Validated batch 111 batch loss 1.26555192\n",
      "Validated batch 112 batch loss 1.15332699\n",
      "Validated batch 113 batch loss 1.18874288\n",
      "Validated batch 114 batch loss 1.36458373\n",
      "Validated batch 115 batch loss 1.19638705\n",
      "Validated batch 116 batch loss 1.38050735\n",
      "Validated batch 117 batch loss 1.28490603\n",
      "Validated batch 118 batch loss 1.22777176\n",
      "Validated batch 119 batch loss 1.25713181\n",
      "Validated batch 120 batch loss 1.17296433\n",
      "Validated batch 121 batch loss 1.27132297\n",
      "Validated batch 122 batch loss 1.2864753\n",
      "Validated batch 123 batch loss 1.2521323\n",
      "Validated batch 124 batch loss 1.22876632\n",
      "Validated batch 125 batch loss 1.29236245\n",
      "Validated batch 126 batch loss 1.29613733\n",
      "Validated batch 127 batch loss 1.35345817\n",
      "Validated batch 128 batch loss 1.28824139\n",
      "Validated batch 129 batch loss 1.17728686\n",
      "Validated batch 130 batch loss 1.22262418\n",
      "Validated batch 131 batch loss 1.28907585\n",
      "Validated batch 132 batch loss 1.28578317\n",
      "Validated batch 133 batch loss 1.31731319\n",
      "Validated batch 134 batch loss 1.32634604\n",
      "Validated batch 135 batch loss 1.50073171\n",
      "Validated batch 136 batch loss 1.39657784\n",
      "Validated batch 137 batch loss 1.2885133\n",
      "Validated batch 138 batch loss 1.2046864\n",
      "Validated batch 139 batch loss 1.17530465\n",
      "Validated batch 140 batch loss 1.11723137\n",
      "Validated batch 141 batch loss 1.21064591\n",
      "Validated batch 142 batch loss 1.21549129\n",
      "Validated batch 143 batch loss 1.20414019\n",
      "Validated batch 144 batch loss 1.2392143\n",
      "Validated batch 145 batch loss 1.24590564\n",
      "Validated batch 146 batch loss 1.27467942\n",
      "Validated batch 147 batch loss 1.37678552\n",
      "Validated batch 148 batch loss 1.14400351\n",
      "Validated batch 149 batch loss 1.35108\n",
      "Validated batch 150 batch loss 1.27328563\n",
      "Validated batch 151 batch loss 1.14335823\n",
      "Validated batch 152 batch loss 1.27598345\n",
      "Validated batch 153 batch loss 1.25712466\n",
      "Validated batch 154 batch loss 1.21039987\n",
      "Validated batch 155 batch loss 1.39759839\n",
      "Validated batch 156 batch loss 1.30146432\n",
      "Validated batch 157 batch loss 1.31943214\n",
      "Validated batch 158 batch loss 1.20498061\n",
      "Validated batch 159 batch loss 1.29871082\n",
      "Validated batch 160 batch loss 1.26180291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 161 batch loss 1.20881844\n",
      "Validated batch 162 batch loss 1.25903189\n",
      "Validated batch 163 batch loss 1.27083445\n",
      "Validated batch 164 batch loss 1.28111982\n",
      "Validated batch 165 batch loss 1.22872078\n",
      "Validated batch 166 batch loss 1.29818332\n",
      "Validated batch 167 batch loss 1.44365108\n",
      "Validated batch 168 batch loss 1.19513988\n",
      "Validated batch 169 batch loss 1.29389071\n",
      "Validated batch 170 batch loss 1.18520737\n",
      "Validated batch 171 batch loss 1.34042144\n",
      "Validated batch 172 batch loss 1.25444937\n",
      "Validated batch 173 batch loss 1.2630837\n",
      "Validated batch 174 batch loss 1.28961802\n",
      "Validated batch 175 batch loss 1.34101522\n",
      "Validated batch 176 batch loss 1.30077481\n",
      "Validated batch 177 batch loss 1.29963756\n",
      "Validated batch 178 batch loss 1.32705653\n",
      "Validated batch 179 batch loss 1.26712918\n",
      "Validated batch 180 batch loss 1.17453945\n",
      "Validated batch 181 batch loss 1.29358089\n",
      "Validated batch 182 batch loss 1.21904612\n",
      "Validated batch 183 batch loss 1.2505393\n",
      "Validated batch 184 batch loss 1.30347764\n",
      "Validated batch 185 batch loss 1.42689133\n",
      "Epoch 3 val loss 1.2649356126785278\n",
      "Model /aiffel/aiffel/mpii/models/model_HG-epoch-3-loss-1.2649.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.28555143 epoch total loss 1.28555143\n",
      "Trained batch 2 batch loss 1.18365216 epoch total loss 1.23460174\n",
      "Trained batch 3 batch loss 1.15789628 epoch total loss 1.20903325\n",
      "Trained batch 4 batch loss 1.01760435 epoch total loss 1.16117597\n",
      "Trained batch 5 batch loss 1.0780704 epoch total loss 1.14455485\n",
      "Trained batch 6 batch loss 1.18240762 epoch total loss 1.15086365\n",
      "Trained batch 7 batch loss 1.09698069 epoch total loss 1.14316618\n",
      "Trained batch 8 batch loss 1.17997038 epoch total loss 1.14776671\n",
      "Trained batch 9 batch loss 1.19042587 epoch total loss 1.15250659\n",
      "Trained batch 10 batch loss 1.23076558 epoch total loss 1.16033244\n",
      "Trained batch 11 batch loss 1.25128484 epoch total loss 1.16860092\n",
      "Trained batch 12 batch loss 1.3331126 epoch total loss 1.18231022\n",
      "Trained batch 13 batch loss 1.32730567 epoch total loss 1.19346368\n",
      "Trained batch 14 batch loss 1.15119588 epoch total loss 1.19044459\n",
      "Trained batch 15 batch loss 1.16786051 epoch total loss 1.18893886\n",
      "Trained batch 16 batch loss 1.21442342 epoch total loss 1.19053173\n",
      "Trained batch 17 batch loss 1.33650947 epoch total loss 1.19911861\n",
      "Trained batch 18 batch loss 1.38538563 epoch total loss 1.20946681\n",
      "Trained batch 19 batch loss 1.32705259 epoch total loss 1.21565557\n",
      "Trained batch 20 batch loss 1.31570268 epoch total loss 1.22065794\n",
      "Trained batch 21 batch loss 1.138726 epoch total loss 1.21675634\n",
      "Trained batch 22 batch loss 1.2274766 epoch total loss 1.21724367\n",
      "Trained batch 23 batch loss 1.17628431 epoch total loss 1.2154628\n",
      "Trained batch 24 batch loss 1.22708082 epoch total loss 1.21594691\n",
      "Trained batch 25 batch loss 1.18363976 epoch total loss 1.21465456\n",
      "Trained batch 26 batch loss 1.13684452 epoch total loss 1.21166193\n",
      "Trained batch 27 batch loss 1.07137895 epoch total loss 1.2064662\n",
      "Trained batch 28 batch loss 1.10826766 epoch total loss 1.20295918\n",
      "Trained batch 29 batch loss 1.30761504 epoch total loss 1.206568\n",
      "Trained batch 30 batch loss 1.24355376 epoch total loss 1.20780075\n",
      "Trained batch 31 batch loss 1.47360277 epoch total loss 1.21637499\n",
      "Trained batch 32 batch loss 1.37293518 epoch total loss 1.22126758\n",
      "Trained batch 33 batch loss 1.3902452 epoch total loss 1.2263881\n",
      "Trained batch 34 batch loss 1.28765953 epoch total loss 1.22819018\n",
      "Trained batch 35 batch loss 1.20084023 epoch total loss 1.22740877\n",
      "Trained batch 36 batch loss 1.10687852 epoch total loss 1.22406065\n",
      "Trained batch 37 batch loss 1.19889987 epoch total loss 1.22338068\n",
      "Trained batch 38 batch loss 1.2838974 epoch total loss 1.2249732\n",
      "Trained batch 39 batch loss 1.33997178 epoch total loss 1.22792184\n",
      "Trained batch 40 batch loss 1.24987733 epoch total loss 1.2284708\n",
      "Trained batch 41 batch loss 1.22702277 epoch total loss 1.22843552\n",
      "Trained batch 42 batch loss 1.24308443 epoch total loss 1.22878432\n",
      "Trained batch 43 batch loss 1.23573661 epoch total loss 1.22894597\n",
      "Trained batch 44 batch loss 1.33653545 epoch total loss 1.23139119\n",
      "Trained batch 45 batch loss 1.21773863 epoch total loss 1.2310878\n",
      "Trained batch 46 batch loss 1.28448141 epoch total loss 1.23224854\n",
      "Trained batch 47 batch loss 1.27912784 epoch total loss 1.23324597\n",
      "Trained batch 48 batch loss 1.08771861 epoch total loss 1.23021424\n",
      "Trained batch 49 batch loss 1.07955217 epoch total loss 1.22713947\n",
      "Trained batch 50 batch loss 1.1380403 epoch total loss 1.22535741\n",
      "Trained batch 51 batch loss 1.150769 epoch total loss 1.22389495\n",
      "Trained batch 52 batch loss 1.11179447 epoch total loss 1.22173905\n",
      "Trained batch 53 batch loss 1.13322473 epoch total loss 1.22006905\n",
      "Trained batch 54 batch loss 1.1129756 epoch total loss 1.21808577\n",
      "Trained batch 55 batch loss 1.09147692 epoch total loss 1.21578383\n",
      "Trained batch 56 batch loss 1.18851411 epoch total loss 1.21529686\n",
      "Trained batch 57 batch loss 1.18491793 epoch total loss 1.214764\n",
      "Trained batch 58 batch loss 1.3242321 epoch total loss 1.21665144\n",
      "Trained batch 59 batch loss 1.241925 epoch total loss 1.21707976\n",
      "Trained batch 60 batch loss 1.18592453 epoch total loss 1.2165606\n",
      "Trained batch 61 batch loss 1.2919035 epoch total loss 1.21779573\n",
      "Trained batch 62 batch loss 1.15488482 epoch total loss 1.21678102\n",
      "Trained batch 63 batch loss 1.11253381 epoch total loss 1.21512628\n",
      "Trained batch 64 batch loss 1.4682498 epoch total loss 1.21908128\n",
      "Trained batch 65 batch loss 1.30296683 epoch total loss 1.22037172\n",
      "Trained batch 66 batch loss 1.23727489 epoch total loss 1.2206279\n",
      "Trained batch 67 batch loss 1.24550056 epoch total loss 1.22099912\n",
      "Trained batch 68 batch loss 1.32054937 epoch total loss 1.22246301\n",
      "Trained batch 69 batch loss 1.28343558 epoch total loss 1.22334671\n",
      "Trained batch 70 batch loss 1.21622658 epoch total loss 1.22324491\n",
      "Trained batch 71 batch loss 1.37580848 epoch total loss 1.22539365\n",
      "Trained batch 72 batch loss 1.21108913 epoch total loss 1.22519505\n",
      "Trained batch 73 batch loss 1.3550247 epoch total loss 1.22697353\n",
      "Trained batch 74 batch loss 1.22086418 epoch total loss 1.22689092\n",
      "Trained batch 75 batch loss 1.20262146 epoch total loss 1.22656739\n",
      "Trained batch 76 batch loss 1.251863 epoch total loss 1.22690022\n",
      "Trained batch 77 batch loss 1.39202 epoch total loss 1.22904468\n",
      "Trained batch 78 batch loss 1.24237645 epoch total loss 1.22921562\n",
      "Trained batch 79 batch loss 1.29334354 epoch total loss 1.23002732\n",
      "Trained batch 80 batch loss 1.20654345 epoch total loss 1.22973371\n",
      "Trained batch 81 batch loss 1.1373086 epoch total loss 1.22859263\n",
      "Trained batch 82 batch loss 1.08663678 epoch total loss 1.22686148\n",
      "Trained batch 83 batch loss 1.14710701 epoch total loss 1.22590065\n",
      "Trained batch 84 batch loss 1.18408585 epoch total loss 1.22540283\n",
      "Trained batch 85 batch loss 1.29762173 epoch total loss 1.22625244\n",
      "Trained batch 86 batch loss 1.32167 epoch total loss 1.22736204\n",
      "Trained batch 87 batch loss 1.27498913 epoch total loss 1.22790945\n",
      "Trained batch 88 batch loss 1.14178264 epoch total loss 1.22693074\n",
      "Trained batch 89 batch loss 1.14370692 epoch total loss 1.22599566\n",
      "Trained batch 90 batch loss 1.20000172 epoch total loss 1.22570682\n",
      "Trained batch 91 batch loss 1.19340396 epoch total loss 1.22535181\n",
      "Trained batch 92 batch loss 1.22717905 epoch total loss 1.22537172\n",
      "Trained batch 93 batch loss 1.27357745 epoch total loss 1.22589\n",
      "Trained batch 94 batch loss 1.33312762 epoch total loss 1.22703087\n",
      "Trained batch 95 batch loss 1.25781059 epoch total loss 1.22735488\n",
      "Trained batch 96 batch loss 1.25195479 epoch total loss 1.22761118\n",
      "Trained batch 97 batch loss 1.27381957 epoch total loss 1.22808754\n",
      "Trained batch 98 batch loss 1.32404029 epoch total loss 1.22906661\n",
      "Trained batch 99 batch loss 1.22171414 epoch total loss 1.22899246\n",
      "Trained batch 100 batch loss 1.20498264 epoch total loss 1.22875226\n",
      "Trained batch 101 batch loss 1.34818363 epoch total loss 1.22993481\n",
      "Trained batch 102 batch loss 1.26825881 epoch total loss 1.23031044\n",
      "Trained batch 103 batch loss 1.29663634 epoch total loss 1.23095441\n",
      "Trained batch 104 batch loss 1.35195577 epoch total loss 1.23211789\n",
      "Trained batch 105 batch loss 1.29354465 epoch total loss 1.23270297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 106 batch loss 1.34382296 epoch total loss 1.2337513\n",
      "Trained batch 107 batch loss 1.24169362 epoch total loss 1.23382556\n",
      "Trained batch 108 batch loss 1.18989646 epoch total loss 1.23341882\n",
      "Trained batch 109 batch loss 1.3389957 epoch total loss 1.23438728\n",
      "Trained batch 110 batch loss 1.35408247 epoch total loss 1.23547542\n",
      "Trained batch 111 batch loss 1.34824169 epoch total loss 1.23649132\n",
      "Trained batch 112 batch loss 1.19613647 epoch total loss 1.23613095\n",
      "Trained batch 113 batch loss 1.33876467 epoch total loss 1.23703921\n",
      "Trained batch 114 batch loss 1.48924184 epoch total loss 1.23925149\n",
      "Trained batch 115 batch loss 1.19810939 epoch total loss 1.23889375\n",
      "Trained batch 116 batch loss 1.2842536 epoch total loss 1.23928475\n",
      "Trained batch 117 batch loss 1.37927198 epoch total loss 1.24048126\n",
      "Trained batch 118 batch loss 1.3904748 epoch total loss 1.24175239\n",
      "Trained batch 119 batch loss 1.25941825 epoch total loss 1.2419008\n",
      "Trained batch 120 batch loss 1.29853916 epoch total loss 1.24237275\n",
      "Trained batch 121 batch loss 1.23628187 epoch total loss 1.24232244\n",
      "Trained batch 122 batch loss 1.15696144 epoch total loss 1.24162281\n",
      "Trained batch 123 batch loss 1.28331113 epoch total loss 1.24196172\n",
      "Trained batch 124 batch loss 1.26832223 epoch total loss 1.24217439\n",
      "Trained batch 125 batch loss 1.32579255 epoch total loss 1.24284327\n",
      "Trained batch 126 batch loss 1.14621985 epoch total loss 1.2420764\n",
      "Trained batch 127 batch loss 1.18551862 epoch total loss 1.24163115\n",
      "Trained batch 128 batch loss 1.22523665 epoch total loss 1.241503\n",
      "Trained batch 129 batch loss 1.21712446 epoch total loss 1.24131393\n",
      "Trained batch 130 batch loss 1.23719049 epoch total loss 1.24128234\n",
      "Trained batch 131 batch loss 1.23762524 epoch total loss 1.24125433\n",
      "Trained batch 132 batch loss 1.08690262 epoch total loss 1.24008501\n",
      "Trained batch 133 batch loss 1.25835848 epoch total loss 1.24022245\n",
      "Trained batch 134 batch loss 1.19248712 epoch total loss 1.23986626\n",
      "Trained batch 135 batch loss 1.19645309 epoch total loss 1.23954463\n",
      "Trained batch 136 batch loss 1.10813212 epoch total loss 1.23857844\n",
      "Trained batch 137 batch loss 1.20358229 epoch total loss 1.23832297\n",
      "Trained batch 138 batch loss 1.20783472 epoch total loss 1.23810208\n",
      "Trained batch 139 batch loss 1.22418046 epoch total loss 1.23800194\n",
      "Trained batch 140 batch loss 1.25203729 epoch total loss 1.23810232\n",
      "Trained batch 141 batch loss 1.2562536 epoch total loss 1.23823106\n",
      "Trained batch 142 batch loss 1.13052928 epoch total loss 1.23747253\n",
      "Trained batch 143 batch loss 1.27687573 epoch total loss 1.23774803\n",
      "Trained batch 144 batch loss 1.15532637 epoch total loss 1.23717558\n",
      "Trained batch 145 batch loss 1.23905945 epoch total loss 1.23718858\n",
      "Trained batch 146 batch loss 1.21270549 epoch total loss 1.23702097\n",
      "Trained batch 147 batch loss 1.46617806 epoch total loss 1.23857975\n",
      "Trained batch 148 batch loss 1.19502187 epoch total loss 1.23828542\n",
      "Trained batch 149 batch loss 1.23787844 epoch total loss 1.2382828\n",
      "Trained batch 150 batch loss 1.13290703 epoch total loss 1.2375803\n",
      "Trained batch 151 batch loss 1.21019423 epoch total loss 1.23739886\n",
      "Trained batch 152 batch loss 1.25600338 epoch total loss 1.23752117\n",
      "Trained batch 153 batch loss 1.25381374 epoch total loss 1.23762774\n",
      "Trained batch 154 batch loss 1.20074666 epoch total loss 1.23738825\n",
      "Trained batch 155 batch loss 1.17104483 epoch total loss 1.23696029\n",
      "Trained batch 156 batch loss 1.17742217 epoch total loss 1.23657858\n",
      "Trained batch 157 batch loss 1.26650238 epoch total loss 1.2367692\n",
      "Trained batch 158 batch loss 1.27142167 epoch total loss 1.23698854\n",
      "Trained batch 159 batch loss 1.43290365 epoch total loss 1.23822069\n",
      "Trained batch 160 batch loss 1.41923034 epoch total loss 1.23935199\n",
      "Trained batch 161 batch loss 1.37974846 epoch total loss 1.240224\n",
      "Trained batch 162 batch loss 1.28661954 epoch total loss 1.24051046\n",
      "Trained batch 163 batch loss 1.25626433 epoch total loss 1.24060714\n",
      "Trained batch 164 batch loss 1.30197179 epoch total loss 1.24098134\n",
      "Trained batch 165 batch loss 1.30616462 epoch total loss 1.2413764\n",
      "Trained batch 166 batch loss 1.15254414 epoch total loss 1.24084127\n",
      "Trained batch 167 batch loss 1.23296 epoch total loss 1.24079406\n",
      "Trained batch 168 batch loss 1.27547789 epoch total loss 1.24100053\n",
      "Trained batch 169 batch loss 1.28414679 epoch total loss 1.24125576\n",
      "Trained batch 170 batch loss 1.26257873 epoch total loss 1.24138117\n",
      "Trained batch 171 batch loss 1.09023499 epoch total loss 1.24049735\n",
      "Trained batch 172 batch loss 1.18643498 epoch total loss 1.240183\n",
      "Trained batch 173 batch loss 1.17283762 epoch total loss 1.23979378\n",
      "Trained batch 174 batch loss 1.32105792 epoch total loss 1.24026072\n",
      "Trained batch 175 batch loss 1.23070097 epoch total loss 1.24020612\n",
      "Trained batch 176 batch loss 1.36168015 epoch total loss 1.24089634\n",
      "Trained batch 177 batch loss 1.25573683 epoch total loss 1.24098015\n",
      "Trained batch 178 batch loss 1.26041853 epoch total loss 1.24108934\n",
      "Trained batch 179 batch loss 1.12928641 epoch total loss 1.24046481\n",
      "Trained batch 180 batch loss 1.23940492 epoch total loss 1.24045897\n",
      "Trained batch 181 batch loss 1.18897152 epoch total loss 1.24017441\n",
      "Trained batch 182 batch loss 1.25765526 epoch total loss 1.2402705\n",
      "Trained batch 183 batch loss 1.09356248 epoch total loss 1.23946881\n",
      "Trained batch 184 batch loss 1.31853914 epoch total loss 1.23989856\n",
      "Trained batch 185 batch loss 1.19558811 epoch total loss 1.23965907\n",
      "Trained batch 186 batch loss 1.23740208 epoch total loss 1.23964691\n",
      "Trained batch 187 batch loss 1.1539793 epoch total loss 1.23918879\n",
      "Trained batch 188 batch loss 1.31188679 epoch total loss 1.23957551\n",
      "Trained batch 189 batch loss 1.20036709 epoch total loss 1.23936796\n",
      "Trained batch 190 batch loss 1.25866961 epoch total loss 1.23946965\n",
      "Trained batch 191 batch loss 1.24768031 epoch total loss 1.23951256\n",
      "Trained batch 192 batch loss 1.23932672 epoch total loss 1.23951161\n",
      "Trained batch 193 batch loss 1.10603023 epoch total loss 1.23882008\n",
      "Trained batch 194 batch loss 1.23391199 epoch total loss 1.2387948\n",
      "Trained batch 195 batch loss 1.22799766 epoch total loss 1.23873937\n",
      "Trained batch 196 batch loss 1.15661573 epoch total loss 1.23832035\n",
      "Trained batch 197 batch loss 1.16699946 epoch total loss 1.23795831\n",
      "Trained batch 198 batch loss 1.25667644 epoch total loss 1.23805296\n",
      "Trained batch 199 batch loss 1.29176664 epoch total loss 1.23832285\n",
      "Trained batch 200 batch loss 1.30293441 epoch total loss 1.23864591\n",
      "Trained batch 201 batch loss 1.21955752 epoch total loss 1.2385509\n",
      "Trained batch 202 batch loss 1.1681993 epoch total loss 1.23820257\n",
      "Trained batch 203 batch loss 1.19326782 epoch total loss 1.23798132\n",
      "Trained batch 204 batch loss 1.17581606 epoch total loss 1.2376765\n",
      "Trained batch 205 batch loss 1.25895071 epoch total loss 1.23778033\n",
      "Trained batch 206 batch loss 1.16840267 epoch total loss 1.23744345\n",
      "Trained batch 207 batch loss 1.05210018 epoch total loss 1.23654807\n",
      "Trained batch 208 batch loss 1.18148136 epoch total loss 1.23628342\n",
      "Trained batch 209 batch loss 1.22850966 epoch total loss 1.23624623\n",
      "Trained batch 210 batch loss 1.36409712 epoch total loss 1.23685503\n",
      "Trained batch 211 batch loss 1.16642392 epoch total loss 1.23652124\n",
      "Trained batch 212 batch loss 1.13918817 epoch total loss 1.23606205\n",
      "Trained batch 213 batch loss 1.08913493 epoch total loss 1.2353723\n",
      "Trained batch 214 batch loss 1.02230608 epoch total loss 1.23437667\n",
      "Trained batch 215 batch loss 0.977502942 epoch total loss 1.23318195\n",
      "Trained batch 216 batch loss 1.03531837 epoch total loss 1.23226595\n",
      "Trained batch 217 batch loss 1.05927587 epoch total loss 1.23146868\n",
      "Trained batch 218 batch loss 0.952283621 epoch total loss 1.23018789\n",
      "Trained batch 219 batch loss 1.10508573 epoch total loss 1.22961664\n",
      "Trained batch 220 batch loss 1.37545288 epoch total loss 1.23027956\n",
      "Trained batch 221 batch loss 1.36052346 epoch total loss 1.23086894\n",
      "Trained batch 222 batch loss 1.31050658 epoch total loss 1.23122776\n",
      "Trained batch 223 batch loss 1.27374411 epoch total loss 1.23141837\n",
      "Trained batch 224 batch loss 1.40089619 epoch total loss 1.23217499\n",
      "Trained batch 225 batch loss 1.2931242 epoch total loss 1.23244584\n",
      "Trained batch 226 batch loss 1.20485342 epoch total loss 1.23232388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 227 batch loss 1.39247549 epoch total loss 1.23302937\n",
      "Trained batch 228 batch loss 1.34162939 epoch total loss 1.23350573\n",
      "Trained batch 229 batch loss 1.23134935 epoch total loss 1.23349643\n",
      "Trained batch 230 batch loss 1.08148384 epoch total loss 1.23283541\n",
      "Trained batch 231 batch loss 1.1182375 epoch total loss 1.23233926\n",
      "Trained batch 232 batch loss 1.04038644 epoch total loss 1.23151183\n",
      "Trained batch 233 batch loss 1.09988618 epoch total loss 1.2309469\n",
      "Trained batch 234 batch loss 1.2408514 epoch total loss 1.23098922\n",
      "Trained batch 235 batch loss 1.19329548 epoch total loss 1.23082888\n",
      "Trained batch 236 batch loss 1.33740795 epoch total loss 1.23128045\n",
      "Trained batch 237 batch loss 1.32443094 epoch total loss 1.23167348\n",
      "Trained batch 238 batch loss 1.20535219 epoch total loss 1.23156285\n",
      "Trained batch 239 batch loss 1.21595526 epoch total loss 1.23149753\n",
      "Trained batch 240 batch loss 1.25024152 epoch total loss 1.23157561\n",
      "Trained batch 241 batch loss 1.25666857 epoch total loss 1.2316798\n",
      "Trained batch 242 batch loss 1.16952503 epoch total loss 1.23142302\n",
      "Trained batch 243 batch loss 1.45235562 epoch total loss 1.23233223\n",
      "Trained batch 244 batch loss 1.50312662 epoch total loss 1.23344195\n",
      "Trained batch 245 batch loss 1.27561402 epoch total loss 1.23361409\n",
      "Trained batch 246 batch loss 1.26941764 epoch total loss 1.23375952\n",
      "Trained batch 247 batch loss 1.23265541 epoch total loss 1.23375511\n",
      "Trained batch 248 batch loss 0.973736703 epoch total loss 1.23270667\n",
      "Trained batch 249 batch loss 1.08787274 epoch total loss 1.23212492\n",
      "Trained batch 250 batch loss 1.21907842 epoch total loss 1.23207271\n",
      "Trained batch 251 batch loss 1.04629135 epoch total loss 1.23133266\n",
      "Trained batch 252 batch loss 1.00880134 epoch total loss 1.23044944\n",
      "Trained batch 253 batch loss 1.00592244 epoch total loss 1.22956204\n",
      "Trained batch 254 batch loss 1.07925606 epoch total loss 1.22897029\n",
      "Trained batch 255 batch loss 1.07042897 epoch total loss 1.22834861\n",
      "Trained batch 256 batch loss 1.24260795 epoch total loss 1.22840428\n",
      "Trained batch 257 batch loss 1.29058802 epoch total loss 1.22864628\n",
      "Trained batch 258 batch loss 1.22203505 epoch total loss 1.22862065\n",
      "Trained batch 259 batch loss 1.31489253 epoch total loss 1.22895372\n",
      "Trained batch 260 batch loss 1.2373997 epoch total loss 1.22898614\n",
      "Trained batch 261 batch loss 1.29721987 epoch total loss 1.22924757\n",
      "Trained batch 262 batch loss 1.25269496 epoch total loss 1.2293371\n",
      "Trained batch 263 batch loss 1.31197155 epoch total loss 1.22965133\n",
      "Trained batch 264 batch loss 1.33728659 epoch total loss 1.23005891\n",
      "Trained batch 265 batch loss 1.31141806 epoch total loss 1.23036599\n",
      "Trained batch 266 batch loss 1.4029516 epoch total loss 1.23101485\n",
      "Trained batch 267 batch loss 1.31850386 epoch total loss 1.23134255\n",
      "Trained batch 268 batch loss 1.27987838 epoch total loss 1.23152363\n",
      "Trained batch 269 batch loss 1.20407355 epoch total loss 1.23142159\n",
      "Trained batch 270 batch loss 1.23146605 epoch total loss 1.23142183\n",
      "Trained batch 271 batch loss 1.18105364 epoch total loss 1.23123598\n",
      "Trained batch 272 batch loss 1.23441744 epoch total loss 1.23124766\n",
      "Trained batch 273 batch loss 1.27451789 epoch total loss 1.23140609\n",
      "Trained batch 274 batch loss 1.1963 epoch total loss 1.23127794\n",
      "Trained batch 275 batch loss 1.19374228 epoch total loss 1.23114145\n",
      "Trained batch 276 batch loss 1.04954171 epoch total loss 1.23048341\n",
      "Trained batch 277 batch loss 1.17129564 epoch total loss 1.23026979\n",
      "Trained batch 278 batch loss 1.24734569 epoch total loss 1.23033118\n",
      "Trained batch 279 batch loss 1.20286798 epoch total loss 1.23023283\n",
      "Trained batch 280 batch loss 1.24730444 epoch total loss 1.23029387\n",
      "Trained batch 281 batch loss 1.27017105 epoch total loss 1.23043573\n",
      "Trained batch 282 batch loss 1.26400208 epoch total loss 1.23055482\n",
      "Trained batch 283 batch loss 1.23502254 epoch total loss 1.23057055\n",
      "Trained batch 284 batch loss 1.25003362 epoch total loss 1.2306391\n",
      "Trained batch 285 batch loss 1.17928362 epoch total loss 1.23045886\n",
      "Trained batch 286 batch loss 1.31492674 epoch total loss 1.23075426\n",
      "Trained batch 287 batch loss 1.33355761 epoch total loss 1.23111248\n",
      "Trained batch 288 batch loss 1.40958118 epoch total loss 1.23173213\n",
      "Trained batch 289 batch loss 1.37838268 epoch total loss 1.2322396\n",
      "Trained batch 290 batch loss 1.21026611 epoch total loss 1.23216379\n",
      "Trained batch 291 batch loss 1.22576261 epoch total loss 1.23214185\n",
      "Trained batch 292 batch loss 1.25463903 epoch total loss 1.23221886\n",
      "Trained batch 293 batch loss 1.2929064 epoch total loss 1.23242605\n",
      "Trained batch 294 batch loss 1.3364327 epoch total loss 1.23277974\n",
      "Trained batch 295 batch loss 1.38411081 epoch total loss 1.23329282\n",
      "Trained batch 296 batch loss 1.50215542 epoch total loss 1.23420119\n",
      "Trained batch 297 batch loss 1.32220566 epoch total loss 1.23449743\n",
      "Trained batch 298 batch loss 1.29668128 epoch total loss 1.23470616\n",
      "Trained batch 299 batch loss 1.34699845 epoch total loss 1.23508167\n",
      "Trained batch 300 batch loss 1.35711 epoch total loss 1.23548853\n",
      "Trained batch 301 batch loss 1.38565373 epoch total loss 1.23598731\n",
      "Trained batch 302 batch loss 1.28944516 epoch total loss 1.23616445\n",
      "Trained batch 303 batch loss 1.07489491 epoch total loss 1.23563218\n",
      "Trained batch 304 batch loss 1.18368781 epoch total loss 1.23546124\n",
      "Trained batch 305 batch loss 1.26214826 epoch total loss 1.23554873\n",
      "Trained batch 306 batch loss 1.10115266 epoch total loss 1.23510957\n",
      "Trained batch 307 batch loss 1.18177319 epoch total loss 1.23493588\n",
      "Trained batch 308 batch loss 1.28497839 epoch total loss 1.23509836\n",
      "Trained batch 309 batch loss 1.29232335 epoch total loss 1.23528349\n",
      "Trained batch 310 batch loss 1.34428763 epoch total loss 1.23563516\n",
      "Trained batch 311 batch loss 1.28449464 epoch total loss 1.23579228\n",
      "Trained batch 312 batch loss 1.23132372 epoch total loss 1.23577797\n",
      "Trained batch 313 batch loss 1.31714523 epoch total loss 1.23603785\n",
      "Trained batch 314 batch loss 1.33588529 epoch total loss 1.23635578\n",
      "Trained batch 315 batch loss 1.21754014 epoch total loss 1.23629606\n",
      "Trained batch 316 batch loss 1.23300791 epoch total loss 1.23628569\n",
      "Trained batch 317 batch loss 1.23904705 epoch total loss 1.23629439\n",
      "Trained batch 318 batch loss 1.28610504 epoch total loss 1.23645091\n",
      "Trained batch 319 batch loss 1.10609436 epoch total loss 1.23604226\n",
      "Trained batch 320 batch loss 1.15671682 epoch total loss 1.23579431\n",
      "Trained batch 321 batch loss 1.15119362 epoch total loss 1.23553073\n",
      "Trained batch 322 batch loss 1.11965919 epoch total loss 1.23517096\n",
      "Trained batch 323 batch loss 1.1706593 epoch total loss 1.23497117\n",
      "Trained batch 324 batch loss 1.18824482 epoch total loss 1.23482692\n",
      "Trained batch 325 batch loss 1.22575164 epoch total loss 1.23479891\n",
      "Trained batch 326 batch loss 1.5612781 epoch total loss 1.23580039\n",
      "Trained batch 327 batch loss 1.37623072 epoch total loss 1.2362299\n",
      "Trained batch 328 batch loss 1.29904497 epoch total loss 1.23642135\n",
      "Trained batch 329 batch loss 1.28516042 epoch total loss 1.23656952\n",
      "Trained batch 330 batch loss 1.43965292 epoch total loss 1.23718488\n",
      "Trained batch 331 batch loss 1.33476806 epoch total loss 1.23747981\n",
      "Trained batch 332 batch loss 1.24488 epoch total loss 1.2375021\n",
      "Trained batch 333 batch loss 1.22955084 epoch total loss 1.23747814\n",
      "Trained batch 334 batch loss 1.19653177 epoch total loss 1.23735559\n",
      "Trained batch 335 batch loss 1.21685874 epoch total loss 1.23729444\n",
      "Trained batch 336 batch loss 1.3594327 epoch total loss 1.2376579\n",
      "Trained batch 337 batch loss 1.27596951 epoch total loss 1.23777163\n",
      "Trained batch 338 batch loss 1.33548677 epoch total loss 1.23806071\n",
      "Trained batch 339 batch loss 1.2571876 epoch total loss 1.2381171\n",
      "Trained batch 340 batch loss 1.36748219 epoch total loss 1.23849761\n",
      "Trained batch 341 batch loss 1.35732055 epoch total loss 1.23884618\n",
      "Trained batch 342 batch loss 1.17620158 epoch total loss 1.23866296\n",
      "Trained batch 343 batch loss 1.09538317 epoch total loss 1.23824525\n",
      "Trained batch 344 batch loss 1.2025274 epoch total loss 1.23814142\n",
      "Trained batch 345 batch loss 1.19425035 epoch total loss 1.23801422\n",
      "Trained batch 346 batch loss 1.23137844 epoch total loss 1.23799503\n",
      "Trained batch 347 batch loss 1.21447241 epoch total loss 1.23792732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 348 batch loss 1.16688848 epoch total loss 1.23772311\n",
      "Trained batch 349 batch loss 1.21419716 epoch total loss 1.23765576\n",
      "Trained batch 350 batch loss 1.18757331 epoch total loss 1.23751259\n",
      "Trained batch 351 batch loss 1.17669725 epoch total loss 1.23733938\n",
      "Trained batch 352 batch loss 1.0947969 epoch total loss 1.23693442\n",
      "Trained batch 353 batch loss 1.32567477 epoch total loss 1.23718584\n",
      "Trained batch 354 batch loss 1.22308159 epoch total loss 1.23714602\n",
      "Trained batch 355 batch loss 1.05061793 epoch total loss 1.23662055\n",
      "Trained batch 356 batch loss 1.22673798 epoch total loss 1.23659289\n",
      "Trained batch 357 batch loss 1.1490159 epoch total loss 1.23634756\n",
      "Trained batch 358 batch loss 1.1896708 epoch total loss 1.23621714\n",
      "Trained batch 359 batch loss 1.0887208 epoch total loss 1.23580623\n",
      "Trained batch 360 batch loss 1.15270317 epoch total loss 1.23557544\n",
      "Trained batch 361 batch loss 1.11506772 epoch total loss 1.23524165\n",
      "Trained batch 362 batch loss 1.21533012 epoch total loss 1.2351867\n",
      "Trained batch 363 batch loss 1.24960804 epoch total loss 1.23522639\n",
      "Trained batch 364 batch loss 1.2232945 epoch total loss 1.23519361\n",
      "Trained batch 365 batch loss 1.33909714 epoch total loss 1.23547828\n",
      "Trained batch 366 batch loss 1.26614571 epoch total loss 1.23556209\n",
      "Trained batch 367 batch loss 1.2555989 epoch total loss 1.23561668\n",
      "Trained batch 368 batch loss 1.30119348 epoch total loss 1.2357949\n",
      "Trained batch 369 batch loss 1.09730887 epoch total loss 1.23541963\n",
      "Trained batch 370 batch loss 1.03601491 epoch total loss 1.23488069\n",
      "Trained batch 371 batch loss 1.11951065 epoch total loss 1.23456967\n",
      "Trained batch 372 batch loss 1.12792516 epoch total loss 1.23428309\n",
      "Trained batch 373 batch loss 1.13221645 epoch total loss 1.23400939\n",
      "Trained batch 374 batch loss 1.23742437 epoch total loss 1.23401845\n",
      "Trained batch 375 batch loss 1.10730648 epoch total loss 1.23368061\n",
      "Trained batch 376 batch loss 1.18460751 epoch total loss 1.23355007\n",
      "Trained batch 377 batch loss 1.15258193 epoch total loss 1.23333526\n",
      "Trained batch 378 batch loss 1.22727454 epoch total loss 1.23331928\n",
      "Trained batch 379 batch loss 1.23553157 epoch total loss 1.23332512\n",
      "Trained batch 380 batch loss 1.11066508 epoch total loss 1.23300231\n",
      "Trained batch 381 batch loss 1.12178564 epoch total loss 1.23271036\n",
      "Trained batch 382 batch loss 1.11982739 epoch total loss 1.23241496\n",
      "Trained batch 383 batch loss 1.18208909 epoch total loss 1.23228359\n",
      "Trained batch 384 batch loss 1.26831055 epoch total loss 1.23237741\n",
      "Trained batch 385 batch loss 1.42208922 epoch total loss 1.2328701\n",
      "Trained batch 386 batch loss 1.47634971 epoch total loss 1.23350096\n",
      "Trained batch 387 batch loss 1.38450766 epoch total loss 1.23389113\n",
      "Trained batch 388 batch loss 1.22531521 epoch total loss 1.23386908\n",
      "Trained batch 389 batch loss 1.23708212 epoch total loss 1.2338773\n",
      "Trained batch 390 batch loss 1.28148806 epoch total loss 1.23399937\n",
      "Trained batch 391 batch loss 1.2247808 epoch total loss 1.23397589\n",
      "Trained batch 392 batch loss 1.23952365 epoch total loss 1.23399007\n",
      "Trained batch 393 batch loss 1.22419059 epoch total loss 1.23396504\n",
      "Trained batch 394 batch loss 1.28261185 epoch total loss 1.23408854\n",
      "Trained batch 395 batch loss 1.24446321 epoch total loss 1.23411489\n",
      "Trained batch 396 batch loss 1.13874567 epoch total loss 1.23387396\n",
      "Trained batch 397 batch loss 1.21141219 epoch total loss 1.23381746\n",
      "Trained batch 398 batch loss 1.1573627 epoch total loss 1.23362529\n",
      "Trained batch 399 batch loss 1.16317546 epoch total loss 1.23344874\n",
      "Trained batch 400 batch loss 0.985446036 epoch total loss 1.23282874\n",
      "Trained batch 401 batch loss 1.12279725 epoch total loss 1.23255432\n",
      "Trained batch 402 batch loss 1.16828489 epoch total loss 1.23239446\n",
      "Trained batch 403 batch loss 1.18639433 epoch total loss 1.23228037\n",
      "Trained batch 404 batch loss 1.05673862 epoch total loss 1.23184586\n",
      "Trained batch 405 batch loss 1.15807343 epoch total loss 1.2316637\n",
      "Trained batch 406 batch loss 1.13497686 epoch total loss 1.23142552\n",
      "Trained batch 407 batch loss 1.36536777 epoch total loss 1.23175466\n",
      "Trained batch 408 batch loss 1.24888897 epoch total loss 1.23179662\n",
      "Trained batch 409 batch loss 1.29693294 epoch total loss 1.23195589\n",
      "Trained batch 410 batch loss 1.23715651 epoch total loss 1.23196852\n",
      "Trained batch 411 batch loss 1.29369199 epoch total loss 1.23211873\n",
      "Trained batch 412 batch loss 1.30178499 epoch total loss 1.23228788\n",
      "Trained batch 413 batch loss 1.29523051 epoch total loss 1.23244023\n",
      "Trained batch 414 batch loss 1.16518772 epoch total loss 1.23227787\n",
      "Trained batch 415 batch loss 1.18638289 epoch total loss 1.23216724\n",
      "Trained batch 416 batch loss 1.30484319 epoch total loss 1.23234189\n",
      "Trained batch 417 batch loss 1.16641903 epoch total loss 1.23218393\n",
      "Trained batch 418 batch loss 1.17253911 epoch total loss 1.23204124\n",
      "Trained batch 419 batch loss 1.30738485 epoch total loss 1.23222101\n",
      "Trained batch 420 batch loss 1.37315214 epoch total loss 1.23255658\n",
      "Trained batch 421 batch loss 1.22982728 epoch total loss 1.23255\n",
      "Trained batch 422 batch loss 1.06712484 epoch total loss 1.23215806\n",
      "Trained batch 423 batch loss 1.05965042 epoch total loss 1.23175025\n",
      "Trained batch 424 batch loss 1.1126895 epoch total loss 1.23146939\n",
      "Trained batch 425 batch loss 1.00985444 epoch total loss 1.23094785\n",
      "Trained batch 426 batch loss 1.08248568 epoch total loss 1.23059928\n",
      "Trained batch 427 batch loss 0.977276862 epoch total loss 1.2300061\n",
      "Trained batch 428 batch loss 1.12438917 epoch total loss 1.22975934\n",
      "Trained batch 429 batch loss 1.19434655 epoch total loss 1.22967672\n",
      "Trained batch 430 batch loss 1.14758039 epoch total loss 1.22948575\n",
      "Trained batch 431 batch loss 1.26517677 epoch total loss 1.2295686\n",
      "Trained batch 432 batch loss 1.28974891 epoch total loss 1.22970796\n",
      "Trained batch 433 batch loss 1.22770774 epoch total loss 1.22970331\n",
      "Trained batch 434 batch loss 1.23695719 epoch total loss 1.22972\n",
      "Trained batch 435 batch loss 1.22012782 epoch total loss 1.22969806\n",
      "Trained batch 436 batch loss 1.36258554 epoch total loss 1.23000288\n",
      "Trained batch 437 batch loss 1.32596123 epoch total loss 1.23022246\n",
      "Trained batch 438 batch loss 1.15645266 epoch total loss 1.23005402\n",
      "Trained batch 439 batch loss 1.09682703 epoch total loss 1.22975051\n",
      "Trained batch 440 batch loss 1.0206399 epoch total loss 1.22927523\n",
      "Trained batch 441 batch loss 1.17399859 epoch total loss 1.22914994\n",
      "Trained batch 442 batch loss 1.31843364 epoch total loss 1.22935188\n",
      "Trained batch 443 batch loss 1.20408046 epoch total loss 1.2292949\n",
      "Trained batch 444 batch loss 1.27283 epoch total loss 1.22939289\n",
      "Trained batch 445 batch loss 1.33289421 epoch total loss 1.22962546\n",
      "Trained batch 446 batch loss 1.23301888 epoch total loss 1.22963309\n",
      "Trained batch 447 batch loss 1.22825313 epoch total loss 1.22963011\n",
      "Trained batch 448 batch loss 1.24206161 epoch total loss 1.22965789\n",
      "Trained batch 449 batch loss 1.32403159 epoch total loss 1.22986805\n",
      "Trained batch 450 batch loss 1.38832355 epoch total loss 1.23022008\n",
      "Trained batch 451 batch loss 1.30005431 epoch total loss 1.23037493\n",
      "Trained batch 452 batch loss 1.16668558 epoch total loss 1.23023403\n",
      "Trained batch 453 batch loss 1.16526043 epoch total loss 1.23009074\n",
      "Trained batch 454 batch loss 1.15506959 epoch total loss 1.22992551\n",
      "Trained batch 455 batch loss 1.1939795 epoch total loss 1.22984648\n",
      "Trained batch 456 batch loss 1.24450815 epoch total loss 1.22987866\n",
      "Trained batch 457 batch loss 1.22562194 epoch total loss 1.22986937\n",
      "Trained batch 458 batch loss 1.17701972 epoch total loss 1.22975397\n",
      "Trained batch 459 batch loss 1.2660538 epoch total loss 1.22983301\n",
      "Trained batch 460 batch loss 1.23679805 epoch total loss 1.22984815\n",
      "Trained batch 461 batch loss 1.28042448 epoch total loss 1.22995782\n",
      "Trained batch 462 batch loss 1.29612279 epoch total loss 1.23010111\n",
      "Trained batch 463 batch loss 1.26679122 epoch total loss 1.23018026\n",
      "Trained batch 464 batch loss 1.07076836 epoch total loss 1.2298367\n",
      "Trained batch 465 batch loss 1.13741493 epoch total loss 1.22963786\n",
      "Trained batch 466 batch loss 1.09931231 epoch total loss 1.2293582\n",
      "Trained batch 467 batch loss 1.22360492 epoch total loss 1.22934592\n",
      "Trained batch 468 batch loss 1.31329763 epoch total loss 1.22952533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 469 batch loss 1.28772545 epoch total loss 1.22964942\n",
      "Trained batch 470 batch loss 1.30256844 epoch total loss 1.22980452\n",
      "Trained batch 471 batch loss 1.22300065 epoch total loss 1.22979009\n",
      "Trained batch 472 batch loss 1.22112441 epoch total loss 1.22977173\n",
      "Trained batch 473 batch loss 1.20990729 epoch total loss 1.22972977\n",
      "Trained batch 474 batch loss 1.26535082 epoch total loss 1.22980499\n",
      "Trained batch 475 batch loss 1.13210177 epoch total loss 1.22959924\n",
      "Trained batch 476 batch loss 1.25684464 epoch total loss 1.22965646\n",
      "Trained batch 477 batch loss 1.26475763 epoch total loss 1.22973\n",
      "Trained batch 478 batch loss 1.16164839 epoch total loss 1.22958755\n",
      "Trained batch 479 batch loss 1.26619887 epoch total loss 1.22966397\n",
      "Trained batch 480 batch loss 1.36221313 epoch total loss 1.22994\n",
      "Trained batch 481 batch loss 1.55189848 epoch total loss 1.2306093\n",
      "Trained batch 482 batch loss 1.42511535 epoch total loss 1.23101282\n",
      "Trained batch 483 batch loss 1.36541939 epoch total loss 1.23129117\n",
      "Trained batch 484 batch loss 1.31271577 epoch total loss 1.23145938\n",
      "Trained batch 485 batch loss 1.30507791 epoch total loss 1.23161113\n",
      "Trained batch 486 batch loss 1.19187355 epoch total loss 1.23152947\n",
      "Trained batch 487 batch loss 1.11673689 epoch total loss 1.2312938\n",
      "Trained batch 488 batch loss 1.22834444 epoch total loss 1.23128772\n",
      "Trained batch 489 batch loss 1.33797669 epoch total loss 1.23150587\n",
      "Trained batch 490 batch loss 1.30183697 epoch total loss 1.2316494\n",
      "Trained batch 491 batch loss 1.2319541 epoch total loss 1.23164988\n",
      "Trained batch 492 batch loss 1.13346887 epoch total loss 1.23145044\n",
      "Trained batch 493 batch loss 1.17636657 epoch total loss 1.23133874\n",
      "Trained batch 494 batch loss 1.0541302 epoch total loss 1.23098\n",
      "Trained batch 495 batch loss 1.14592934 epoch total loss 1.23080826\n",
      "Trained batch 496 batch loss 1.14309132 epoch total loss 1.23063135\n",
      "Trained batch 497 batch loss 1.08823919 epoch total loss 1.23034489\n",
      "Trained batch 498 batch loss 1.17022192 epoch total loss 1.23022413\n",
      "Trained batch 499 batch loss 1.1305548 epoch total loss 1.23002434\n",
      "Trained batch 500 batch loss 1.27081525 epoch total loss 1.230106\n",
      "Trained batch 501 batch loss 1.34904671 epoch total loss 1.23034334\n",
      "Trained batch 502 batch loss 1.21490765 epoch total loss 1.23031259\n",
      "Trained batch 503 batch loss 1.2291894 epoch total loss 1.23031044\n",
      "Trained batch 504 batch loss 1.36039877 epoch total loss 1.23056853\n",
      "Trained batch 505 batch loss 1.27356422 epoch total loss 1.23065364\n",
      "Trained batch 506 batch loss 1.27902961 epoch total loss 1.23074937\n",
      "Trained batch 507 batch loss 1.36117 epoch total loss 1.2310065\n",
      "Trained batch 508 batch loss 1.4129442 epoch total loss 1.23136473\n",
      "Trained batch 509 batch loss 1.29117072 epoch total loss 1.23148227\n",
      "Trained batch 510 batch loss 1.16507626 epoch total loss 1.23135209\n",
      "Trained batch 511 batch loss 1.2961452 epoch total loss 1.23147893\n",
      "Trained batch 512 batch loss 1.28559875 epoch total loss 1.23158455\n",
      "Trained batch 513 batch loss 1.33296132 epoch total loss 1.23178208\n",
      "Trained batch 514 batch loss 1.29233599 epoch total loss 1.2319\n",
      "Trained batch 515 batch loss 1.22676992 epoch total loss 1.23189\n",
      "Trained batch 516 batch loss 1.2266382 epoch total loss 1.23187983\n",
      "Trained batch 517 batch loss 1.28153682 epoch total loss 1.23197591\n",
      "Trained batch 518 batch loss 1.3358072 epoch total loss 1.2321763\n",
      "Trained batch 519 batch loss 1.27724051 epoch total loss 1.23226309\n",
      "Trained batch 520 batch loss 1.1779722 epoch total loss 1.23215866\n",
      "Trained batch 521 batch loss 1.21903062 epoch total loss 1.23213363\n",
      "Trained batch 522 batch loss 1.26762664 epoch total loss 1.23220158\n",
      "Trained batch 523 batch loss 1.18073356 epoch total loss 1.23210311\n",
      "Trained batch 524 batch loss 1.09434748 epoch total loss 1.23184025\n",
      "Trained batch 525 batch loss 1.1250459 epoch total loss 1.23163688\n",
      "Trained batch 526 batch loss 1.1556828 epoch total loss 1.23149252\n",
      "Trained batch 527 batch loss 1.19036043 epoch total loss 1.23141456\n",
      "Trained batch 528 batch loss 1.19010603 epoch total loss 1.23133636\n",
      "Trained batch 529 batch loss 1.25761628 epoch total loss 1.23138607\n",
      "Trained batch 530 batch loss 1.26304042 epoch total loss 1.23144579\n",
      "Trained batch 531 batch loss 1.16726613 epoch total loss 1.23132479\n",
      "Trained batch 532 batch loss 1.32564282 epoch total loss 1.23150206\n",
      "Trained batch 533 batch loss 1.27603829 epoch total loss 1.23158574\n",
      "Trained batch 534 batch loss 1.25843787 epoch total loss 1.23163593\n",
      "Trained batch 535 batch loss 1.21861076 epoch total loss 1.23161161\n",
      "Trained batch 536 batch loss 1.23687816 epoch total loss 1.2316215\n",
      "Trained batch 537 batch loss 1.16942036 epoch total loss 1.23150563\n",
      "Trained batch 538 batch loss 1.13538837 epoch total loss 1.23132694\n",
      "Trained batch 539 batch loss 1.2095468 epoch total loss 1.23128653\n",
      "Trained batch 540 batch loss 1.29219139 epoch total loss 1.2313993\n",
      "Trained batch 541 batch loss 1.18697453 epoch total loss 1.23131716\n",
      "Trained batch 542 batch loss 1.2795763 epoch total loss 1.23140621\n",
      "Trained batch 543 batch loss 1.18841314 epoch total loss 1.23132706\n",
      "Trained batch 544 batch loss 1.25850773 epoch total loss 1.23137701\n",
      "Trained batch 545 batch loss 1.25199962 epoch total loss 1.23141479\n",
      "Trained batch 546 batch loss 1.07584894 epoch total loss 1.23113\n",
      "Trained batch 547 batch loss 1.09880567 epoch total loss 1.23088813\n",
      "Trained batch 548 batch loss 1.13431263 epoch total loss 1.23071194\n",
      "Trained batch 549 batch loss 1.14419985 epoch total loss 1.23055434\n",
      "Trained batch 550 batch loss 1.16098332 epoch total loss 1.23042786\n",
      "Trained batch 551 batch loss 1.27406442 epoch total loss 1.23050714\n",
      "Trained batch 552 batch loss 1.36392093 epoch total loss 1.23074877\n",
      "Trained batch 553 batch loss 1.22944164 epoch total loss 1.23074639\n",
      "Trained batch 554 batch loss 1.18927169 epoch total loss 1.23067153\n",
      "Trained batch 555 batch loss 1.3682636 epoch total loss 1.23091936\n",
      "Trained batch 556 batch loss 1.37755084 epoch total loss 1.23118317\n",
      "Trained batch 557 batch loss 1.45754719 epoch total loss 1.23158956\n",
      "Trained batch 558 batch loss 1.36374247 epoch total loss 1.23182642\n",
      "Trained batch 559 batch loss 1.27589107 epoch total loss 1.23190522\n",
      "Trained batch 560 batch loss 1.08302164 epoch total loss 1.23163927\n",
      "Trained batch 561 batch loss 1.34490657 epoch total loss 1.23184121\n",
      "Trained batch 562 batch loss 1.43232727 epoch total loss 1.23219788\n",
      "Trained batch 563 batch loss 1.30851972 epoch total loss 1.23233354\n",
      "Trained batch 564 batch loss 1.33456218 epoch total loss 1.23251474\n",
      "Trained batch 565 batch loss 1.34550798 epoch total loss 1.23271477\n",
      "Trained batch 566 batch loss 1.27410913 epoch total loss 1.23278785\n",
      "Trained batch 567 batch loss 1.31840491 epoch total loss 1.23293889\n",
      "Trained batch 568 batch loss 1.20436454 epoch total loss 1.23288858\n",
      "Trained batch 569 batch loss 1.23875082 epoch total loss 1.23289895\n",
      "Trained batch 570 batch loss 1.21420908 epoch total loss 1.23286617\n",
      "Trained batch 571 batch loss 1.2669307 epoch total loss 1.23292577\n",
      "Trained batch 572 batch loss 1.31582654 epoch total loss 1.23307073\n",
      "Trained batch 573 batch loss 1.30491841 epoch total loss 1.23319614\n",
      "Trained batch 574 batch loss 1.36610079 epoch total loss 1.23342764\n",
      "Trained batch 575 batch loss 1.36957061 epoch total loss 1.23366439\n",
      "Trained batch 576 batch loss 1.28771949 epoch total loss 1.23375833\n",
      "Trained batch 577 batch loss 1.27484465 epoch total loss 1.2338295\n",
      "Trained batch 578 batch loss 1.1600132 epoch total loss 1.23370183\n",
      "Trained batch 579 batch loss 1.25808871 epoch total loss 1.23374403\n",
      "Trained batch 580 batch loss 1.28926754 epoch total loss 1.23383963\n",
      "Trained batch 581 batch loss 1.21910405 epoch total loss 1.23381436\n",
      "Trained batch 582 batch loss 1.18437648 epoch total loss 1.23372936\n",
      "Trained batch 583 batch loss 1.21963573 epoch total loss 1.23370528\n",
      "Trained batch 584 batch loss 1.23024511 epoch total loss 1.23369932\n",
      "Trained batch 585 batch loss 1.24555898 epoch total loss 1.23371959\n",
      "Trained batch 586 batch loss 1.39922953 epoch total loss 1.23400199\n",
      "Trained batch 587 batch loss 1.34002304 epoch total loss 1.2341826\n",
      "Trained batch 588 batch loss 1.24436498 epoch total loss 1.2342\n",
      "Trained batch 589 batch loss 1.27226543 epoch total loss 1.23426461\n",
      "Trained batch 590 batch loss 1.33221734 epoch total loss 1.23443067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 591 batch loss 1.31991363 epoch total loss 1.23457527\n",
      "Trained batch 592 batch loss 1.16322589 epoch total loss 1.23445475\n",
      "Trained batch 593 batch loss 1.19398475 epoch total loss 1.23438644\n",
      "Trained batch 594 batch loss 1.18253899 epoch total loss 1.23429918\n",
      "Trained batch 595 batch loss 1.12150967 epoch total loss 1.23410964\n",
      "Trained batch 596 batch loss 1.08597922 epoch total loss 1.23386109\n",
      "Trained batch 597 batch loss 1.07821441 epoch total loss 1.23360038\n",
      "Trained batch 598 batch loss 1.12949145 epoch total loss 1.23342633\n",
      "Trained batch 599 batch loss 1.03026199 epoch total loss 1.23308718\n",
      "Trained batch 600 batch loss 0.95124954 epoch total loss 1.23261738\n",
      "Trained batch 601 batch loss 0.914200485 epoch total loss 1.23208761\n",
      "Trained batch 602 batch loss 1.01085424 epoch total loss 1.23172009\n",
      "Trained batch 603 batch loss 1.19325602 epoch total loss 1.23165619\n",
      "Trained batch 604 batch loss 1.27793741 epoch total loss 1.23173296\n",
      "Trained batch 605 batch loss 1.14117169 epoch total loss 1.23158324\n",
      "Trained batch 606 batch loss 1.16227114 epoch total loss 1.23146892\n",
      "Trained batch 607 batch loss 1.24023461 epoch total loss 1.23148334\n",
      "Trained batch 608 batch loss 1.3618654 epoch total loss 1.2316978\n",
      "Trained batch 609 batch loss 1.25336361 epoch total loss 1.23173332\n",
      "Trained batch 610 batch loss 1.23443866 epoch total loss 1.23173773\n",
      "Trained batch 611 batch loss 1.2978971 epoch total loss 1.23184609\n",
      "Trained batch 612 batch loss 1.18733156 epoch total loss 1.23177338\n",
      "Trained batch 613 batch loss 1.30266285 epoch total loss 1.23188901\n",
      "Trained batch 614 batch loss 1.29122138 epoch total loss 1.23198557\n",
      "Trained batch 615 batch loss 1.27148032 epoch total loss 1.23204982\n",
      "Trained batch 616 batch loss 1.11682963 epoch total loss 1.23186278\n",
      "Trained batch 617 batch loss 1.17819476 epoch total loss 1.23177576\n",
      "Trained batch 618 batch loss 1.23291337 epoch total loss 1.23177767\n",
      "Trained batch 619 batch loss 1.27394867 epoch total loss 1.23184574\n",
      "Trained batch 620 batch loss 1.2487824 epoch total loss 1.23187304\n",
      "Trained batch 621 batch loss 1.24107623 epoch total loss 1.23188794\n",
      "Trained batch 622 batch loss 1.35547161 epoch total loss 1.23208654\n",
      "Trained batch 623 batch loss 1.15993 epoch total loss 1.23197067\n",
      "Trained batch 624 batch loss 1.12772918 epoch total loss 1.23180366\n",
      "Trained batch 625 batch loss 1.10545075 epoch total loss 1.2316016\n",
      "Trained batch 626 batch loss 1.31428611 epoch total loss 1.23173368\n",
      "Trained batch 627 batch loss 1.25507236 epoch total loss 1.23177087\n",
      "Trained batch 628 batch loss 1.2892313 epoch total loss 1.23186231\n",
      "Trained batch 629 batch loss 1.26578295 epoch total loss 1.23191631\n",
      "Trained batch 630 batch loss 1.22699499 epoch total loss 1.23190856\n",
      "Trained batch 631 batch loss 1.22616613 epoch total loss 1.2318995\n",
      "Trained batch 632 batch loss 1.26552713 epoch total loss 1.23195267\n",
      "Trained batch 633 batch loss 1.25365472 epoch total loss 1.23198688\n",
      "Trained batch 634 batch loss 1.2069726 epoch total loss 1.23194742\n",
      "Trained batch 635 batch loss 1.23729193 epoch total loss 1.23195589\n",
      "Trained batch 636 batch loss 1.06999457 epoch total loss 1.23170125\n",
      "Trained batch 637 batch loss 1.10697043 epoch total loss 1.23150551\n",
      "Trained batch 638 batch loss 1.12941241 epoch total loss 1.23134542\n",
      "Trained batch 639 batch loss 1.20534635 epoch total loss 1.23130476\n",
      "Trained batch 640 batch loss 1.23011172 epoch total loss 1.23130286\n",
      "Trained batch 641 batch loss 1.18235016 epoch total loss 1.23122644\n",
      "Trained batch 642 batch loss 1.09181857 epoch total loss 1.23100936\n",
      "Trained batch 643 batch loss 1.18982601 epoch total loss 1.23094523\n",
      "Trained batch 644 batch loss 1.37454236 epoch total loss 1.23116827\n",
      "Trained batch 645 batch loss 1.23858619 epoch total loss 1.23117983\n",
      "Trained batch 646 batch loss 1.18943596 epoch total loss 1.23111522\n",
      "Trained batch 647 batch loss 1.13031268 epoch total loss 1.23095942\n",
      "Trained batch 648 batch loss 1.02078235 epoch total loss 1.23063505\n",
      "Trained batch 649 batch loss 0.939808726 epoch total loss 1.23018694\n",
      "Trained batch 650 batch loss 0.94661361 epoch total loss 1.22975063\n",
      "Trained batch 651 batch loss 1.01460314 epoch total loss 1.22942007\n",
      "Trained batch 652 batch loss 1.23505425 epoch total loss 1.22942877\n",
      "Trained batch 653 batch loss 1.01958442 epoch total loss 1.22910738\n",
      "Trained batch 654 batch loss 1.04846549 epoch total loss 1.22883117\n",
      "Trained batch 655 batch loss 1.02663052 epoch total loss 1.22852242\n",
      "Trained batch 656 batch loss 1.10198081 epoch total loss 1.22832954\n",
      "Trained batch 657 batch loss 1.2645365 epoch total loss 1.22838461\n",
      "Trained batch 658 batch loss 1.25240541 epoch total loss 1.22842109\n",
      "Trained batch 659 batch loss 1.43177044 epoch total loss 1.22872961\n",
      "Trained batch 660 batch loss 1.33896685 epoch total loss 1.22889674\n",
      "Trained batch 661 batch loss 1.24965346 epoch total loss 1.22892809\n",
      "Trained batch 662 batch loss 1.00338221 epoch total loss 1.22858739\n",
      "Trained batch 663 batch loss 1.29297853 epoch total loss 1.22868443\n",
      "Trained batch 664 batch loss 1.22866964 epoch total loss 1.22868443\n",
      "Trained batch 665 batch loss 1.2467438 epoch total loss 1.22871172\n",
      "Trained batch 666 batch loss 1.24382281 epoch total loss 1.22873437\n",
      "Trained batch 667 batch loss 1.44747698 epoch total loss 1.22906232\n",
      "Trained batch 668 batch loss 1.41713929 epoch total loss 1.22934377\n",
      "Trained batch 669 batch loss 1.19239783 epoch total loss 1.22928858\n",
      "Trained batch 670 batch loss 1.27849388 epoch total loss 1.22936201\n",
      "Trained batch 671 batch loss 1.12062383 epoch total loss 1.22919989\n",
      "Trained batch 672 batch loss 1.05635834 epoch total loss 1.22894263\n",
      "Trained batch 673 batch loss 1.22873306 epoch total loss 1.22894239\n",
      "Trained batch 674 batch loss 1.32745266 epoch total loss 1.22908854\n",
      "Trained batch 675 batch loss 1.2765168 epoch total loss 1.22915876\n",
      "Trained batch 676 batch loss 1.28585732 epoch total loss 1.22924268\n",
      "Trained batch 677 batch loss 1.14481115 epoch total loss 1.22911799\n",
      "Trained batch 678 batch loss 1.17045355 epoch total loss 1.22903144\n",
      "Trained batch 679 batch loss 1.04328418 epoch total loss 1.22875786\n",
      "Trained batch 680 batch loss 1.2786746 epoch total loss 1.22883129\n",
      "Trained batch 681 batch loss 1.20434391 epoch total loss 1.22879529\n",
      "Trained batch 682 batch loss 1.43073905 epoch total loss 1.22909141\n",
      "Trained batch 683 batch loss 1.30785155 epoch total loss 1.2292068\n",
      "Trained batch 684 batch loss 1.34009993 epoch total loss 1.22936893\n",
      "Trained batch 685 batch loss 1.23192239 epoch total loss 1.22937262\n",
      "Trained batch 686 batch loss 1.2683866 epoch total loss 1.22942948\n",
      "Trained batch 687 batch loss 1.27371728 epoch total loss 1.22949398\n",
      "Trained batch 688 batch loss 1.29050338 epoch total loss 1.22958267\n",
      "Trained batch 689 batch loss 1.25774121 epoch total loss 1.22962356\n",
      "Trained batch 690 batch loss 1.32311201 epoch total loss 1.2297591\n",
      "Trained batch 691 batch loss 1.28378546 epoch total loss 1.2298373\n",
      "Trained batch 692 batch loss 1.27262616 epoch total loss 1.22989917\n",
      "Trained batch 693 batch loss 1.27498746 epoch total loss 1.22996414\n",
      "Trained batch 694 batch loss 1.22880495 epoch total loss 1.22996247\n",
      "Trained batch 695 batch loss 1.11053133 epoch total loss 1.22979069\n",
      "Trained batch 696 batch loss 1.12744164 epoch total loss 1.22964358\n",
      "Trained batch 697 batch loss 1.08171535 epoch total loss 1.22943139\n",
      "Trained batch 698 batch loss 1.16099358 epoch total loss 1.2293334\n",
      "Trained batch 699 batch loss 1.16566 epoch total loss 1.22924232\n",
      "Trained batch 700 batch loss 1.2392329 epoch total loss 1.22925663\n",
      "Trained batch 701 batch loss 1.30060112 epoch total loss 1.22935832\n",
      "Trained batch 702 batch loss 1.31385708 epoch total loss 1.22947872\n",
      "Trained batch 703 batch loss 1.31831753 epoch total loss 1.22960508\n",
      "Trained batch 704 batch loss 1.22955441 epoch total loss 1.22960496\n",
      "Trained batch 705 batch loss 1.18678331 epoch total loss 1.22954416\n",
      "Trained batch 706 batch loss 1.0740788 epoch total loss 1.22932398\n",
      "Trained batch 707 batch loss 1.08022022 epoch total loss 1.2291131\n",
      "Trained batch 708 batch loss 1.26632488 epoch total loss 1.22916567\n",
      "Trained batch 709 batch loss 1.31409383 epoch total loss 1.22928536\n",
      "Trained batch 710 batch loss 1.2362802 epoch total loss 1.22929525\n",
      "Trained batch 711 batch loss 1.19297743 epoch total loss 1.22924423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 712 batch loss 1.21959138 epoch total loss 1.22923064\n",
      "Trained batch 713 batch loss 1.19000328 epoch total loss 1.22917557\n",
      "Trained batch 714 batch loss 1.21872473 epoch total loss 1.22916102\n",
      "Trained batch 715 batch loss 1.12163961 epoch total loss 1.2290107\n",
      "Trained batch 716 batch loss 0.958286047 epoch total loss 1.22863257\n",
      "Trained batch 717 batch loss 1.01429975 epoch total loss 1.22833359\n",
      "Trained batch 718 batch loss 1.27799714 epoch total loss 1.22840285\n",
      "Trained batch 719 batch loss 1.27147758 epoch total loss 1.2284627\n",
      "Trained batch 720 batch loss 1.30397153 epoch total loss 1.2285676\n",
      "Trained batch 721 batch loss 1.41211331 epoch total loss 1.22882211\n",
      "Trained batch 722 batch loss 1.30435443 epoch total loss 1.22892678\n",
      "Trained batch 723 batch loss 1.17601442 epoch total loss 1.22885358\n",
      "Trained batch 724 batch loss 1.08211672 epoch total loss 1.22865093\n",
      "Trained batch 725 batch loss 1.17390299 epoch total loss 1.22857535\n",
      "Trained batch 726 batch loss 1.30042267 epoch total loss 1.22867429\n",
      "Trained batch 727 batch loss 1.17201006 epoch total loss 1.22859645\n",
      "Trained batch 728 batch loss 1.22144151 epoch total loss 1.22858655\n",
      "Trained batch 729 batch loss 1.37389874 epoch total loss 1.22878587\n",
      "Trained batch 730 batch loss 1.25469875 epoch total loss 1.2288214\n",
      "Trained batch 731 batch loss 1.29440355 epoch total loss 1.22891116\n",
      "Trained batch 732 batch loss 1.36983705 epoch total loss 1.22910368\n",
      "Trained batch 733 batch loss 1.38762379 epoch total loss 1.22931993\n",
      "Trained batch 734 batch loss 1.34784269 epoch total loss 1.22948134\n",
      "Trained batch 735 batch loss 1.40516734 epoch total loss 1.22972035\n",
      "Trained batch 736 batch loss 1.29789054 epoch total loss 1.22981298\n",
      "Trained batch 737 batch loss 1.22899175 epoch total loss 1.22981191\n",
      "Trained batch 738 batch loss 1.21704817 epoch total loss 1.22979462\n",
      "Trained batch 739 batch loss 1.25641012 epoch total loss 1.22983062\n",
      "Trained batch 740 batch loss 1.05168521 epoch total loss 1.22958994\n",
      "Trained batch 741 batch loss 1.0802865 epoch total loss 1.22938836\n",
      "Trained batch 742 batch loss 1.17528415 epoch total loss 1.22931552\n",
      "Trained batch 743 batch loss 1.15307617 epoch total loss 1.22921288\n",
      "Trained batch 744 batch loss 1.2022022 epoch total loss 1.22917664\n",
      "Trained batch 745 batch loss 1.28048849 epoch total loss 1.22924554\n",
      "Trained batch 746 batch loss 1.34918475 epoch total loss 1.22940624\n",
      "Trained batch 747 batch loss 1.26573062 epoch total loss 1.22945487\n",
      "Trained batch 748 batch loss 1.17081702 epoch total loss 1.22937655\n",
      "Trained batch 749 batch loss 1.17323256 epoch total loss 1.22930157\n",
      "Trained batch 750 batch loss 1.21747577 epoch total loss 1.22928584\n",
      "Trained batch 751 batch loss 1.31211281 epoch total loss 1.2293961\n",
      "Trained batch 752 batch loss 1.15147746 epoch total loss 1.22929251\n",
      "Trained batch 753 batch loss 0.946248472 epoch total loss 1.22891665\n",
      "Trained batch 754 batch loss 0.941003084 epoch total loss 1.2285347\n",
      "Trained batch 755 batch loss 1.19668055 epoch total loss 1.2284925\n",
      "Trained batch 756 batch loss 1.27312803 epoch total loss 1.22855151\n",
      "Trained batch 757 batch loss 1.47764051 epoch total loss 1.22888064\n",
      "Trained batch 758 batch loss 1.3970437 epoch total loss 1.22910249\n",
      "Trained batch 759 batch loss 1.46223712 epoch total loss 1.22940958\n",
      "Trained batch 760 batch loss 1.41303086 epoch total loss 1.22965121\n",
      "Trained batch 761 batch loss 1.51003623 epoch total loss 1.23001957\n",
      "Trained batch 762 batch loss 1.17142725 epoch total loss 1.2299428\n",
      "Trained batch 763 batch loss 1.18873036 epoch total loss 1.22988868\n",
      "Trained batch 764 batch loss 1.17038488 epoch total loss 1.22981083\n",
      "Trained batch 765 batch loss 1.2104764 epoch total loss 1.22978556\n",
      "Trained batch 766 batch loss 1.28608441 epoch total loss 1.22985899\n",
      "Trained batch 767 batch loss 1.2717917 epoch total loss 1.22991371\n",
      "Trained batch 768 batch loss 1.25119364 epoch total loss 1.22994149\n",
      "Trained batch 769 batch loss 1.32828236 epoch total loss 1.2300694\n",
      "Trained batch 770 batch loss 1.31736553 epoch total loss 1.23018277\n",
      "Trained batch 771 batch loss 1.39529514 epoch total loss 1.23039699\n",
      "Trained batch 772 batch loss 1.270504 epoch total loss 1.23044896\n",
      "Trained batch 773 batch loss 1.23385823 epoch total loss 1.23045337\n",
      "Trained batch 774 batch loss 1.1327405 epoch total loss 1.23032713\n",
      "Trained batch 775 batch loss 1.32012665 epoch total loss 1.230443\n",
      "Trained batch 776 batch loss 1.25949872 epoch total loss 1.23048043\n",
      "Trained batch 777 batch loss 1.11069775 epoch total loss 1.23032629\n",
      "Trained batch 778 batch loss 1.25282562 epoch total loss 1.23035526\n",
      "Trained batch 779 batch loss 1.19043672 epoch total loss 1.230304\n",
      "Trained batch 780 batch loss 1.14867663 epoch total loss 1.23019934\n",
      "Trained batch 781 batch loss 1.1815598 epoch total loss 1.23013711\n",
      "Trained batch 782 batch loss 1.22451496 epoch total loss 1.23012984\n",
      "Trained batch 783 batch loss 1.4220773 epoch total loss 1.23037493\n",
      "Trained batch 784 batch loss 1.22060132 epoch total loss 1.23036253\n",
      "Trained batch 785 batch loss 1.14267755 epoch total loss 1.23025084\n",
      "Trained batch 786 batch loss 1.10881805 epoch total loss 1.23009634\n",
      "Trained batch 787 batch loss 1.16787601 epoch total loss 1.23001719\n",
      "Trained batch 788 batch loss 1.23376298 epoch total loss 1.23002195\n",
      "Trained batch 789 batch loss 1.21972704 epoch total loss 1.23000896\n",
      "Trained batch 790 batch loss 1.20461071 epoch total loss 1.22997677\n",
      "Trained batch 791 batch loss 1.22020888 epoch total loss 1.22996438\n",
      "Trained batch 792 batch loss 1.1932404 epoch total loss 1.229918\n",
      "Trained batch 793 batch loss 1.14102411 epoch total loss 1.22980595\n",
      "Trained batch 794 batch loss 1.17072344 epoch total loss 1.22973156\n",
      "Trained batch 795 batch loss 1.21609139 epoch total loss 1.22971439\n",
      "Trained batch 796 batch loss 1.12517869 epoch total loss 1.22958302\n",
      "Trained batch 797 batch loss 1.26563847 epoch total loss 1.22962832\n",
      "Trained batch 798 batch loss 1.21721733 epoch total loss 1.22961271\n",
      "Trained batch 799 batch loss 1.17724514 epoch total loss 1.22954714\n",
      "Trained batch 800 batch loss 1.23966479 epoch total loss 1.2295599\n",
      "Trained batch 801 batch loss 1.10678887 epoch total loss 1.2294066\n",
      "Trained batch 802 batch loss 1.12490273 epoch total loss 1.2292763\n",
      "Trained batch 803 batch loss 1.19603753 epoch total loss 1.22923493\n",
      "Trained batch 804 batch loss 1.24426639 epoch total loss 1.22925353\n",
      "Trained batch 805 batch loss 1.17409122 epoch total loss 1.22918499\n",
      "Trained batch 806 batch loss 1.24023342 epoch total loss 1.22919869\n",
      "Trained batch 807 batch loss 1.20306647 epoch total loss 1.22916639\n",
      "Trained batch 808 batch loss 1.2728591 epoch total loss 1.22922051\n",
      "Trained batch 809 batch loss 1.1955843 epoch total loss 1.22917891\n",
      "Trained batch 810 batch loss 1.38639176 epoch total loss 1.22937298\n",
      "Trained batch 811 batch loss 1.19735086 epoch total loss 1.22933352\n",
      "Trained batch 812 batch loss 1.18449557 epoch total loss 1.22927821\n",
      "Trained batch 813 batch loss 1.02385139 epoch total loss 1.2290256\n",
      "Trained batch 814 batch loss 1.11805654 epoch total loss 1.22888923\n",
      "Trained batch 815 batch loss 1.21430838 epoch total loss 1.22887135\n",
      "Trained batch 816 batch loss 1.1374445 epoch total loss 1.22875929\n",
      "Trained batch 817 batch loss 1.07597923 epoch total loss 1.22857237\n",
      "Trained batch 818 batch loss 1.26014793 epoch total loss 1.22861087\n",
      "Trained batch 819 batch loss 1.28865671 epoch total loss 1.22868419\n",
      "Trained batch 820 batch loss 1.31177592 epoch total loss 1.22878551\n",
      "Trained batch 821 batch loss 1.28730154 epoch total loss 1.2288568\n",
      "Trained batch 822 batch loss 1.38124847 epoch total loss 1.22904217\n",
      "Trained batch 823 batch loss 1.40823221 epoch total loss 1.22925985\n",
      "Trained batch 824 batch loss 1.35368788 epoch total loss 1.22941089\n",
      "Trained batch 825 batch loss 1.11776447 epoch total loss 1.22927547\n",
      "Trained batch 826 batch loss 0.998184562 epoch total loss 1.22899568\n",
      "Trained batch 827 batch loss 1.04288554 epoch total loss 1.22877073\n",
      "Trained batch 828 batch loss 1.26212633 epoch total loss 1.22881103\n",
      "Trained batch 829 batch loss 1.41638184 epoch total loss 1.22903728\n",
      "Trained batch 830 batch loss 1.31485164 epoch total loss 1.22914064\n",
      "Trained batch 831 batch loss 1.36338222 epoch total loss 1.22930229\n",
      "Trained batch 832 batch loss 1.1186173 epoch total loss 1.22916913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 833 batch loss 1.20812833 epoch total loss 1.22914398\n",
      "Trained batch 834 batch loss 1.28064919 epoch total loss 1.22920573\n",
      "Trained batch 835 batch loss 1.29725695 epoch total loss 1.22928727\n",
      "Trained batch 836 batch loss 1.26819205 epoch total loss 1.22933376\n",
      "Trained batch 837 batch loss 1.2291162 epoch total loss 1.22933352\n",
      "Trained batch 838 batch loss 1.30548501 epoch total loss 1.22942448\n",
      "Trained batch 839 batch loss 1.21187258 epoch total loss 1.22940361\n",
      "Trained batch 840 batch loss 1.20232773 epoch total loss 1.22937131\n",
      "Trained batch 841 batch loss 1.08729911 epoch total loss 1.22920227\n",
      "Trained batch 842 batch loss 1.16346562 epoch total loss 1.22912419\n",
      "Trained batch 843 batch loss 1.33948731 epoch total loss 1.2292552\n",
      "Trained batch 844 batch loss 1.15611231 epoch total loss 1.22916853\n",
      "Trained batch 845 batch loss 0.965926886 epoch total loss 1.22885704\n",
      "Trained batch 846 batch loss 1.09928823 epoch total loss 1.22870374\n",
      "Trained batch 847 batch loss 1.28071642 epoch total loss 1.22876525\n",
      "Trained batch 848 batch loss 1.17604911 epoch total loss 1.22870302\n",
      "Trained batch 849 batch loss 1.08450341 epoch total loss 1.22853315\n",
      "Trained batch 850 batch loss 1.14209342 epoch total loss 1.22843146\n",
      "Trained batch 851 batch loss 1.03178275 epoch total loss 1.22820032\n",
      "Trained batch 852 batch loss 1.09585071 epoch total loss 1.22804499\n",
      "Trained batch 853 batch loss 1.0569948 epoch total loss 1.22784448\n",
      "Trained batch 854 batch loss 1.19506085 epoch total loss 1.22780609\n",
      "Trained batch 855 batch loss 1.0421524 epoch total loss 1.22758889\n",
      "Trained batch 856 batch loss 1.17020369 epoch total loss 1.22752178\n",
      "Trained batch 857 batch loss 1.16466939 epoch total loss 1.22744846\n",
      "Trained batch 858 batch loss 1.0835892 epoch total loss 1.22728086\n",
      "Trained batch 859 batch loss 1.24119854 epoch total loss 1.22729707\n",
      "Trained batch 860 batch loss 1.31169188 epoch total loss 1.22739518\n",
      "Trained batch 861 batch loss 1.24336433 epoch total loss 1.22741377\n",
      "Trained batch 862 batch loss 1.30623174 epoch total loss 1.22750521\n",
      "Trained batch 863 batch loss 1.19605708 epoch total loss 1.22746873\n",
      "Trained batch 864 batch loss 1.31518459 epoch total loss 1.2275703\n",
      "Trained batch 865 batch loss 1.19359314 epoch total loss 1.22753108\n",
      "Trained batch 866 batch loss 1.18832278 epoch total loss 1.22748578\n",
      "Trained batch 867 batch loss 1.16188169 epoch total loss 1.22741008\n",
      "Trained batch 868 batch loss 1.24162734 epoch total loss 1.22742641\n",
      "Trained batch 869 batch loss 1.37768984 epoch total loss 1.22759938\n",
      "Trained batch 870 batch loss 1.20556927 epoch total loss 1.22757399\n",
      "Trained batch 871 batch loss 1.05625582 epoch total loss 1.2273773\n",
      "Trained batch 872 batch loss 1.1977222 epoch total loss 1.22734332\n",
      "Trained batch 873 batch loss 1.30161643 epoch total loss 1.22742844\n",
      "Trained batch 874 batch loss 1.31088841 epoch total loss 1.22752392\n",
      "Trained batch 875 batch loss 1.25581193 epoch total loss 1.22755635\n",
      "Trained batch 876 batch loss 1.22410893 epoch total loss 1.22755241\n",
      "Trained batch 877 batch loss 1.29591548 epoch total loss 1.22763038\n",
      "Trained batch 878 batch loss 1.23801231 epoch total loss 1.22764218\n",
      "Trained batch 879 batch loss 1.27283359 epoch total loss 1.22769368\n",
      "Trained batch 880 batch loss 1.24179208 epoch total loss 1.22770965\n",
      "Trained batch 881 batch loss 1.14673352 epoch total loss 1.22761774\n",
      "Trained batch 882 batch loss 1.20102191 epoch total loss 1.2275877\n",
      "Trained batch 883 batch loss 1.19201458 epoch total loss 1.22754741\n",
      "Trained batch 884 batch loss 1.24559593 epoch total loss 1.22756779\n",
      "Trained batch 885 batch loss 1.26163852 epoch total loss 1.22760618\n",
      "Trained batch 886 batch loss 1.26625657 epoch total loss 1.22764981\n",
      "Trained batch 887 batch loss 1.27839947 epoch total loss 1.22770715\n",
      "Trained batch 888 batch loss 1.24309242 epoch total loss 1.22772431\n",
      "Trained batch 889 batch loss 1.25768661 epoch total loss 1.22775805\n",
      "Trained batch 890 batch loss 1.22022307 epoch total loss 1.22774959\n",
      "Trained batch 891 batch loss 1.28504133 epoch total loss 1.22781384\n",
      "Trained batch 892 batch loss 1.40335071 epoch total loss 1.22801065\n",
      "Trained batch 893 batch loss 1.22815633 epoch total loss 1.22801077\n",
      "Trained batch 894 batch loss 1.32224822 epoch total loss 1.22811627\n",
      "Trained batch 895 batch loss 1.34509861 epoch total loss 1.22824693\n",
      "Trained batch 896 batch loss 1.17023814 epoch total loss 1.2281822\n",
      "Trained batch 897 batch loss 1.14306343 epoch total loss 1.22808731\n",
      "Trained batch 898 batch loss 1.20555019 epoch total loss 1.22806227\n",
      "Trained batch 899 batch loss 1.36253333 epoch total loss 1.22821188\n",
      "Trained batch 900 batch loss 1.40047264 epoch total loss 1.22840333\n",
      "Trained batch 901 batch loss 1.28277111 epoch total loss 1.22846365\n",
      "Trained batch 902 batch loss 1.27288961 epoch total loss 1.22851288\n",
      "Trained batch 903 batch loss 1.19507158 epoch total loss 1.22847593\n",
      "Trained batch 904 batch loss 1.21479511 epoch total loss 1.22846079\n",
      "Trained batch 905 batch loss 1.29020238 epoch total loss 1.22852898\n",
      "Trained batch 906 batch loss 1.39212108 epoch total loss 1.22870946\n",
      "Trained batch 907 batch loss 1.42778325 epoch total loss 1.22892892\n",
      "Trained batch 908 batch loss 1.27318358 epoch total loss 1.22897768\n",
      "Trained batch 909 batch loss 1.18668413 epoch total loss 1.22893107\n",
      "Trained batch 910 batch loss 1.13254917 epoch total loss 1.22882521\n",
      "Trained batch 911 batch loss 1.11876929 epoch total loss 1.22870445\n",
      "Trained batch 912 batch loss 1.08499157 epoch total loss 1.22854686\n",
      "Trained batch 913 batch loss 1.11838794 epoch total loss 1.22842622\n",
      "Trained batch 914 batch loss 1.19802439 epoch total loss 1.22839284\n",
      "Trained batch 915 batch loss 1.134727 epoch total loss 1.22829056\n",
      "Trained batch 916 batch loss 1.2171036 epoch total loss 1.2282784\n",
      "Trained batch 917 batch loss 1.34076858 epoch total loss 1.22840118\n",
      "Trained batch 918 batch loss 1.2802875 epoch total loss 1.22845769\n",
      "Trained batch 919 batch loss 1.40432787 epoch total loss 1.22864902\n",
      "Trained batch 920 batch loss 1.36490834 epoch total loss 1.22879708\n",
      "Trained batch 921 batch loss 1.30419934 epoch total loss 1.22887897\n",
      "Trained batch 922 batch loss 1.22842669 epoch total loss 1.22887838\n",
      "Trained batch 923 batch loss 1.2078048 epoch total loss 1.22885549\n",
      "Trained batch 924 batch loss 1.19016671 epoch total loss 1.22881365\n",
      "Trained batch 925 batch loss 1.30192029 epoch total loss 1.22889268\n",
      "Trained batch 926 batch loss 1.22497356 epoch total loss 1.22888839\n",
      "Trained batch 927 batch loss 1.10118413 epoch total loss 1.22875071\n",
      "Trained batch 928 batch loss 1.21271014 epoch total loss 1.22873342\n",
      "Trained batch 929 batch loss 1.12625813 epoch total loss 1.22862315\n",
      "Trained batch 930 batch loss 1.12094307 epoch total loss 1.2285074\n",
      "Trained batch 931 batch loss 1.25201297 epoch total loss 1.22853255\n",
      "Trained batch 932 batch loss 1.17552805 epoch total loss 1.22847569\n",
      "Trained batch 933 batch loss 1.30144048 epoch total loss 1.22855377\n",
      "Trained batch 934 batch loss 1.2082442 epoch total loss 1.22853208\n",
      "Trained batch 935 batch loss 1.2380569 epoch total loss 1.22854221\n",
      "Trained batch 936 batch loss 1.26269984 epoch total loss 1.22857881\n",
      "Trained batch 937 batch loss 1.25024056 epoch total loss 1.22860181\n",
      "Trained batch 938 batch loss 1.25711703 epoch total loss 1.22863221\n",
      "Trained batch 939 batch loss 1.11084557 epoch total loss 1.2285068\n",
      "Trained batch 940 batch loss 1.09807575 epoch total loss 1.22836792\n",
      "Trained batch 941 batch loss 1.18160856 epoch total loss 1.22831833\n",
      "Trained batch 942 batch loss 1.10471582 epoch total loss 1.22818708\n",
      "Trained batch 943 batch loss 1.11408901 epoch total loss 1.22806621\n",
      "Trained batch 944 batch loss 1.19804287 epoch total loss 1.22803438\n",
      "Trained batch 945 batch loss 1.19321132 epoch total loss 1.22799754\n",
      "Trained batch 946 batch loss 1.12977803 epoch total loss 1.22789371\n",
      "Trained batch 947 batch loss 1.06943679 epoch total loss 1.22772634\n",
      "Trained batch 948 batch loss 1.15918875 epoch total loss 1.2276541\n",
      "Trained batch 949 batch loss 1.09188843 epoch total loss 1.22751105\n",
      "Trained batch 950 batch loss 1.21116376 epoch total loss 1.22749388\n",
      "Trained batch 951 batch loss 1.25137496 epoch total loss 1.22751892\n",
      "Trained batch 952 batch loss 1.2511735 epoch total loss 1.22754383\n",
      "Trained batch 953 batch loss 1.26409948 epoch total loss 1.22758222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 954 batch loss 1.21496296 epoch total loss 1.22756898\n",
      "Trained batch 955 batch loss 1.2698065 epoch total loss 1.22761321\n",
      "Trained batch 956 batch loss 1.35311282 epoch total loss 1.22774446\n",
      "Trained batch 957 batch loss 1.16087532 epoch total loss 1.2276746\n",
      "Trained batch 958 batch loss 1.24019301 epoch total loss 1.22768772\n",
      "Trained batch 959 batch loss 1.26128113 epoch total loss 1.22772276\n",
      "Trained batch 960 batch loss 1.22019911 epoch total loss 1.2277149\n",
      "Trained batch 961 batch loss 1.26818287 epoch total loss 1.22775698\n",
      "Trained batch 962 batch loss 1.12412167 epoch total loss 1.22764933\n",
      "Trained batch 963 batch loss 1.29339921 epoch total loss 1.22771764\n",
      "Trained batch 964 batch loss 1.04631758 epoch total loss 1.22752941\n",
      "Trained batch 965 batch loss 1.12180567 epoch total loss 1.22741985\n",
      "Trained batch 966 batch loss 1.15215409 epoch total loss 1.22734189\n",
      "Trained batch 967 batch loss 1.31876922 epoch total loss 1.22743642\n",
      "Trained batch 968 batch loss 1.36801529 epoch total loss 1.22758174\n",
      "Trained batch 969 batch loss 1.36897969 epoch total loss 1.22772765\n",
      "Trained batch 970 batch loss 1.45299065 epoch total loss 1.22795987\n",
      "Trained batch 971 batch loss 1.25172234 epoch total loss 1.22798431\n",
      "Trained batch 972 batch loss 1.21624696 epoch total loss 1.22797227\n",
      "Trained batch 973 batch loss 1.07864118 epoch total loss 1.22781873\n",
      "Trained batch 974 batch loss 0.994236112 epoch total loss 1.22757888\n",
      "Trained batch 975 batch loss 0.963983953 epoch total loss 1.22730851\n",
      "Trained batch 976 batch loss 1.03448892 epoch total loss 1.2271111\n",
      "Trained batch 977 batch loss 1.20178 epoch total loss 1.22708511\n",
      "Trained batch 978 batch loss 1.21272993 epoch total loss 1.22707045\n",
      "Trained batch 979 batch loss 1.12338328 epoch total loss 1.22696459\n",
      "Trained batch 980 batch loss 1.26800096 epoch total loss 1.22700644\n",
      "Trained batch 981 batch loss 1.17026353 epoch total loss 1.22694862\n",
      "Trained batch 982 batch loss 1.10194767 epoch total loss 1.2268213\n",
      "Trained batch 983 batch loss 1.09610534 epoch total loss 1.22668827\n",
      "Trained batch 984 batch loss 1.18318343 epoch total loss 1.22664416\n",
      "Trained batch 985 batch loss 1.16750836 epoch total loss 1.22658408\n",
      "Trained batch 986 batch loss 1.22841644 epoch total loss 1.22658587\n",
      "Trained batch 987 batch loss 1.21012878 epoch total loss 1.22656918\n",
      "Trained batch 988 batch loss 1.1023922 epoch total loss 1.22644353\n",
      "Trained batch 989 batch loss 1.22297513 epoch total loss 1.22644007\n",
      "Trained batch 990 batch loss 1.18802226 epoch total loss 1.22640121\n",
      "Trained batch 991 batch loss 1.21663523 epoch total loss 1.22639143\n",
      "Trained batch 992 batch loss 1.18726861 epoch total loss 1.22635198\n",
      "Trained batch 993 batch loss 1.24797094 epoch total loss 1.22637367\n",
      "Trained batch 994 batch loss 1.15485179 epoch total loss 1.22630179\n",
      "Trained batch 995 batch loss 1.05431211 epoch total loss 1.22612894\n",
      "Trained batch 996 batch loss 1.15876389 epoch total loss 1.22606134\n",
      "Trained batch 997 batch loss 1.20332503 epoch total loss 1.22603858\n",
      "Trained batch 998 batch loss 1.23195052 epoch total loss 1.22604454\n",
      "Trained batch 999 batch loss 1.28253078 epoch total loss 1.22610092\n",
      "Trained batch 1000 batch loss 1.19346333 epoch total loss 1.22606838\n",
      "Trained batch 1001 batch loss 1.22996593 epoch total loss 1.22607231\n",
      "Trained batch 1002 batch loss 1.27951694 epoch total loss 1.2261256\n",
      "Trained batch 1003 batch loss 1.28805375 epoch total loss 1.22618735\n",
      "Trained batch 1004 batch loss 1.21632063 epoch total loss 1.22617757\n",
      "Trained batch 1005 batch loss 1.21540427 epoch total loss 1.22616684\n",
      "Trained batch 1006 batch loss 1.3366375 epoch total loss 1.22627676\n",
      "Trained batch 1007 batch loss 1.23375225 epoch total loss 1.22628415\n",
      "Trained batch 1008 batch loss 1.18166804 epoch total loss 1.22623992\n",
      "Trained batch 1009 batch loss 1.28556788 epoch total loss 1.22629869\n",
      "Trained batch 1010 batch loss 1.26541591 epoch total loss 1.22633731\n",
      "Trained batch 1011 batch loss 1.2105931 epoch total loss 1.2263217\n",
      "Trained batch 1012 batch loss 1.14793086 epoch total loss 1.22624433\n",
      "Trained batch 1013 batch loss 1.13588214 epoch total loss 1.22615504\n",
      "Trained batch 1014 batch loss 1.07112169 epoch total loss 1.22600222\n",
      "Trained batch 1015 batch loss 1.07598937 epoch total loss 1.22585452\n",
      "Trained batch 1016 batch loss 1.14242399 epoch total loss 1.22577238\n",
      "Trained batch 1017 batch loss 1.20499778 epoch total loss 1.225752\n",
      "Trained batch 1018 batch loss 1.05790663 epoch total loss 1.22558701\n",
      "Trained batch 1019 batch loss 1.12281442 epoch total loss 1.22548616\n",
      "Trained batch 1020 batch loss 1.04138422 epoch total loss 1.22530568\n",
      "Trained batch 1021 batch loss 1.08864045 epoch total loss 1.2251718\n",
      "Trained batch 1022 batch loss 1.10260403 epoch total loss 1.22505188\n",
      "Trained batch 1023 batch loss 1.06337643 epoch total loss 1.22489381\n",
      "Trained batch 1024 batch loss 1.30040658 epoch total loss 1.2249676\n",
      "Trained batch 1025 batch loss 1.2088846 epoch total loss 1.22495186\n",
      "Trained batch 1026 batch loss 1.21238685 epoch total loss 1.2249397\n",
      "Trained batch 1027 batch loss 1.15542734 epoch total loss 1.22487199\n",
      "Trained batch 1028 batch loss 1.35109496 epoch total loss 1.22499466\n",
      "Trained batch 1029 batch loss 1.21692526 epoch total loss 1.22498691\n",
      "Trained batch 1030 batch loss 1.28197694 epoch total loss 1.22504222\n",
      "Trained batch 1031 batch loss 1.27650762 epoch total loss 1.22509205\n",
      "Trained batch 1032 batch loss 1.17188287 epoch total loss 1.22504056\n",
      "Trained batch 1033 batch loss 1.12644553 epoch total loss 1.22494507\n",
      "Trained batch 1034 batch loss 1.19152832 epoch total loss 1.22491276\n",
      "Trained batch 1035 batch loss 1.25911009 epoch total loss 1.2249459\n",
      "Trained batch 1036 batch loss 1.30682397 epoch total loss 1.22502494\n",
      "Trained batch 1037 batch loss 1.30822968 epoch total loss 1.22510517\n",
      "Trained batch 1038 batch loss 1.28715098 epoch total loss 1.22516489\n",
      "Trained batch 1039 batch loss 1.14862823 epoch total loss 1.22509134\n",
      "Trained batch 1040 batch loss 1.21134269 epoch total loss 1.22507811\n",
      "Trained batch 1041 batch loss 1.31748748 epoch total loss 1.2251668\n",
      "Trained batch 1042 batch loss 1.16519964 epoch total loss 1.22510922\n",
      "Trained batch 1043 batch loss 1.21794784 epoch total loss 1.22510231\n",
      "Trained batch 1044 batch loss 1.09826267 epoch total loss 1.22498083\n",
      "Trained batch 1045 batch loss 1.05725646 epoch total loss 1.22482038\n",
      "Trained batch 1046 batch loss 1.03727794 epoch total loss 1.22464097\n",
      "Trained batch 1047 batch loss 1.375862 epoch total loss 1.22478545\n",
      "Trained batch 1048 batch loss 1.3441962 epoch total loss 1.22489941\n",
      "Trained batch 1049 batch loss 1.20308 epoch total loss 1.22487867\n",
      "Trained batch 1050 batch loss 1.16172552 epoch total loss 1.22481847\n",
      "Trained batch 1051 batch loss 1.17745459 epoch total loss 1.22477353\n",
      "Trained batch 1052 batch loss 1.22680497 epoch total loss 1.22477543\n",
      "Trained batch 1053 batch loss 1.1228497 epoch total loss 1.22467864\n",
      "Trained batch 1054 batch loss 1.12654448 epoch total loss 1.22458553\n",
      "Trained batch 1055 batch loss 1.21633625 epoch total loss 1.22457767\n",
      "Trained batch 1056 batch loss 1.24838686 epoch total loss 1.2246002\n",
      "Trained batch 1057 batch loss 1.22395325 epoch total loss 1.22459972\n",
      "Trained batch 1058 batch loss 1.19987845 epoch total loss 1.22457623\n",
      "Trained batch 1059 batch loss 1.30251169 epoch total loss 1.22464979\n",
      "Trained batch 1060 batch loss 1.17634237 epoch total loss 1.22460425\n",
      "Trained batch 1061 batch loss 1.15116704 epoch total loss 1.22453511\n",
      "Trained batch 1062 batch loss 1.06462693 epoch total loss 1.22438443\n",
      "Trained batch 1063 batch loss 1.2248913 epoch total loss 1.2243849\n",
      "Trained batch 1064 batch loss 1.24859715 epoch total loss 1.22440767\n",
      "Trained batch 1065 batch loss 1.22077143 epoch total loss 1.22440434\n",
      "Trained batch 1066 batch loss 1.23627758 epoch total loss 1.22441554\n",
      "Trained batch 1067 batch loss 1.27622557 epoch total loss 1.22446406\n",
      "Trained batch 1068 batch loss 1.30731416 epoch total loss 1.22454166\n",
      "Trained batch 1069 batch loss 1.18970418 epoch total loss 1.22450912\n",
      "Trained batch 1070 batch loss 1.22495055 epoch total loss 1.2245096\n",
      "Trained batch 1071 batch loss 1.13227129 epoch total loss 1.22442353\n",
      "Trained batch 1072 batch loss 1.15956426 epoch total loss 1.22436297\n",
      "Trained batch 1073 batch loss 1.13659942 epoch total loss 1.22428119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1074 batch loss 1.2627182 epoch total loss 1.22431695\n",
      "Trained batch 1075 batch loss 1.34237337 epoch total loss 1.22442675\n",
      "Trained batch 1076 batch loss 1.18685126 epoch total loss 1.22439194\n",
      "Trained batch 1077 batch loss 1.28236294 epoch total loss 1.2244457\n",
      "Trained batch 1078 batch loss 1.17487156 epoch total loss 1.22439981\n",
      "Trained batch 1079 batch loss 1.25779831 epoch total loss 1.22443068\n",
      "Trained batch 1080 batch loss 1.2802881 epoch total loss 1.22448242\n",
      "Trained batch 1081 batch loss 1.17180669 epoch total loss 1.22443366\n",
      "Trained batch 1082 batch loss 1.17641687 epoch total loss 1.22438931\n",
      "Trained batch 1083 batch loss 1.1440444 epoch total loss 1.22431505\n",
      "Trained batch 1084 batch loss 1.19594908 epoch total loss 1.22428894\n",
      "Trained batch 1085 batch loss 1.20213032 epoch total loss 1.22426844\n",
      "Trained batch 1086 batch loss 1.15555656 epoch total loss 1.22420514\n",
      "Trained batch 1087 batch loss 1.07411849 epoch total loss 1.22406709\n",
      "Trained batch 1088 batch loss 1.11057377 epoch total loss 1.22396278\n",
      "Trained batch 1089 batch loss 1.11428976 epoch total loss 1.22386205\n",
      "Trained batch 1090 batch loss 0.974156082 epoch total loss 1.22363293\n",
      "Trained batch 1091 batch loss 1.16486931 epoch total loss 1.22357905\n",
      "Trained batch 1092 batch loss 1.06684315 epoch total loss 1.22343564\n",
      "Trained batch 1093 batch loss 1.08418941 epoch total loss 1.22330821\n",
      "Trained batch 1094 batch loss 1.12936842 epoch total loss 1.22322237\n",
      "Trained batch 1095 batch loss 1.12182236 epoch total loss 1.22312987\n",
      "Trained batch 1096 batch loss 1.11250615 epoch total loss 1.2230289\n",
      "Trained batch 1097 batch loss 1.03062105 epoch total loss 1.22285354\n",
      "Trained batch 1098 batch loss 1.39978075 epoch total loss 1.22301471\n",
      "Trained batch 1099 batch loss 1.38755381 epoch total loss 1.22316444\n",
      "Trained batch 1100 batch loss 1.12179482 epoch total loss 1.22307229\n",
      "Trained batch 1101 batch loss 1.25548601 epoch total loss 1.22310174\n",
      "Trained batch 1102 batch loss 1.42883372 epoch total loss 1.22328842\n",
      "Trained batch 1103 batch loss 1.24106336 epoch total loss 1.22330451\n",
      "Trained batch 1104 batch loss 1.30912876 epoch total loss 1.22338223\n",
      "Trained batch 1105 batch loss 1.2046684 epoch total loss 1.22336531\n",
      "Trained batch 1106 batch loss 1.15157819 epoch total loss 1.22330046\n",
      "Trained batch 1107 batch loss 1.23075485 epoch total loss 1.22330713\n",
      "Trained batch 1108 batch loss 1.22426462 epoch total loss 1.22330797\n",
      "Trained batch 1109 batch loss 1.2497474 epoch total loss 1.22333193\n",
      "Trained batch 1110 batch loss 1.22182286 epoch total loss 1.2233305\n",
      "Trained batch 1111 batch loss 1.1974932 epoch total loss 1.22330725\n",
      "Trained batch 1112 batch loss 1.26245439 epoch total loss 1.22334242\n",
      "Trained batch 1113 batch loss 1.18322325 epoch total loss 1.22330642\n",
      "Trained batch 1114 batch loss 1.16945267 epoch total loss 1.22325802\n",
      "Trained batch 1115 batch loss 1.33624661 epoch total loss 1.22335947\n",
      "Trained batch 1116 batch loss 1.35028601 epoch total loss 1.22347319\n",
      "Trained batch 1117 batch loss 1.3960315 epoch total loss 1.22362769\n",
      "Trained batch 1118 batch loss 1.32143474 epoch total loss 1.22371519\n",
      "Trained batch 1119 batch loss 1.39545679 epoch total loss 1.22386861\n",
      "Trained batch 1120 batch loss 1.21815777 epoch total loss 1.2238636\n",
      "Trained batch 1121 batch loss 1.30322623 epoch total loss 1.22393429\n",
      "Trained batch 1122 batch loss 1.23652363 epoch total loss 1.22394562\n",
      "Trained batch 1123 batch loss 1.34028721 epoch total loss 1.22404921\n",
      "Trained batch 1124 batch loss 1.20327723 epoch total loss 1.22403073\n",
      "Trained batch 1125 batch loss 1.16083491 epoch total loss 1.22397459\n",
      "Trained batch 1126 batch loss 1.10810637 epoch total loss 1.22387171\n",
      "Trained batch 1127 batch loss 1.1661222 epoch total loss 1.22382057\n",
      "Trained batch 1128 batch loss 1.11860764 epoch total loss 1.22372735\n",
      "Trained batch 1129 batch loss 1.2629571 epoch total loss 1.22376204\n",
      "Trained batch 1130 batch loss 1.19739842 epoch total loss 1.22373867\n",
      "Trained batch 1131 batch loss 1.15074301 epoch total loss 1.22367418\n",
      "Trained batch 1132 batch loss 1.13071525 epoch total loss 1.22359204\n",
      "Trained batch 1133 batch loss 1.16433358 epoch total loss 1.22353971\n",
      "Trained batch 1134 batch loss 1.23555923 epoch total loss 1.22355032\n",
      "Trained batch 1135 batch loss 1.23814273 epoch total loss 1.22356319\n",
      "Trained batch 1136 batch loss 1.31457758 epoch total loss 1.2236433\n",
      "Trained batch 1137 batch loss 1.18162584 epoch total loss 1.22360635\n",
      "Trained batch 1138 batch loss 1.32387185 epoch total loss 1.22369444\n",
      "Trained batch 1139 batch loss 1.37762833 epoch total loss 1.22382975\n",
      "Trained batch 1140 batch loss 1.22266197 epoch total loss 1.22382867\n",
      "Trained batch 1141 batch loss 1.34346581 epoch total loss 1.22393358\n",
      "Trained batch 1142 batch loss 1.41418135 epoch total loss 1.22410011\n",
      "Trained batch 1143 batch loss 1.16047943 epoch total loss 1.22404456\n",
      "Trained batch 1144 batch loss 1.15466654 epoch total loss 1.22398388\n",
      "Trained batch 1145 batch loss 1.18287647 epoch total loss 1.223948\n",
      "Trained batch 1146 batch loss 1.1348089 epoch total loss 1.22387016\n",
      "Trained batch 1147 batch loss 1.11565924 epoch total loss 1.22377574\n",
      "Trained batch 1148 batch loss 1.09439421 epoch total loss 1.22366297\n",
      "Trained batch 1149 batch loss 1.29770088 epoch total loss 1.22372746\n",
      "Trained batch 1150 batch loss 1.30583096 epoch total loss 1.22379887\n",
      "Trained batch 1151 batch loss 1.28004897 epoch total loss 1.22384763\n",
      "Trained batch 1152 batch loss 1.32911444 epoch total loss 1.22393906\n",
      "Trained batch 1153 batch loss 1.32114935 epoch total loss 1.22402334\n",
      "Trained batch 1154 batch loss 1.26176596 epoch total loss 1.22405601\n",
      "Trained batch 1155 batch loss 1.19121683 epoch total loss 1.22402751\n",
      "Trained batch 1156 batch loss 1.18329644 epoch total loss 1.22399235\n",
      "Trained batch 1157 batch loss 1.21136427 epoch total loss 1.22398138\n",
      "Trained batch 1158 batch loss 1.14105046 epoch total loss 1.22390974\n",
      "Trained batch 1159 batch loss 1.28406787 epoch total loss 1.22396159\n",
      "Trained batch 1160 batch loss 1.23394775 epoch total loss 1.22397017\n",
      "Trained batch 1161 batch loss 1.21129894 epoch total loss 1.22395933\n",
      "Trained batch 1162 batch loss 1.25019 epoch total loss 1.22398186\n",
      "Trained batch 1163 batch loss 1.13600969 epoch total loss 1.22390628\n",
      "Trained batch 1164 batch loss 1.05752897 epoch total loss 1.22376323\n",
      "Trained batch 1165 batch loss 1.24467921 epoch total loss 1.22378123\n",
      "Trained batch 1166 batch loss 1.11222339 epoch total loss 1.2236855\n",
      "Trained batch 1167 batch loss 1.08541107 epoch total loss 1.22356701\n",
      "Trained batch 1168 batch loss 1.17686284 epoch total loss 1.22352707\n",
      "Trained batch 1169 batch loss 1.10928118 epoch total loss 1.22342932\n",
      "Trained batch 1170 batch loss 1.15901721 epoch total loss 1.22337425\n",
      "Trained batch 1171 batch loss 1.26766491 epoch total loss 1.22341216\n",
      "Trained batch 1172 batch loss 1.28428519 epoch total loss 1.22346413\n",
      "Trained batch 1173 batch loss 1.19251132 epoch total loss 1.22343767\n",
      "Trained batch 1174 batch loss 1.0026468 epoch total loss 1.22324967\n",
      "Trained batch 1175 batch loss 1.10548449 epoch total loss 1.22314942\n",
      "Trained batch 1176 batch loss 1.16879582 epoch total loss 1.22310317\n",
      "Trained batch 1177 batch loss 1.23517156 epoch total loss 1.22311354\n",
      "Trained batch 1178 batch loss 1.13578415 epoch total loss 1.22303939\n",
      "Trained batch 1179 batch loss 1.27353251 epoch total loss 1.22308218\n",
      "Trained batch 1180 batch loss 0.964834929 epoch total loss 1.22286332\n",
      "Trained batch 1181 batch loss 0.934446 epoch total loss 1.22261918\n",
      "Trained batch 1182 batch loss 1.08023643 epoch total loss 1.22249866\n",
      "Trained batch 1183 batch loss 1.24638438 epoch total loss 1.2225188\n",
      "Trained batch 1184 batch loss 1.19461918 epoch total loss 1.2224952\n",
      "Trained batch 1185 batch loss 1.26983845 epoch total loss 1.22253525\n",
      "Trained batch 1186 batch loss 1.19374 epoch total loss 1.22251093\n",
      "Trained batch 1187 batch loss 1.15061545 epoch total loss 1.22245038\n",
      "Trained batch 1188 batch loss 1.11491668 epoch total loss 1.22235978\n",
      "Trained batch 1189 batch loss 1.10788655 epoch total loss 1.22226357\n",
      "Trained batch 1190 batch loss 1.1369704 epoch total loss 1.22219181\n",
      "Trained batch 1191 batch loss 1.2784524 epoch total loss 1.22223914\n",
      "Trained batch 1192 batch loss 1.21275401 epoch total loss 1.22223115\n",
      "Trained batch 1193 batch loss 1.29368353 epoch total loss 1.22229111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1194 batch loss 1.32804966 epoch total loss 1.22237957\n",
      "Trained batch 1195 batch loss 1.32763147 epoch total loss 1.22246766\n",
      "Trained batch 1196 batch loss 1.21785355 epoch total loss 1.22246385\n",
      "Trained batch 1197 batch loss 1.10015059 epoch total loss 1.22236168\n",
      "Trained batch 1198 batch loss 1.29985535 epoch total loss 1.2224263\n",
      "Trained batch 1199 batch loss 1.27039015 epoch total loss 1.22246623\n",
      "Trained batch 1200 batch loss 1.15401435 epoch total loss 1.22240925\n",
      "Trained batch 1201 batch loss 1.04048574 epoch total loss 1.22225785\n",
      "Trained batch 1202 batch loss 1.04356 epoch total loss 1.2221092\n",
      "Trained batch 1203 batch loss 1.13779914 epoch total loss 1.2220391\n",
      "Trained batch 1204 batch loss 1.05556417 epoch total loss 1.22190082\n",
      "Trained batch 1205 batch loss 1.11086774 epoch total loss 1.22180867\n",
      "Trained batch 1206 batch loss 1.16059279 epoch total loss 1.22175789\n",
      "Trained batch 1207 batch loss 1.12416291 epoch total loss 1.22167706\n",
      "Trained batch 1208 batch loss 1.22950339 epoch total loss 1.2216835\n",
      "Trained batch 1209 batch loss 1.08594728 epoch total loss 1.22157121\n",
      "Trained batch 1210 batch loss 1.17820215 epoch total loss 1.22153544\n",
      "Trained batch 1211 batch loss 1.1604836 epoch total loss 1.22148502\n",
      "Trained batch 1212 batch loss 1.3035537 epoch total loss 1.22155273\n",
      "Trained batch 1213 batch loss 1.31310892 epoch total loss 1.22162831\n",
      "Trained batch 1214 batch loss 1.16982067 epoch total loss 1.22158551\n",
      "Trained batch 1215 batch loss 1.12787938 epoch total loss 1.2215085\n",
      "Trained batch 1216 batch loss 1.19917512 epoch total loss 1.22149014\n",
      "Trained batch 1217 batch loss 1.20819557 epoch total loss 1.2214793\n",
      "Trained batch 1218 batch loss 1.20919895 epoch total loss 1.22146916\n",
      "Trained batch 1219 batch loss 1.26029384 epoch total loss 1.22150099\n",
      "Trained batch 1220 batch loss 1.11574972 epoch total loss 1.22141433\n",
      "Trained batch 1221 batch loss 1.16981673 epoch total loss 1.22137201\n",
      "Trained batch 1222 batch loss 1.21462727 epoch total loss 1.22136652\n",
      "Trained batch 1223 batch loss 1.18887436 epoch total loss 1.22134\n",
      "Trained batch 1224 batch loss 1.26649845 epoch total loss 1.22137678\n",
      "Trained batch 1225 batch loss 1.33693838 epoch total loss 1.22147107\n",
      "Trained batch 1226 batch loss 1.19872737 epoch total loss 1.22145259\n",
      "Trained batch 1227 batch loss 1.07921374 epoch total loss 1.2213366\n",
      "Trained batch 1228 batch loss 1.17420113 epoch total loss 1.22129822\n",
      "Trained batch 1229 batch loss 1.25597715 epoch total loss 1.22132647\n",
      "Trained batch 1230 batch loss 1.29608369 epoch total loss 1.22138727\n",
      "Trained batch 1231 batch loss 1.32672763 epoch total loss 1.22147298\n",
      "Trained batch 1232 batch loss 1.32724714 epoch total loss 1.22155881\n",
      "Trained batch 1233 batch loss 1.34582126 epoch total loss 1.22165954\n",
      "Trained batch 1234 batch loss 1.37544644 epoch total loss 1.22178423\n",
      "Trained batch 1235 batch loss 1.0691762 epoch total loss 1.22166073\n",
      "Trained batch 1236 batch loss 1.12420046 epoch total loss 1.22158182\n",
      "Trained batch 1237 batch loss 1.17488456 epoch total loss 1.22154415\n",
      "Trained batch 1238 batch loss 1.39379358 epoch total loss 1.22168326\n",
      "Trained batch 1239 batch loss 1.30227673 epoch total loss 1.22174823\n",
      "Trained batch 1240 batch loss 1.42341185 epoch total loss 1.22191095\n",
      "Trained batch 1241 batch loss 1.35562348 epoch total loss 1.2220186\n",
      "Trained batch 1242 batch loss 1.31158412 epoch total loss 1.22209072\n",
      "Trained batch 1243 batch loss 1.26509666 epoch total loss 1.22212529\n",
      "Trained batch 1244 batch loss 1.24013519 epoch total loss 1.22213984\n",
      "Trained batch 1245 batch loss 1.1904887 epoch total loss 1.22211432\n",
      "Trained batch 1246 batch loss 1.26095057 epoch total loss 1.22214556\n",
      "Trained batch 1247 batch loss 1.23595464 epoch total loss 1.22215664\n",
      "Trained batch 1248 batch loss 1.1765945 epoch total loss 1.22212017\n",
      "Trained batch 1249 batch loss 1.24402368 epoch total loss 1.22213769\n",
      "Trained batch 1250 batch loss 1.16928089 epoch total loss 1.22209537\n",
      "Trained batch 1251 batch loss 1.16527128 epoch total loss 1.22205\n",
      "Trained batch 1252 batch loss 1.07571149 epoch total loss 1.22193313\n",
      "Trained batch 1253 batch loss 1.03945947 epoch total loss 1.22178745\n",
      "Trained batch 1254 batch loss 1.23031747 epoch total loss 1.22179425\n",
      "Trained batch 1255 batch loss 1.14445734 epoch total loss 1.22173262\n",
      "Trained batch 1256 batch loss 1.45001757 epoch total loss 1.22191441\n",
      "Trained batch 1257 batch loss 1.41034615 epoch total loss 1.22206438\n",
      "Trained batch 1258 batch loss 1.34719419 epoch total loss 1.2221638\n",
      "Trained batch 1259 batch loss 1.02565813 epoch total loss 1.22200775\n",
      "Trained batch 1260 batch loss 1.38067174 epoch total loss 1.22213352\n",
      "Trained batch 1261 batch loss 1.29794765 epoch total loss 1.22219372\n",
      "Trained batch 1262 batch loss 1.33890343 epoch total loss 1.22228622\n",
      "Trained batch 1263 batch loss 1.41775656 epoch total loss 1.22244096\n",
      "Trained batch 1264 batch loss 1.37810659 epoch total loss 1.22256398\n",
      "Trained batch 1265 batch loss 1.22190619 epoch total loss 1.22256351\n",
      "Trained batch 1266 batch loss 1.28845859 epoch total loss 1.2226156\n",
      "Trained batch 1267 batch loss 1.19988561 epoch total loss 1.2225976\n",
      "Trained batch 1268 batch loss 1.24552703 epoch total loss 1.2226156\n",
      "Trained batch 1269 batch loss 1.20704377 epoch total loss 1.22260332\n",
      "Trained batch 1270 batch loss 1.30778825 epoch total loss 1.22267044\n",
      "Trained batch 1271 batch loss 1.28512526 epoch total loss 1.22271955\n",
      "Trained batch 1272 batch loss 1.456532 epoch total loss 1.22290337\n",
      "Trained batch 1273 batch loss 1.37558103 epoch total loss 1.2230233\n",
      "Trained batch 1274 batch loss 1.45051 epoch total loss 1.22320187\n",
      "Trained batch 1275 batch loss 1.39480293 epoch total loss 1.22333646\n",
      "Trained batch 1276 batch loss 1.3780663 epoch total loss 1.22345769\n",
      "Trained batch 1277 batch loss 1.30439627 epoch total loss 1.22352111\n",
      "Trained batch 1278 batch loss 1.29082441 epoch total loss 1.2235738\n",
      "Trained batch 1279 batch loss 1.33998215 epoch total loss 1.22366476\n",
      "Trained batch 1280 batch loss 1.27690268 epoch total loss 1.22370636\n",
      "Trained batch 1281 batch loss 1.2658937 epoch total loss 1.22373927\n",
      "Trained batch 1282 batch loss 1.28258228 epoch total loss 1.22378516\n",
      "Trained batch 1283 batch loss 1.28393221 epoch total loss 1.22383201\n",
      "Trained batch 1284 batch loss 1.34229231 epoch total loss 1.22392428\n",
      "Trained batch 1285 batch loss 1.37295234 epoch total loss 1.22404027\n",
      "Trained batch 1286 batch loss 1.45063317 epoch total loss 1.22421646\n",
      "Trained batch 1287 batch loss 1.33196104 epoch total loss 1.22430015\n",
      "Trained batch 1288 batch loss 1.22943342 epoch total loss 1.2243042\n",
      "Trained batch 1289 batch loss 1.19746816 epoch total loss 1.22428346\n",
      "Trained batch 1290 batch loss 1.19933367 epoch total loss 1.22426403\n",
      "Trained batch 1291 batch loss 1.09441102 epoch total loss 1.22416341\n",
      "Trained batch 1292 batch loss 1.10737836 epoch total loss 1.22407305\n",
      "Trained batch 1293 batch loss 1.07563543 epoch total loss 1.22395837\n",
      "Trained batch 1294 batch loss 1.14390087 epoch total loss 1.2238965\n",
      "Trained batch 1295 batch loss 1.04897356 epoch total loss 1.22376144\n",
      "Trained batch 1296 batch loss 1.09356511 epoch total loss 1.22366083\n",
      "Trained batch 1297 batch loss 1.10933542 epoch total loss 1.22357273\n",
      "Trained batch 1298 batch loss 1.08713901 epoch total loss 1.22346771\n",
      "Trained batch 1299 batch loss 1.13153481 epoch total loss 1.2233969\n",
      "Trained batch 1300 batch loss 1.00526714 epoch total loss 1.22322917\n",
      "Trained batch 1301 batch loss 1.18428719 epoch total loss 1.22319925\n",
      "Trained batch 1302 batch loss 1.13073254 epoch total loss 1.2231282\n",
      "Trained batch 1303 batch loss 1.21452951 epoch total loss 1.22312152\n",
      "Trained batch 1304 batch loss 1.19732332 epoch total loss 1.22310174\n",
      "Trained batch 1305 batch loss 1.29766512 epoch total loss 1.22315884\n",
      "Trained batch 1306 batch loss 1.25463402 epoch total loss 1.22318292\n",
      "Trained batch 1307 batch loss 1.22603631 epoch total loss 1.22318518\n",
      "Trained batch 1308 batch loss 1.26833034 epoch total loss 1.22321963\n",
      "Trained batch 1309 batch loss 1.3815527 epoch total loss 1.22334063\n",
      "Trained batch 1310 batch loss 1.23910773 epoch total loss 1.22335267\n",
      "Trained batch 1311 batch loss 1.3208462 epoch total loss 1.22342706\n",
      "Trained batch 1312 batch loss 1.33725119 epoch total loss 1.22351384\n",
      "Trained batch 1313 batch loss 1.28253102 epoch total loss 1.22355866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1314 batch loss 1.26513493 epoch total loss 1.22359037\n",
      "Trained batch 1315 batch loss 1.25367689 epoch total loss 1.22361326\n",
      "Trained batch 1316 batch loss 1.2255708 epoch total loss 1.22361469\n",
      "Trained batch 1317 batch loss 1.29044008 epoch total loss 1.22366548\n",
      "Trained batch 1318 batch loss 1.32290673 epoch total loss 1.2237407\n",
      "Trained batch 1319 batch loss 1.14922702 epoch total loss 1.22368419\n",
      "Trained batch 1320 batch loss 1.30831242 epoch total loss 1.22374833\n",
      "Trained batch 1321 batch loss 1.15342522 epoch total loss 1.22369504\n",
      "Trained batch 1322 batch loss 1.13954592 epoch total loss 1.22363138\n",
      "Trained batch 1323 batch loss 1.26177216 epoch total loss 1.22366023\n",
      "Trained batch 1324 batch loss 1.12815547 epoch total loss 1.22358811\n",
      "Trained batch 1325 batch loss 1.26257181 epoch total loss 1.22361755\n",
      "Trained batch 1326 batch loss 1.51033258 epoch total loss 1.2238338\n",
      "Trained batch 1327 batch loss 1.33803487 epoch total loss 1.22391987\n",
      "Trained batch 1328 batch loss 1.25493979 epoch total loss 1.22394311\n",
      "Trained batch 1329 batch loss 1.19752693 epoch total loss 1.22392321\n",
      "Trained batch 1330 batch loss 1.27177024 epoch total loss 1.22395921\n",
      "Trained batch 1331 batch loss 1.10991311 epoch total loss 1.2238735\n",
      "Trained batch 1332 batch loss 1.13941348 epoch total loss 1.22381008\n",
      "Trained batch 1333 batch loss 1.27522671 epoch total loss 1.2238487\n",
      "Trained batch 1334 batch loss 1.11661053 epoch total loss 1.22376823\n",
      "Trained batch 1335 batch loss 1.11599362 epoch total loss 1.22368753\n",
      "Trained batch 1336 batch loss 1.04788303 epoch total loss 1.22355592\n",
      "Trained batch 1337 batch loss 1.04362381 epoch total loss 1.22342122\n",
      "Trained batch 1338 batch loss 1.0915854 epoch total loss 1.22332275\n",
      "Trained batch 1339 batch loss 1.37524581 epoch total loss 1.22343612\n",
      "Trained batch 1340 batch loss 1.55245173 epoch total loss 1.22368169\n",
      "Trained batch 1341 batch loss 1.51280499 epoch total loss 1.22389734\n",
      "Trained batch 1342 batch loss 1.32915938 epoch total loss 1.22397578\n",
      "Trained batch 1343 batch loss 1.29734087 epoch total loss 1.22403038\n",
      "Trained batch 1344 batch loss 1.43894792 epoch total loss 1.22419035\n",
      "Trained batch 1345 batch loss 1.27261281 epoch total loss 1.22422624\n",
      "Trained batch 1346 batch loss 1.22494173 epoch total loss 1.22422683\n",
      "Trained batch 1347 batch loss 1.37334883 epoch total loss 1.22433746\n",
      "Trained batch 1348 batch loss 1.25964129 epoch total loss 1.22436368\n",
      "Trained batch 1349 batch loss 1.17221689 epoch total loss 1.22432506\n",
      "Trained batch 1350 batch loss 1.18748 epoch total loss 1.22429776\n",
      "Trained batch 1351 batch loss 1.16139627 epoch total loss 1.22425115\n",
      "Trained batch 1352 batch loss 1.18998992 epoch total loss 1.22422588\n",
      "Trained batch 1353 batch loss 1.15563977 epoch total loss 1.2241751\n",
      "Trained batch 1354 batch loss 1.19982219 epoch total loss 1.22415721\n",
      "Trained batch 1355 batch loss 1.20697141 epoch total loss 1.22414446\n",
      "Trained batch 1356 batch loss 1.18690968 epoch total loss 1.22411704\n",
      "Trained batch 1357 batch loss 1.15893817 epoch total loss 1.224069\n",
      "Trained batch 1358 batch loss 1.08474064 epoch total loss 1.22396636\n",
      "Trained batch 1359 batch loss 1.15915203 epoch total loss 1.22391868\n",
      "Trained batch 1360 batch loss 1.16461 epoch total loss 1.22387505\n",
      "Trained batch 1361 batch loss 1.11874187 epoch total loss 1.2237978\n",
      "Trained batch 1362 batch loss 1.19071 epoch total loss 1.22377348\n",
      "Trained batch 1363 batch loss 1.1990819 epoch total loss 1.22375548\n",
      "Trained batch 1364 batch loss 1.16509748 epoch total loss 1.22371233\n",
      "Trained batch 1365 batch loss 1.28905213 epoch total loss 1.22376025\n",
      "Trained batch 1366 batch loss 1.27608478 epoch total loss 1.22379863\n",
      "Trained batch 1367 batch loss 1.18044519 epoch total loss 1.2237668\n",
      "Trained batch 1368 batch loss 1.22315431 epoch total loss 1.22376645\n",
      "Trained batch 1369 batch loss 1.22963667 epoch total loss 1.22377062\n",
      "Trained batch 1370 batch loss 1.21405303 epoch total loss 1.22376359\n",
      "Trained batch 1371 batch loss 1.17979 epoch total loss 1.22373152\n",
      "Trained batch 1372 batch loss 1.25370419 epoch total loss 1.22375333\n",
      "Trained batch 1373 batch loss 1.42490017 epoch total loss 1.22389984\n",
      "Trained batch 1374 batch loss 1.20337832 epoch total loss 1.22388494\n",
      "Trained batch 1375 batch loss 1.1858 epoch total loss 1.22385728\n",
      "Trained batch 1376 batch loss 1.16570616 epoch total loss 1.22381496\n",
      "Trained batch 1377 batch loss 1.32352114 epoch total loss 1.22388732\n",
      "Trained batch 1378 batch loss 1.21188736 epoch total loss 1.22387862\n",
      "Trained batch 1379 batch loss 1.19641209 epoch total loss 1.22385871\n",
      "Trained batch 1380 batch loss 1.19412 epoch total loss 1.22383714\n",
      "Trained batch 1381 batch loss 1.24321103 epoch total loss 1.22385108\n",
      "Trained batch 1382 batch loss 1.28124368 epoch total loss 1.22389269\n",
      "Trained batch 1383 batch loss 1.30342495 epoch total loss 1.22395027\n",
      "Trained batch 1384 batch loss 1.21557426 epoch total loss 1.22394419\n",
      "Trained batch 1385 batch loss 1.31016386 epoch total loss 1.22400641\n",
      "Trained batch 1386 batch loss 1.29572463 epoch total loss 1.22405815\n",
      "Trained batch 1387 batch loss 1.26560819 epoch total loss 1.22408819\n",
      "Trained batch 1388 batch loss 1.18762255 epoch total loss 1.22406185\n",
      "Epoch 4 train loss 1.2240618467330933\n",
      "Validated batch 1 batch loss 1.1585288\n",
      "Validated batch 2 batch loss 1.12371325\n",
      "Validated batch 3 batch loss 1.19805336\n",
      "Validated batch 4 batch loss 1.17261577\n",
      "Validated batch 5 batch loss 1.18842709\n",
      "Validated batch 6 batch loss 1.23078275\n",
      "Validated batch 7 batch loss 1.22818589\n",
      "Validated batch 8 batch loss 1.20876575\n",
      "Validated batch 9 batch loss 1.2191596\n",
      "Validated batch 10 batch loss 1.24079013\n",
      "Validated batch 11 batch loss 1.19104719\n",
      "Validated batch 12 batch loss 1.22221804\n",
      "Validated batch 13 batch loss 1.20082641\n",
      "Validated batch 14 batch loss 1.21378016\n",
      "Validated batch 15 batch loss 1.19556332\n",
      "Validated batch 16 batch loss 1.19071639\n",
      "Validated batch 17 batch loss 1.34944308\n",
      "Validated batch 18 batch loss 1.23893809\n",
      "Validated batch 19 batch loss 1.08285284\n",
      "Validated batch 20 batch loss 1.33843756\n",
      "Validated batch 21 batch loss 1.19287193\n",
      "Validated batch 22 batch loss 1.15550673\n",
      "Validated batch 23 batch loss 1.21507418\n",
      "Validated batch 24 batch loss 1.31295955\n",
      "Validated batch 25 batch loss 1.24064851\n",
      "Validated batch 26 batch loss 1.14453125\n",
      "Validated batch 27 batch loss 1.17772949\n",
      "Validated batch 28 batch loss 1.1207248\n",
      "Validated batch 29 batch loss 1.22247815\n",
      "Validated batch 30 batch loss 1.25328374\n",
      "Validated batch 31 batch loss 1.12861705\n",
      "Validated batch 32 batch loss 1.25631797\n",
      "Validated batch 33 batch loss 1.19073212\n",
      "Validated batch 34 batch loss 1.2531774\n",
      "Validated batch 35 batch loss 1.21325231\n",
      "Validated batch 36 batch loss 1.23890364\n",
      "Validated batch 37 batch loss 1.17782664\n",
      "Validated batch 38 batch loss 1.17688227\n",
      "Validated batch 39 batch loss 1.21573424\n",
      "Validated batch 40 batch loss 1.22059143\n",
      "Validated batch 41 batch loss 1.25125504\n",
      "Validated batch 42 batch loss 1.28570795\n",
      "Validated batch 43 batch loss 1.45890689\n",
      "Validated batch 44 batch loss 1.25052857\n",
      "Validated batch 45 batch loss 1.24465585\n",
      "Validated batch 46 batch loss 1.13356316\n",
      "Validated batch 47 batch loss 1.17480826\n",
      "Validated batch 48 batch loss 1.17622161\n",
      "Validated batch 49 batch loss 1.17028594\n",
      "Validated batch 50 batch loss 1.2035234\n",
      "Validated batch 51 batch loss 1.23316967\n",
      "Validated batch 52 batch loss 1.33147478\n",
      "Validated batch 53 batch loss 1.10234714\n",
      "Validated batch 54 batch loss 1.22798538\n",
      "Validated batch 55 batch loss 1.16742504\n",
      "Validated batch 56 batch loss 1.2238313\n",
      "Validated batch 57 batch loss 1.22712398\n",
      "Validated batch 58 batch loss 1.10978055\n",
      "Validated batch 59 batch loss 1.37098742\n",
      "Validated batch 60 batch loss 1.17280936\n",
      "Validated batch 61 batch loss 1.26904035\n",
      "Validated batch 62 batch loss 1.19799805\n",
      "Validated batch 63 batch loss 1.2951231\n",
      "Validated batch 64 batch loss 1.10948682\n",
      "Validated batch 65 batch loss 1.23246264\n",
      "Validated batch 66 batch loss 1.17030501\n",
      "Validated batch 67 batch loss 1.16891241\n",
      "Validated batch 68 batch loss 1.22563863\n",
      "Validated batch 69 batch loss 1.18458724\n",
      "Validated batch 70 batch loss 1.22295916\n",
      "Validated batch 71 batch loss 1.14161944\n",
      "Validated batch 72 batch loss 1.20337319\n",
      "Validated batch 73 batch loss 1.11749732\n",
      "Validated batch 74 batch loss 1.21088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 75 batch loss 1.26031\n",
      "Validated batch 76 batch loss 1.22157657\n",
      "Validated batch 77 batch loss 1.3126266\n",
      "Validated batch 78 batch loss 1.27438617\n",
      "Validated batch 79 batch loss 1.24148929\n",
      "Validated batch 80 batch loss 1.21439552\n",
      "Validated batch 81 batch loss 1.36835897\n",
      "Validated batch 82 batch loss 1.28533745\n",
      "Validated batch 83 batch loss 1.32678819\n",
      "Validated batch 84 batch loss 1.30740476\n",
      "Validated batch 85 batch loss 1.31272364\n",
      "Validated batch 86 batch loss 1.317747\n",
      "Validated batch 87 batch loss 1.1599313\n",
      "Validated batch 88 batch loss 1.22282457\n",
      "Validated batch 89 batch loss 1.31393647\n",
      "Validated batch 90 batch loss 1.2714231\n",
      "Validated batch 91 batch loss 1.14674556\n",
      "Validated batch 92 batch loss 1.2289716\n",
      "Validated batch 93 batch loss 1.22198069\n",
      "Validated batch 94 batch loss 1.26841\n",
      "Validated batch 95 batch loss 1.14694786\n",
      "Validated batch 96 batch loss 1.22856033\n",
      "Validated batch 97 batch loss 1.19975889\n",
      "Validated batch 98 batch loss 1.20323563\n",
      "Validated batch 99 batch loss 1.23561025\n",
      "Validated batch 100 batch loss 1.20539236\n",
      "Validated batch 101 batch loss 1.15744519\n",
      "Validated batch 102 batch loss 1.29244769\n",
      "Validated batch 103 batch loss 1.14514327\n",
      "Validated batch 104 batch loss 1.08930302\n",
      "Validated batch 105 batch loss 1.19132888\n",
      "Validated batch 106 batch loss 1.2852056\n",
      "Validated batch 107 batch loss 1.31857526\n",
      "Validated batch 108 batch loss 1.33870983\n",
      "Validated batch 109 batch loss 1.18473434\n",
      "Validated batch 110 batch loss 1.32572234\n",
      "Validated batch 111 batch loss 1.2167995\n",
      "Validated batch 112 batch loss 1.30536795\n",
      "Validated batch 113 batch loss 1.2764411\n",
      "Validated batch 114 batch loss 0.980448306\n",
      "Validated batch 115 batch loss 1.21029723\n",
      "Validated batch 116 batch loss 1.18929029\n",
      "Validated batch 117 batch loss 1.26729178\n",
      "Validated batch 118 batch loss 1.15176153\n",
      "Validated batch 119 batch loss 1.15731502\n",
      "Validated batch 120 batch loss 1.24519026\n",
      "Validated batch 121 batch loss 1.25432897\n",
      "Validated batch 122 batch loss 1.28698039\n",
      "Validated batch 123 batch loss 1.29371607\n",
      "Validated batch 124 batch loss 1.27133656\n",
      "Validated batch 125 batch loss 1.18158102\n",
      "Validated batch 126 batch loss 1.24653256\n",
      "Validated batch 127 batch loss 1.2145021\n",
      "Validated batch 128 batch loss 1.22182012\n",
      "Validated batch 129 batch loss 1.30904365\n",
      "Validated batch 130 batch loss 1.27896643\n",
      "Validated batch 131 batch loss 1.27321327\n",
      "Validated batch 132 batch loss 1.32307887\n",
      "Validated batch 133 batch loss 1.14516735\n",
      "Validated batch 134 batch loss 1.20559514\n",
      "Validated batch 135 batch loss 1.13753736\n",
      "Validated batch 136 batch loss 1.11841393\n",
      "Validated batch 137 batch loss 1.31994116\n",
      "Validated batch 138 batch loss 1.15865493\n",
      "Validated batch 139 batch loss 1.18813634\n",
      "Validated batch 140 batch loss 1.19240487\n",
      "Validated batch 141 batch loss 1.26922429\n",
      "Validated batch 142 batch loss 1.1533438\n",
      "Validated batch 143 batch loss 1.25485241\n",
      "Validated batch 144 batch loss 1.39945424\n",
      "Validated batch 145 batch loss 1.10295928\n",
      "Validated batch 146 batch loss 1.20340395\n",
      "Validated batch 147 batch loss 1.21189737\n",
      "Validated batch 148 batch loss 1.23613095\n",
      "Validated batch 149 batch loss 1.28118622\n",
      "Validated batch 150 batch loss 1.17988074\n",
      "Validated batch 151 batch loss 0.996302068\n",
      "Validated batch 152 batch loss 1.20933568\n",
      "Validated batch 153 batch loss 1.15021229\n",
      "Validated batch 154 batch loss 1.22125161\n",
      "Validated batch 155 batch loss 1.2699827\n",
      "Validated batch 156 batch loss 1.10194314\n",
      "Validated batch 157 batch loss 1.22351193\n",
      "Validated batch 158 batch loss 1.29028332\n",
      "Validated batch 159 batch loss 1.24322784\n",
      "Validated batch 160 batch loss 1.16654932\n",
      "Validated batch 161 batch loss 1.08744586\n",
      "Validated batch 162 batch loss 1.14820838\n",
      "Validated batch 163 batch loss 1.23988247\n",
      "Validated batch 164 batch loss 1.19710374\n",
      "Validated batch 165 batch loss 1.09321451\n",
      "Validated batch 166 batch loss 1.14208448\n",
      "Validated batch 167 batch loss 1.28783262\n",
      "Validated batch 168 batch loss 1.10714698\n",
      "Validated batch 169 batch loss 1.11851823\n",
      "Validated batch 170 batch loss 1.12348318\n",
      "Validated batch 171 batch loss 1.20524216\n",
      "Validated batch 172 batch loss 1.09612286\n",
      "Validated batch 173 batch loss 1.24351931\n",
      "Validated batch 174 batch loss 1.09757745\n",
      "Validated batch 175 batch loss 1.21940041\n",
      "Validated batch 176 batch loss 1.27442658\n",
      "Validated batch 177 batch loss 1.28561151\n",
      "Validated batch 178 batch loss 1.14027691\n",
      "Validated batch 179 batch loss 1.31313753\n",
      "Validated batch 180 batch loss 1.05316567\n",
      "Validated batch 181 batch loss 1.02491784\n",
      "Validated batch 182 batch loss 1.20309258\n",
      "Validated batch 183 batch loss 1.15914452\n",
      "Validated batch 184 batch loss 1.34040713\n",
      "Validated batch 185 batch loss 1.34945607\n",
      "Epoch 4 val loss 1.2147516012191772\n",
      "Model /aiffel/aiffel/mpii/models/model_HG-epoch-4-loss-1.2148.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.29233193 epoch total loss 1.29233193\n",
      "Trained batch 2 batch loss 1.10290825 epoch total loss 1.19762015\n",
      "Trained batch 3 batch loss 1.0227592 epoch total loss 1.13933313\n",
      "Trained batch 4 batch loss 0.994954467 epoch total loss 1.10323846\n",
      "Trained batch 5 batch loss 1.07047927 epoch total loss 1.0966866\n",
      "Trained batch 6 batch loss 1.16903496 epoch total loss 1.10874474\n",
      "Trained batch 7 batch loss 1.38177681 epoch total loss 1.14774919\n",
      "Trained batch 8 batch loss 1.23874807 epoch total loss 1.15912414\n",
      "Trained batch 9 batch loss 1.26873386 epoch total loss 1.17130303\n",
      "Trained batch 10 batch loss 1.04771709 epoch total loss 1.15894437\n",
      "Trained batch 11 batch loss 1.22903121 epoch total loss 1.16531599\n",
      "Trained batch 12 batch loss 1.20761991 epoch total loss 1.16884124\n",
      "Trained batch 13 batch loss 1.17109764 epoch total loss 1.16901481\n",
      "Trained batch 14 batch loss 1.23427558 epoch total loss 1.17367637\n",
      "Trained batch 15 batch loss 1.2001133 epoch total loss 1.17543876\n",
      "Trained batch 16 batch loss 1.32657743 epoch total loss 1.18488503\n",
      "Trained batch 17 batch loss 1.33912468 epoch total loss 1.19395792\n",
      "Trained batch 18 batch loss 1.20820796 epoch total loss 1.19474959\n",
      "Trained batch 19 batch loss 1.2704668 epoch total loss 1.19873464\n",
      "Trained batch 20 batch loss 1.13076866 epoch total loss 1.19533634\n",
      "Trained batch 21 batch loss 1.09675443 epoch total loss 1.190642\n",
      "Trained batch 22 batch loss 1.22334909 epoch total loss 1.19212866\n",
      "Trained batch 23 batch loss 1.2256161 epoch total loss 1.19358456\n",
      "Trained batch 24 batch loss 1.21030951 epoch total loss 1.19428146\n",
      "Trained batch 25 batch loss 1.17111516 epoch total loss 1.19335485\n",
      "Trained batch 26 batch loss 1.21596527 epoch total loss 1.19422448\n",
      "Trained batch 27 batch loss 1.24501121 epoch total loss 1.19610548\n",
      "Trained batch 28 batch loss 1.31528878 epoch total loss 1.20036209\n",
      "Trained batch 29 batch loss 1.25492585 epoch total loss 1.20224357\n",
      "Trained batch 30 batch loss 1.27821946 epoch total loss 1.20477617\n",
      "Trained batch 31 batch loss 1.2948972 epoch total loss 1.20768332\n",
      "Trained batch 32 batch loss 1.35853434 epoch total loss 1.21239746\n",
      "Trained batch 33 batch loss 1.3153559 epoch total loss 1.2155174\n",
      "Trained batch 34 batch loss 1.25424755 epoch total loss 1.21665657\n",
      "Trained batch 35 batch loss 1.16420352 epoch total loss 1.21515787\n",
      "Trained batch 36 batch loss 1.24659204 epoch total loss 1.21603107\n",
      "Trained batch 37 batch loss 1.14699221 epoch total loss 1.21416509\n",
      "Trained batch 38 batch loss 1.21546602 epoch total loss 1.21419942\n",
      "Trained batch 39 batch loss 1.20501637 epoch total loss 1.21396387\n",
      "Trained batch 40 batch loss 1.24593616 epoch total loss 1.21476328\n",
      "Trained batch 41 batch loss 1.23687446 epoch total loss 1.21530259\n",
      "Trained batch 42 batch loss 1.17223418 epoch total loss 1.21427703\n",
      "Trained batch 43 batch loss 1.20714378 epoch total loss 1.21411121\n",
      "Trained batch 44 batch loss 1.25582933 epoch total loss 1.21505928\n",
      "Trained batch 45 batch loss 1.28717816 epoch total loss 1.21666193\n",
      "Trained batch 46 batch loss 1.3093257 epoch total loss 1.21867633\n",
      "Trained batch 47 batch loss 1.25586963 epoch total loss 1.21946776\n",
      "Trained batch 48 batch loss 1.1982131 epoch total loss 1.2190249\n",
      "Trained batch 49 batch loss 1.01723254 epoch total loss 1.21490669\n",
      "Trained batch 50 batch loss 0.989205658 epoch total loss 1.21039259\n",
      "Trained batch 51 batch loss 1.17861295 epoch total loss 1.20976949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 52 batch loss 1.27343571 epoch total loss 1.21099389\n",
      "Trained batch 53 batch loss 1.29278111 epoch total loss 1.21253693\n",
      "Trained batch 54 batch loss 1.28610647 epoch total loss 1.21389937\n",
      "Trained batch 55 batch loss 1.33493114 epoch total loss 1.2161\n",
      "Trained batch 56 batch loss 1.30779755 epoch total loss 1.21773744\n",
      "Trained batch 57 batch loss 1.17856622 epoch total loss 1.21705019\n",
      "Trained batch 58 batch loss 1.27602482 epoch total loss 1.21806705\n",
      "Trained batch 59 batch loss 1.23255777 epoch total loss 1.21831262\n",
      "Trained batch 60 batch loss 1.25894523 epoch total loss 1.21898985\n",
      "Trained batch 61 batch loss 1.24259055 epoch total loss 1.21937668\n",
      "Trained batch 62 batch loss 1.37673223 epoch total loss 1.22191477\n",
      "Trained batch 63 batch loss 1.31185424 epoch total loss 1.2233423\n",
      "Trained batch 64 batch loss 1.2213887 epoch total loss 1.22331178\n",
      "Trained batch 65 batch loss 1.20648813 epoch total loss 1.22305298\n",
      "Trained batch 66 batch loss 1.21348178 epoch total loss 1.2229079\n",
      "Trained batch 67 batch loss 1.27326953 epoch total loss 1.22365952\n",
      "Trained batch 68 batch loss 1.11660635 epoch total loss 1.22208524\n",
      "Trained batch 69 batch loss 1.21676099 epoch total loss 1.22200811\n",
      "Trained batch 70 batch loss 1.16615295 epoch total loss 1.22121012\n",
      "Trained batch 71 batch loss 1.08563554 epoch total loss 1.21930063\n",
      "Trained batch 72 batch loss 1.31891966 epoch total loss 1.22068417\n",
      "Trained batch 73 batch loss 1.27301228 epoch total loss 1.22140098\n",
      "Trained batch 74 batch loss 1.19316268 epoch total loss 1.22101939\n",
      "Trained batch 75 batch loss 1.06985426 epoch total loss 1.2190038\n",
      "Trained batch 76 batch loss 0.985954881 epoch total loss 1.21593738\n",
      "Trained batch 77 batch loss 1.02862048 epoch total loss 1.21350467\n",
      "Trained batch 78 batch loss 1.03093433 epoch total loss 1.211164\n",
      "Trained batch 79 batch loss 1.00627673 epoch total loss 1.2085706\n",
      "Trained batch 80 batch loss 1.05797374 epoch total loss 1.20668817\n",
      "Trained batch 81 batch loss 1.02694416 epoch total loss 1.20446908\n",
      "Trained batch 82 batch loss 1.14678121 epoch total loss 1.20376563\n",
      "Trained batch 83 batch loss 1.14182067 epoch total loss 1.20301926\n",
      "Trained batch 84 batch loss 1.20691645 epoch total loss 1.20306563\n",
      "Trained batch 85 batch loss 1.15812755 epoch total loss 1.20253694\n",
      "Trained batch 86 batch loss 1.22926056 epoch total loss 1.20284772\n",
      "Trained batch 87 batch loss 1.11329949 epoch total loss 1.20181847\n",
      "Trained batch 88 batch loss 1.19134629 epoch total loss 1.20169938\n",
      "Trained batch 89 batch loss 1.28326523 epoch total loss 1.20261586\n",
      "Trained batch 90 batch loss 1.25825238 epoch total loss 1.20323408\n",
      "Trained batch 91 batch loss 1.28692245 epoch total loss 1.20415366\n",
      "Trained batch 92 batch loss 1.44434834 epoch total loss 1.20676458\n",
      "Trained batch 93 batch loss 1.42414033 epoch total loss 1.20910192\n",
      "Trained batch 94 batch loss 1.32925963 epoch total loss 1.2103802\n",
      "Trained batch 95 batch loss 1.11936259 epoch total loss 1.20942211\n",
      "Trained batch 96 batch loss 1.19472253 epoch total loss 1.20926905\n",
      "Trained batch 97 batch loss 1.05732965 epoch total loss 1.20770264\n",
      "Trained batch 98 batch loss 1.09728408 epoch total loss 1.20657587\n",
      "Trained batch 99 batch loss 1.05650401 epoch total loss 1.20506\n",
      "Trained batch 100 batch loss 1.10445809 epoch total loss 1.204054\n",
      "Trained batch 101 batch loss 1.07999301 epoch total loss 1.20282567\n",
      "Trained batch 102 batch loss 1.0272789 epoch total loss 1.20110476\n",
      "Trained batch 103 batch loss 0.993718 epoch total loss 1.19909132\n",
      "Trained batch 104 batch loss 1.05484772 epoch total loss 1.19770432\n",
      "Trained batch 105 batch loss 1.1387732 epoch total loss 1.19714308\n",
      "Trained batch 106 batch loss 1.14212942 epoch total loss 1.19662404\n",
      "Trained batch 107 batch loss 0.99425447 epoch total loss 1.19473279\n",
      "Trained batch 108 batch loss 0.938245118 epoch total loss 1.19235778\n",
      "Trained batch 109 batch loss 1.11063361 epoch total loss 1.19160795\n",
      "Trained batch 110 batch loss 1.16307759 epoch total loss 1.19134855\n",
      "Trained batch 111 batch loss 1.19813633 epoch total loss 1.19140971\n",
      "Trained batch 112 batch loss 1.1172576 epoch total loss 1.19074762\n",
      "Trained batch 113 batch loss 1.05422044 epoch total loss 1.18953943\n",
      "Trained batch 114 batch loss 0.949314177 epoch total loss 1.18743217\n",
      "Trained batch 115 batch loss 1.16547227 epoch total loss 1.18724108\n",
      "Trained batch 116 batch loss 1.27384412 epoch total loss 1.1879878\n",
      "Trained batch 117 batch loss 1.41396248 epoch total loss 1.18991911\n",
      "Trained batch 118 batch loss 1.38619912 epoch total loss 1.19158256\n",
      "Trained batch 119 batch loss 1.24955416 epoch total loss 1.19206965\n",
      "Trained batch 120 batch loss 1.32836199 epoch total loss 1.19320548\n",
      "Trained batch 121 batch loss 1.28230512 epoch total loss 1.19394183\n",
      "Trained batch 122 batch loss 1.17416668 epoch total loss 1.19377971\n",
      "Trained batch 123 batch loss 1.1997211 epoch total loss 1.19382811\n",
      "Trained batch 124 batch loss 1.13892937 epoch total loss 1.19338536\n",
      "Trained batch 125 batch loss 1.09045243 epoch total loss 1.19256186\n",
      "Trained batch 126 batch loss 1.18041062 epoch total loss 1.19246542\n",
      "Trained batch 127 batch loss 1.25720263 epoch total loss 1.19297516\n",
      "Trained batch 128 batch loss 1.18677771 epoch total loss 1.19292676\n",
      "Trained batch 129 batch loss 1.29693711 epoch total loss 1.1937331\n",
      "Trained batch 130 batch loss 1.19934464 epoch total loss 1.19377613\n",
      "Trained batch 131 batch loss 1.29367781 epoch total loss 1.19453871\n",
      "Trained batch 132 batch loss 1.26086509 epoch total loss 1.19504118\n",
      "Trained batch 133 batch loss 1.11185992 epoch total loss 1.19441581\n",
      "Trained batch 134 batch loss 1.33530593 epoch total loss 1.19546723\n",
      "Trained batch 135 batch loss 1.16883576 epoch total loss 1.19527\n",
      "Trained batch 136 batch loss 1.18313622 epoch total loss 1.19518077\n",
      "Trained batch 137 batch loss 1.09173226 epoch total loss 1.1944257\n",
      "Trained batch 138 batch loss 1.14357519 epoch total loss 1.19405723\n",
      "Trained batch 139 batch loss 1.14843643 epoch total loss 1.19372904\n",
      "Trained batch 140 batch loss 1.10006416 epoch total loss 1.19305992\n",
      "Trained batch 141 batch loss 1.24618435 epoch total loss 1.19343674\n",
      "Trained batch 142 batch loss 1.20670402 epoch total loss 1.1935302\n",
      "Trained batch 143 batch loss 1.28701162 epoch total loss 1.19418395\n",
      "Trained batch 144 batch loss 1.25289774 epoch total loss 1.19459176\n",
      "Trained batch 145 batch loss 1.23270488 epoch total loss 1.19485462\n",
      "Trained batch 146 batch loss 1.23505175 epoch total loss 1.19512987\n",
      "Trained batch 147 batch loss 1.23186874 epoch total loss 1.19537985\n",
      "Trained batch 148 batch loss 1.04263854 epoch total loss 1.19434774\n",
      "Trained batch 149 batch loss 1.12016296 epoch total loss 1.19384992\n",
      "Trained batch 150 batch loss 1.05713725 epoch total loss 1.19293857\n",
      "Trained batch 151 batch loss 1.18051779 epoch total loss 1.19285619\n",
      "Trained batch 152 batch loss 1.1993587 epoch total loss 1.19289899\n",
      "Trained batch 153 batch loss 1.27328658 epoch total loss 1.19342434\n",
      "Trained batch 154 batch loss 1.16687524 epoch total loss 1.19325197\n",
      "Trained batch 155 batch loss 1.20614839 epoch total loss 1.19333518\n",
      "Trained batch 156 batch loss 1.28775716 epoch total loss 1.1939404\n",
      "Trained batch 157 batch loss 1.31592703 epoch total loss 1.19471741\n",
      "Trained batch 158 batch loss 1.19008517 epoch total loss 1.19468808\n",
      "Trained batch 159 batch loss 1.20000565 epoch total loss 1.19472158\n",
      "Trained batch 160 batch loss 1.02116513 epoch total loss 1.19363678\n",
      "Trained batch 161 batch loss 1.03621268 epoch total loss 1.19265902\n",
      "Trained batch 162 batch loss 1.04902422 epoch total loss 1.19177234\n",
      "Trained batch 163 batch loss 1.15344167 epoch total loss 1.19153726\n",
      "Trained batch 164 batch loss 1.03446805 epoch total loss 1.19057953\n",
      "Trained batch 165 batch loss 1.14159846 epoch total loss 1.1902827\n",
      "Trained batch 166 batch loss 1.10054517 epoch total loss 1.18974209\n",
      "Trained batch 167 batch loss 1.16603303 epoch total loss 1.1896\n",
      "Trained batch 168 batch loss 1.09235573 epoch total loss 1.18902123\n",
      "Trained batch 169 batch loss 1.02719808 epoch total loss 1.18806362\n",
      "Trained batch 170 batch loss 1.24697375 epoch total loss 1.18841028\n",
      "Trained batch 171 batch loss 1.237113 epoch total loss 1.18869495\n",
      "Trained batch 172 batch loss 1.30283177 epoch total loss 1.18935859\n",
      "Trained batch 173 batch loss 1.15154946 epoch total loss 1.18914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 174 batch loss 1.23353148 epoch total loss 1.18939519\n",
      "Trained batch 175 batch loss 1.19169211 epoch total loss 1.1894083\n",
      "Trained batch 176 batch loss 1.10699868 epoch total loss 1.18894\n",
      "Trained batch 177 batch loss 1.23585069 epoch total loss 1.18920505\n",
      "Trained batch 178 batch loss 1.17295241 epoch total loss 1.18911386\n",
      "Trained batch 179 batch loss 1.19645953 epoch total loss 1.18915486\n",
      "Trained batch 180 batch loss 1.32012737 epoch total loss 1.18988252\n",
      "Trained batch 181 batch loss 1.24356174 epoch total loss 1.19017911\n",
      "Trained batch 182 batch loss 1.14571202 epoch total loss 1.18993473\n",
      "Trained batch 183 batch loss 1.24356353 epoch total loss 1.19022775\n",
      "Trained batch 184 batch loss 1.2052176 epoch total loss 1.19030917\n",
      "Trained batch 185 batch loss 1.1774801 epoch total loss 1.19023979\n",
      "Trained batch 186 batch loss 1.22733665 epoch total loss 1.19043934\n",
      "Trained batch 187 batch loss 1.20517302 epoch total loss 1.19051802\n",
      "Trained batch 188 batch loss 1.16352057 epoch total loss 1.19037437\n",
      "Trained batch 189 batch loss 1.28732562 epoch total loss 1.19088733\n",
      "Trained batch 190 batch loss 1.39623082 epoch total loss 1.19196808\n",
      "Trained batch 191 batch loss 1.15659702 epoch total loss 1.19178295\n",
      "Trained batch 192 batch loss 1.29821181 epoch total loss 1.19233727\n",
      "Trained batch 193 batch loss 1.15421915 epoch total loss 1.19213974\n",
      "Trained batch 194 batch loss 1.24372697 epoch total loss 1.1924057\n",
      "Trained batch 195 batch loss 1.19443083 epoch total loss 1.19241607\n",
      "Trained batch 196 batch loss 1.25052845 epoch total loss 1.19271255\n",
      "Trained batch 197 batch loss 1.13376069 epoch total loss 1.19241333\n",
      "Trained batch 198 batch loss 1.28118455 epoch total loss 1.19286168\n",
      "Trained batch 199 batch loss 1.17892826 epoch total loss 1.1927917\n",
      "Trained batch 200 batch loss 1.15795541 epoch total loss 1.19261754\n",
      "Trained batch 201 batch loss 1.13024402 epoch total loss 1.19230723\n",
      "Trained batch 202 batch loss 1.09795237 epoch total loss 1.19184\n",
      "Trained batch 203 batch loss 1.15383101 epoch total loss 1.19165277\n",
      "Trained batch 204 batch loss 1.2423718 epoch total loss 1.19190145\n",
      "Trained batch 205 batch loss 1.25275731 epoch total loss 1.19219828\n",
      "Trained batch 206 batch loss 1.27896869 epoch total loss 1.19261944\n",
      "Trained batch 207 batch loss 1.2361846 epoch total loss 1.19283\n",
      "Trained batch 208 batch loss 1.28664136 epoch total loss 1.19328094\n",
      "Trained batch 209 batch loss 1.2078644 epoch total loss 1.19335079\n",
      "Trained batch 210 batch loss 1.28695726 epoch total loss 1.19379652\n",
      "Trained batch 211 batch loss 1.23975015 epoch total loss 1.19401431\n",
      "Trained batch 212 batch loss 1.15776908 epoch total loss 1.19384336\n",
      "Trained batch 213 batch loss 1.2331835 epoch total loss 1.19402802\n",
      "Trained batch 214 batch loss 1.145679 epoch total loss 1.19380212\n",
      "Trained batch 215 batch loss 1.1211555 epoch total loss 1.19346428\n",
      "Trained batch 216 batch loss 1.12361765 epoch total loss 1.19314098\n",
      "Trained batch 217 batch loss 1.15675664 epoch total loss 1.19297338\n",
      "Trained batch 218 batch loss 1.19540048 epoch total loss 1.19298446\n",
      "Trained batch 219 batch loss 1.21351779 epoch total loss 1.19307828\n",
      "Trained batch 220 batch loss 1.2494297 epoch total loss 1.19333446\n",
      "Trained batch 221 batch loss 1.24081969 epoch total loss 1.19354928\n",
      "Trained batch 222 batch loss 1.28273046 epoch total loss 1.19395101\n",
      "Trained batch 223 batch loss 1.27140474 epoch total loss 1.19429827\n",
      "Trained batch 224 batch loss 1.20538867 epoch total loss 1.19434774\n",
      "Trained batch 225 batch loss 1.15936637 epoch total loss 1.19419229\n",
      "Trained batch 226 batch loss 1.1744734 epoch total loss 1.19410503\n",
      "Trained batch 227 batch loss 1.06227756 epoch total loss 1.19352436\n",
      "Trained batch 228 batch loss 1.1292218 epoch total loss 1.19324231\n",
      "Trained batch 229 batch loss 1.24741268 epoch total loss 1.19347882\n",
      "Trained batch 230 batch loss 1.10925579 epoch total loss 1.19311261\n",
      "Trained batch 231 batch loss 1.10907912 epoch total loss 1.19274879\n",
      "Trained batch 232 batch loss 1.11479115 epoch total loss 1.19241261\n",
      "Trained batch 233 batch loss 1.08780897 epoch total loss 1.19196367\n",
      "Trained batch 234 batch loss 1.15664637 epoch total loss 1.19181275\n",
      "Trained batch 235 batch loss 1.18264198 epoch total loss 1.19177377\n",
      "Trained batch 236 batch loss 1.13496709 epoch total loss 1.19153309\n",
      "Trained batch 237 batch loss 1.23008823 epoch total loss 1.19169581\n",
      "Trained batch 238 batch loss 1.19374824 epoch total loss 1.19170451\n",
      "Trained batch 239 batch loss 1.09820497 epoch total loss 1.19131327\n",
      "Trained batch 240 batch loss 1.06147552 epoch total loss 1.19077229\n",
      "Trained batch 241 batch loss 1.07428765 epoch total loss 1.1902889\n",
      "Trained batch 242 batch loss 1.224491 epoch total loss 1.19043016\n",
      "Trained batch 243 batch loss 1.10096729 epoch total loss 1.19006193\n",
      "Trained batch 244 batch loss 1.09661615 epoch total loss 1.18967903\n",
      "Trained batch 245 batch loss 1.08004189 epoch total loss 1.18923151\n",
      "Trained batch 246 batch loss 1.11940634 epoch total loss 1.18894768\n",
      "Trained batch 247 batch loss 1.23701859 epoch total loss 1.18914235\n",
      "Trained batch 248 batch loss 1.09323299 epoch total loss 1.18875563\n",
      "Trained batch 249 batch loss 1.20397377 epoch total loss 1.18881679\n",
      "Trained batch 250 batch loss 1.12105215 epoch total loss 1.18854582\n",
      "Trained batch 251 batch loss 1.07998836 epoch total loss 1.18811321\n",
      "Trained batch 252 batch loss 1.04566419 epoch total loss 1.18754792\n",
      "Trained batch 253 batch loss 1.22729194 epoch total loss 1.18770504\n",
      "Trained batch 254 batch loss 1.13859773 epoch total loss 1.1875118\n",
      "Trained batch 255 batch loss 1.32536566 epoch total loss 1.18805242\n",
      "Trained batch 256 batch loss 1.40425396 epoch total loss 1.18889701\n",
      "Trained batch 257 batch loss 1.35492206 epoch total loss 1.18954301\n",
      "Trained batch 258 batch loss 1.20157373 epoch total loss 1.18958962\n",
      "Trained batch 259 batch loss 1.14712703 epoch total loss 1.18942571\n",
      "Trained batch 260 batch loss 1.21052313 epoch total loss 1.18950677\n",
      "Trained batch 261 batch loss 1.29148555 epoch total loss 1.18989742\n",
      "Trained batch 262 batch loss 1.34279954 epoch total loss 1.19048107\n",
      "Trained batch 263 batch loss 1.34231782 epoch total loss 1.1910584\n",
      "Trained batch 264 batch loss 1.15831375 epoch total loss 1.19093442\n",
      "Trained batch 265 batch loss 1.17947781 epoch total loss 1.19089115\n",
      "Trained batch 266 batch loss 1.24939013 epoch total loss 1.19111109\n",
      "Trained batch 267 batch loss 1.20277488 epoch total loss 1.19115484\n",
      "Trained batch 268 batch loss 1.21406078 epoch total loss 1.19124019\n",
      "Trained batch 269 batch loss 1.20037913 epoch total loss 1.19127417\n",
      "Trained batch 270 batch loss 1.33763313 epoch total loss 1.19181633\n",
      "Trained batch 271 batch loss 1.16086102 epoch total loss 1.19170213\n",
      "Trained batch 272 batch loss 1.1641202 epoch total loss 1.19160068\n",
      "Trained batch 273 batch loss 1.17188668 epoch total loss 1.19152844\n",
      "Trained batch 274 batch loss 1.08025694 epoch total loss 1.19112229\n",
      "Trained batch 275 batch loss 1.19694042 epoch total loss 1.19114351\n",
      "Trained batch 276 batch loss 1.07723475 epoch total loss 1.19073081\n",
      "Trained batch 277 batch loss 1.19637275 epoch total loss 1.19075119\n",
      "Trained batch 278 batch loss 1.0147599 epoch total loss 1.19011819\n",
      "Trained batch 279 batch loss 1.06146371 epoch total loss 1.18965697\n",
      "Trained batch 280 batch loss 1.13075697 epoch total loss 1.18944669\n",
      "Trained batch 281 batch loss 1.21161473 epoch total loss 1.1895256\n",
      "Trained batch 282 batch loss 1.26557922 epoch total loss 1.18979526\n",
      "Trained batch 283 batch loss 1.28136897 epoch total loss 1.19011891\n",
      "Trained batch 284 batch loss 1.27340174 epoch total loss 1.19041216\n",
      "Trained batch 285 batch loss 1.29874694 epoch total loss 1.1907922\n",
      "Trained batch 286 batch loss 1.08319473 epoch total loss 1.19041598\n",
      "Trained batch 287 batch loss 1.14499652 epoch total loss 1.19025779\n",
      "Trained batch 288 batch loss 1.09692144 epoch total loss 1.18993366\n",
      "Trained batch 289 batch loss 1.20988798 epoch total loss 1.1900028\n",
      "Trained batch 290 batch loss 1.15879881 epoch total loss 1.18989515\n",
      "Trained batch 291 batch loss 1.27920437 epoch total loss 1.19020212\n",
      "Trained batch 292 batch loss 1.12843549 epoch total loss 1.18999064\n",
      "Trained batch 293 batch loss 1.13158488 epoch total loss 1.18979132\n",
      "Trained batch 294 batch loss 1.11013615 epoch total loss 1.18952036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 295 batch loss 1.15806007 epoch total loss 1.18941367\n",
      "Trained batch 296 batch loss 1.205585 epoch total loss 1.18946838\n",
      "Trained batch 297 batch loss 1.06500947 epoch total loss 1.18904936\n",
      "Trained batch 298 batch loss 1.20457137 epoch total loss 1.18910134\n",
      "Trained batch 299 batch loss 1.14526772 epoch total loss 1.18895471\n",
      "Trained batch 300 batch loss 1.14858437 epoch total loss 1.18882024\n",
      "Trained batch 301 batch loss 1.16557252 epoch total loss 1.18874288\n",
      "Trained batch 302 batch loss 1.11460853 epoch total loss 1.18849742\n",
      "Trained batch 303 batch loss 1.07091832 epoch total loss 1.1881094\n",
      "Trained batch 304 batch loss 1.10209584 epoch total loss 1.1878264\n",
      "Trained batch 305 batch loss 1.23203611 epoch total loss 1.18797123\n",
      "Trained batch 306 batch loss 1.26802826 epoch total loss 1.1882329\n",
      "Trained batch 307 batch loss 1.41973031 epoch total loss 1.18898702\n",
      "Trained batch 308 batch loss 1.43527758 epoch total loss 1.18978667\n",
      "Trained batch 309 batch loss 1.29756224 epoch total loss 1.19013548\n",
      "Trained batch 310 batch loss 1.26721168 epoch total loss 1.19038415\n",
      "Trained batch 311 batch loss 1.31895268 epoch total loss 1.19079745\n",
      "Trained batch 312 batch loss 1.21867013 epoch total loss 1.19088674\n",
      "Trained batch 313 batch loss 1.06079149 epoch total loss 1.19047117\n",
      "Trained batch 314 batch loss 1.18261576 epoch total loss 1.19044614\n",
      "Trained batch 315 batch loss 1.22084618 epoch total loss 1.1905427\n",
      "Trained batch 316 batch loss 1.2799418 epoch total loss 1.19082558\n",
      "Trained batch 317 batch loss 1.23312008 epoch total loss 1.19095898\n",
      "Trained batch 318 batch loss 1.1570729 epoch total loss 1.1908524\n",
      "Trained batch 319 batch loss 1.12965441 epoch total loss 1.1906606\n",
      "Trained batch 320 batch loss 1.11489618 epoch total loss 1.19042385\n",
      "Trained batch 321 batch loss 1.02897429 epoch total loss 1.1899209\n",
      "Trained batch 322 batch loss 1.11310768 epoch total loss 1.18968225\n",
      "Trained batch 323 batch loss 1.16093814 epoch total loss 1.18959332\n",
      "Trained batch 324 batch loss 1.08101177 epoch total loss 1.18925822\n",
      "Trained batch 325 batch loss 1.18747187 epoch total loss 1.18925273\n",
      "Trained batch 326 batch loss 1.1432904 epoch total loss 1.18911171\n",
      "Trained batch 327 batch loss 1.13523078 epoch total loss 1.18894696\n",
      "Trained batch 328 batch loss 1.04288483 epoch total loss 1.1885016\n",
      "Trained batch 329 batch loss 1.22975469 epoch total loss 1.188627\n",
      "Trained batch 330 batch loss 1.20059443 epoch total loss 1.18866324\n",
      "Trained batch 331 batch loss 1.27715015 epoch total loss 1.18893063\n",
      "Trained batch 332 batch loss 1.20131707 epoch total loss 1.18896794\n",
      "Trained batch 333 batch loss 1.22755206 epoch total loss 1.18908381\n",
      "Trained batch 334 batch loss 1.25581777 epoch total loss 1.18928361\n",
      "Trained batch 335 batch loss 1.31260967 epoch total loss 1.18965185\n",
      "Trained batch 336 batch loss 1.26923931 epoch total loss 1.1898886\n",
      "Trained batch 337 batch loss 1.13981938 epoch total loss 1.18974006\n",
      "Trained batch 338 batch loss 1.15181375 epoch total loss 1.18962789\n",
      "Trained batch 339 batch loss 1.14174533 epoch total loss 1.18948674\n",
      "Trained batch 340 batch loss 1.13595462 epoch total loss 1.18932927\n",
      "Trained batch 341 batch loss 1.09836769 epoch total loss 1.18906248\n",
      "Trained batch 342 batch loss 1.20488071 epoch total loss 1.18910873\n",
      "Trained batch 343 batch loss 1.13740897 epoch total loss 1.18895805\n",
      "Trained batch 344 batch loss 1.11108875 epoch total loss 1.18873167\n",
      "Trained batch 345 batch loss 1.1333046 epoch total loss 1.18857098\n",
      "Trained batch 346 batch loss 1.1904273 epoch total loss 1.18857646\n",
      "Trained batch 347 batch loss 1.17418098 epoch total loss 1.18853498\n",
      "Trained batch 348 batch loss 1.47999406 epoch total loss 1.18937242\n",
      "Trained batch 349 batch loss 1.44075942 epoch total loss 1.1900928\n",
      "Trained batch 350 batch loss 1.07470107 epoch total loss 1.18976307\n",
      "Trained batch 351 batch loss 1.23111391 epoch total loss 1.18988085\n",
      "Trained batch 352 batch loss 1.08013749 epoch total loss 1.18956912\n",
      "Trained batch 353 batch loss 1.28596878 epoch total loss 1.18984222\n",
      "Trained batch 354 batch loss 1.27300894 epoch total loss 1.19007719\n",
      "Trained batch 355 batch loss 1.17699313 epoch total loss 1.19004035\n",
      "Trained batch 356 batch loss 1.22973919 epoch total loss 1.19015181\n",
      "Trained batch 357 batch loss 1.28962159 epoch total loss 1.1904304\n",
      "Trained batch 358 batch loss 1.36021 epoch total loss 1.19090462\n",
      "Trained batch 359 batch loss 1.46517956 epoch total loss 1.19166863\n",
      "Trained batch 360 batch loss 1.23331141 epoch total loss 1.19178438\n",
      "Trained batch 361 batch loss 1.17720294 epoch total loss 1.19174397\n",
      "Trained batch 362 batch loss 1.16288614 epoch total loss 1.19166422\n",
      "Trained batch 363 batch loss 1.24540305 epoch total loss 1.19181228\n",
      "Trained batch 364 batch loss 1.20872772 epoch total loss 1.19185877\n",
      "Trained batch 365 batch loss 1.19391251 epoch total loss 1.19186437\n",
      "Trained batch 366 batch loss 1.1076982 epoch total loss 1.19163442\n",
      "Trained batch 367 batch loss 1.10530436 epoch total loss 1.19139922\n",
      "Trained batch 368 batch loss 1.12266064 epoch total loss 1.19121242\n",
      "Trained batch 369 batch loss 1.27757168 epoch total loss 1.1914463\n",
      "Trained batch 370 batch loss 1.23286426 epoch total loss 1.19155824\n",
      "Trained batch 371 batch loss 1.21264303 epoch total loss 1.1916151\n",
      "Trained batch 372 batch loss 1.21137786 epoch total loss 1.19166815\n",
      "Trained batch 373 batch loss 1.1342653 epoch total loss 1.19151437\n",
      "Trained batch 374 batch loss 1.04755068 epoch total loss 1.19112945\n",
      "Trained batch 375 batch loss 1.12786651 epoch total loss 1.19096065\n",
      "Trained batch 376 batch loss 1.07982564 epoch total loss 1.19066513\n",
      "Trained batch 377 batch loss 1.00145507 epoch total loss 1.19016325\n",
      "Trained batch 378 batch loss 1.09472525 epoch total loss 1.18991077\n",
      "Trained batch 379 batch loss 1.04486167 epoch total loss 1.18952811\n",
      "Trained batch 380 batch loss 1.04913282 epoch total loss 1.18915868\n",
      "Trained batch 381 batch loss 1.06968427 epoch total loss 1.18884504\n",
      "Trained batch 382 batch loss 1.3311528 epoch total loss 1.18921757\n",
      "Trained batch 383 batch loss 1.31362426 epoch total loss 1.18954241\n",
      "Trained batch 384 batch loss 1.10932374 epoch total loss 1.18933344\n",
      "Trained batch 385 batch loss 1.20242667 epoch total loss 1.18936741\n",
      "Trained batch 386 batch loss 1.18326402 epoch total loss 1.18935156\n",
      "Trained batch 387 batch loss 1.11884093 epoch total loss 1.18916941\n",
      "Trained batch 388 batch loss 1.03643513 epoch total loss 1.18877578\n",
      "Trained batch 389 batch loss 1.29504514 epoch total loss 1.18904889\n",
      "Trained batch 390 batch loss 1.23332572 epoch total loss 1.18916249\n",
      "Trained batch 391 batch loss 1.24858701 epoch total loss 1.18931448\n",
      "Trained batch 392 batch loss 1.06550765 epoch total loss 1.1889987\n",
      "Trained batch 393 batch loss 1.22304904 epoch total loss 1.18908536\n",
      "Trained batch 394 batch loss 1.3169663 epoch total loss 1.18940985\n",
      "Trained batch 395 batch loss 1.31040978 epoch total loss 1.18971622\n",
      "Trained batch 396 batch loss 1.432 epoch total loss 1.19032812\n",
      "Trained batch 397 batch loss 1.3054415 epoch total loss 1.19061804\n",
      "Trained batch 398 batch loss 1.20194089 epoch total loss 1.19064653\n",
      "Trained batch 399 batch loss 1.1461072 epoch total loss 1.19053495\n",
      "Trained batch 400 batch loss 1.16382885 epoch total loss 1.19046819\n",
      "Trained batch 401 batch loss 1.20224929 epoch total loss 1.19049752\n",
      "Trained batch 402 batch loss 1.26031852 epoch total loss 1.19067121\n",
      "Trained batch 403 batch loss 1.32359 epoch total loss 1.19100094\n",
      "Trained batch 404 batch loss 1.34770823 epoch total loss 1.19138885\n",
      "Trained batch 405 batch loss 1.3325417 epoch total loss 1.19173741\n",
      "Trained batch 406 batch loss 1.38797235 epoch total loss 1.19222081\n",
      "Trained batch 407 batch loss 1.29469407 epoch total loss 1.19247258\n",
      "Trained batch 408 batch loss 1.3811183 epoch total loss 1.19293487\n",
      "Trained batch 409 batch loss 1.33677733 epoch total loss 1.19328666\n",
      "Trained batch 410 batch loss 1.25345147 epoch total loss 1.1934334\n",
      "Trained batch 411 batch loss 1.30971932 epoch total loss 1.19371629\n",
      "Trained batch 412 batch loss 1.2727077 epoch total loss 1.19390798\n",
      "Trained batch 413 batch loss 1.22748625 epoch total loss 1.19398928\n",
      "Trained batch 414 batch loss 1.10899961 epoch total loss 1.193784\n",
      "Trained batch 415 batch loss 1.15857732 epoch total loss 1.19369924\n",
      "Trained batch 416 batch loss 1.20935798 epoch total loss 1.19373679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 417 batch loss 1.15084481 epoch total loss 1.19363391\n",
      "Trained batch 418 batch loss 1.2038312 epoch total loss 1.19365835\n",
      "Trained batch 419 batch loss 1.03846931 epoch total loss 1.19328797\n",
      "Trained batch 420 batch loss 1.23531067 epoch total loss 1.1933881\n",
      "Trained batch 421 batch loss 1.16154516 epoch total loss 1.19331253\n",
      "Trained batch 422 batch loss 1.2606473 epoch total loss 1.19347203\n",
      "Trained batch 423 batch loss 1.17317224 epoch total loss 1.19342411\n",
      "Trained batch 424 batch loss 1.13272345 epoch total loss 1.19328094\n",
      "Trained batch 425 batch loss 1.10146105 epoch total loss 1.19306493\n",
      "Trained batch 426 batch loss 1.12907505 epoch total loss 1.19291472\n",
      "Trained batch 427 batch loss 1.21172523 epoch total loss 1.19295883\n",
      "Trained batch 428 batch loss 1.21630979 epoch total loss 1.19301331\n",
      "Trained batch 429 batch loss 1.21664834 epoch total loss 1.19306839\n",
      "Trained batch 430 batch loss 1.13193202 epoch total loss 1.19292629\n",
      "Trained batch 431 batch loss 1.05411577 epoch total loss 1.1926043\n",
      "Trained batch 432 batch loss 1.15304804 epoch total loss 1.19251275\n",
      "Trained batch 433 batch loss 1.0816344 epoch total loss 1.19225669\n",
      "Trained batch 434 batch loss 1.06514668 epoch total loss 1.19196367\n",
      "Trained batch 435 batch loss 1.25281024 epoch total loss 1.19210362\n",
      "Trained batch 436 batch loss 1.13083172 epoch total loss 1.19196308\n",
      "Trained batch 437 batch loss 0.963078 epoch total loss 1.19143939\n",
      "Trained batch 438 batch loss 1.02064776 epoch total loss 1.19104934\n",
      "Trained batch 439 batch loss 1.07798553 epoch total loss 1.19079185\n",
      "Trained batch 440 batch loss 1.07294977 epoch total loss 1.19052398\n",
      "Trained batch 441 batch loss 1.24278069 epoch total loss 1.1906426\n",
      "Trained batch 442 batch loss 1.12102389 epoch total loss 1.19048512\n",
      "Trained batch 443 batch loss 1.13214564 epoch total loss 1.19035339\n",
      "Trained batch 444 batch loss 1.2771095 epoch total loss 1.19054878\n",
      "Trained batch 445 batch loss 1.22514844 epoch total loss 1.1906265\n",
      "Trained batch 446 batch loss 1.35530114 epoch total loss 1.19099569\n",
      "Trained batch 447 batch loss 1.24399376 epoch total loss 1.19111431\n",
      "Trained batch 448 batch loss 1.04557824 epoch total loss 1.19078946\n",
      "Trained batch 449 batch loss 1.05037761 epoch total loss 1.19047678\n",
      "Trained batch 450 batch loss 1.1918869 epoch total loss 1.19047987\n",
      "Trained batch 451 batch loss 1.3352108 epoch total loss 1.19080079\n",
      "Trained batch 452 batch loss 1.30151486 epoch total loss 1.19104576\n",
      "Trained batch 453 batch loss 1.27117586 epoch total loss 1.19122255\n",
      "Trained batch 454 batch loss 1.12636328 epoch total loss 1.19107974\n",
      "Trained batch 455 batch loss 1.16311121 epoch total loss 1.19101822\n",
      "Trained batch 456 batch loss 1.19547558 epoch total loss 1.191028\n",
      "Trained batch 457 batch loss 1.19356596 epoch total loss 1.19103348\n",
      "Trained batch 458 batch loss 1.09685862 epoch total loss 1.19082785\n",
      "Trained batch 459 batch loss 1.03578591 epoch total loss 1.19049\n",
      "Trained batch 460 batch loss 1.03285146 epoch total loss 1.19014728\n",
      "Trained batch 461 batch loss 1.11193705 epoch total loss 1.18997765\n",
      "Trained batch 462 batch loss 1.51997089 epoch total loss 1.19069195\n",
      "Trained batch 463 batch loss 1.23597026 epoch total loss 1.1907897\n",
      "Trained batch 464 batch loss 1.11259055 epoch total loss 1.19062126\n",
      "Trained batch 465 batch loss 1.12267828 epoch total loss 1.19047511\n",
      "Trained batch 466 batch loss 1.14067447 epoch total loss 1.19036829\n",
      "Trained batch 467 batch loss 1.12836206 epoch total loss 1.1902355\n",
      "Trained batch 468 batch loss 1.15084827 epoch total loss 1.19015121\n",
      "Trained batch 469 batch loss 1.11315823 epoch total loss 1.18998706\n",
      "Trained batch 470 batch loss 1.2198776 epoch total loss 1.1900506\n",
      "Trained batch 471 batch loss 1.19668913 epoch total loss 1.19006479\n",
      "Trained batch 472 batch loss 1.20165253 epoch total loss 1.19008934\n",
      "Trained batch 473 batch loss 1.20125425 epoch total loss 1.19011295\n",
      "Trained batch 474 batch loss 1.1812191 epoch total loss 1.19009411\n",
      "Trained batch 475 batch loss 1.05348468 epoch total loss 1.18980646\n",
      "Trained batch 476 batch loss 1.07521176 epoch total loss 1.18956566\n",
      "Trained batch 477 batch loss 1.13501179 epoch total loss 1.18945134\n",
      "Trained batch 478 batch loss 1.13168454 epoch total loss 1.18933058\n",
      "Trained batch 479 batch loss 1.15521038 epoch total loss 1.18925929\n",
      "Trained batch 480 batch loss 1.11655927 epoch total loss 1.18910789\n",
      "Trained batch 481 batch loss 0.982877076 epoch total loss 1.1886791\n",
      "Trained batch 482 batch loss 1.02695751 epoch total loss 1.18834364\n",
      "Trained batch 483 batch loss 0.963277936 epoch total loss 1.18787754\n",
      "Trained batch 484 batch loss 1.03932548 epoch total loss 1.18757057\n",
      "Trained batch 485 batch loss 1.12828231 epoch total loss 1.18744838\n",
      "Trained batch 486 batch loss 1.01667976 epoch total loss 1.18709695\n",
      "Trained batch 487 batch loss 1.07312524 epoch total loss 1.18686295\n",
      "Trained batch 488 batch loss 0.996875048 epoch total loss 1.18647361\n",
      "Trained batch 489 batch loss 1.1292851 epoch total loss 1.18635666\n",
      "Trained batch 490 batch loss 1.12591517 epoch total loss 1.18623328\n",
      "Trained batch 491 batch loss 1.33816087 epoch total loss 1.18654275\n",
      "Trained batch 492 batch loss 1.27572262 epoch total loss 1.18672395\n",
      "Trained batch 493 batch loss 1.23271132 epoch total loss 1.18681717\n",
      "Trained batch 494 batch loss 1.32780027 epoch total loss 1.18710268\n",
      "Trained batch 495 batch loss 1.29638839 epoch total loss 1.18732345\n",
      "Trained batch 496 batch loss 1.17456388 epoch total loss 1.1872977\n",
      "Trained batch 497 batch loss 1.22962916 epoch total loss 1.18738282\n",
      "Trained batch 498 batch loss 1.15504587 epoch total loss 1.18731785\n",
      "Trained batch 499 batch loss 1.28477979 epoch total loss 1.18751323\n",
      "Trained batch 500 batch loss 1.25236595 epoch total loss 1.18764293\n",
      "Trained batch 501 batch loss 1.22550702 epoch total loss 1.18771851\n",
      "Trained batch 502 batch loss 1.07128513 epoch total loss 1.18748665\n",
      "Trained batch 503 batch loss 1.07404625 epoch total loss 1.1872611\n",
      "Trained batch 504 batch loss 1.29802907 epoch total loss 1.18748081\n",
      "Trained batch 505 batch loss 1.3699584 epoch total loss 1.18784213\n",
      "Trained batch 506 batch loss 1.29813433 epoch total loss 1.18806016\n",
      "Trained batch 507 batch loss 1.24123216 epoch total loss 1.18816495\n",
      "Trained batch 508 batch loss 1.22656918 epoch total loss 1.18824065\n",
      "Trained batch 509 batch loss 1.25073779 epoch total loss 1.18836331\n",
      "Trained batch 510 batch loss 1.28431785 epoch total loss 1.18855143\n",
      "Trained batch 511 batch loss 1.15784073 epoch total loss 1.18849134\n",
      "Trained batch 512 batch loss 1.22563 epoch total loss 1.18856394\n",
      "Trained batch 513 batch loss 1.21464264 epoch total loss 1.18861485\n",
      "Trained batch 514 batch loss 1.14206302 epoch total loss 1.18852425\n",
      "Trained batch 515 batch loss 1.18701625 epoch total loss 1.18852139\n",
      "Trained batch 516 batch loss 1.17696536 epoch total loss 1.18849897\n",
      "Trained batch 517 batch loss 1.23691607 epoch total loss 1.18859255\n",
      "Trained batch 518 batch loss 1.28083277 epoch total loss 1.18877065\n",
      "Trained batch 519 batch loss 1.23336637 epoch total loss 1.18885648\n",
      "Trained batch 520 batch loss 1.29108858 epoch total loss 1.18905306\n",
      "Trained batch 521 batch loss 1.21303689 epoch total loss 1.18909907\n",
      "Trained batch 522 batch loss 1.22215104 epoch total loss 1.18916249\n",
      "Trained batch 523 batch loss 1.25346923 epoch total loss 1.1892854\n",
      "Trained batch 524 batch loss 1.16643178 epoch total loss 1.18924189\n",
      "Trained batch 525 batch loss 1.11976719 epoch total loss 1.18910944\n",
      "Trained batch 526 batch loss 1.14196992 epoch total loss 1.1890198\n",
      "Trained batch 527 batch loss 1.21387601 epoch total loss 1.18906701\n",
      "Trained batch 528 batch loss 1.16542101 epoch total loss 1.18902218\n",
      "Trained batch 529 batch loss 1.1884079 epoch total loss 1.18902099\n",
      "Trained batch 530 batch loss 1.27950609 epoch total loss 1.1891917\n",
      "Trained batch 531 batch loss 1.19456673 epoch total loss 1.18920183\n",
      "Trained batch 532 batch loss 1.30313468 epoch total loss 1.18941605\n",
      "Trained batch 533 batch loss 1.27994776 epoch total loss 1.18958592\n",
      "Trained batch 534 batch loss 1.35078967 epoch total loss 1.18988776\n",
      "Trained batch 535 batch loss 1.31644678 epoch total loss 1.19012439\n",
      "Trained batch 536 batch loss 1.12673628 epoch total loss 1.19000614\n",
      "Trained batch 537 batch loss 1.15156734 epoch total loss 1.18993449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 538 batch loss 1.12018335 epoch total loss 1.18980479\n",
      "Trained batch 539 batch loss 1.18055081 epoch total loss 1.18978763\n",
      "Trained batch 540 batch loss 1.06069899 epoch total loss 1.18954849\n",
      "Trained batch 541 batch loss 1.1722542 epoch total loss 1.18951654\n",
      "Trained batch 542 batch loss 1.15714192 epoch total loss 1.18945682\n",
      "Trained batch 543 batch loss 0.894424796 epoch total loss 1.18891346\n",
      "Trained batch 544 batch loss 0.905786216 epoch total loss 1.188393\n",
      "Trained batch 545 batch loss 1.14406383 epoch total loss 1.18831158\n",
      "Trained batch 546 batch loss 1.24587238 epoch total loss 1.18841696\n",
      "Trained batch 547 batch loss 1.37850177 epoch total loss 1.18876445\n",
      "Trained batch 548 batch loss 1.4186542 epoch total loss 1.18918395\n",
      "Trained batch 549 batch loss 1.27624679 epoch total loss 1.1893425\n",
      "Trained batch 550 batch loss 1.05026078 epoch total loss 1.18908954\n",
      "Trained batch 551 batch loss 1.15600038 epoch total loss 1.18902957\n",
      "Trained batch 552 batch loss 1.21092701 epoch total loss 1.18906927\n",
      "Trained batch 553 batch loss 1.23624575 epoch total loss 1.18915462\n",
      "Trained batch 554 batch loss 1.18370485 epoch total loss 1.18914473\n",
      "Trained batch 555 batch loss 1.25273645 epoch total loss 1.18925941\n",
      "Trained batch 556 batch loss 1.21478009 epoch total loss 1.18930531\n",
      "Trained batch 557 batch loss 1.121472 epoch total loss 1.18918347\n",
      "Trained batch 558 batch loss 1.17024946 epoch total loss 1.1891495\n",
      "Trained batch 559 batch loss 1.12887287 epoch total loss 1.18904161\n",
      "Trained batch 560 batch loss 1.15936708 epoch total loss 1.18898857\n",
      "Trained batch 561 batch loss 1.1305095 epoch total loss 1.18888438\n",
      "Trained batch 562 batch loss 1.18237102 epoch total loss 1.18887269\n",
      "Trained batch 563 batch loss 1.02691841 epoch total loss 1.18858504\n",
      "Trained batch 564 batch loss 1.11151767 epoch total loss 1.18844843\n",
      "Trained batch 565 batch loss 1.28784883 epoch total loss 1.18862438\n",
      "Trained batch 566 batch loss 1.22449958 epoch total loss 1.18868768\n",
      "Trained batch 567 batch loss 1.05507624 epoch total loss 1.18845201\n",
      "Trained batch 568 batch loss 1.01441681 epoch total loss 1.18814564\n",
      "Trained batch 569 batch loss 1.00746882 epoch total loss 1.18782806\n",
      "Trained batch 570 batch loss 1.15719116 epoch total loss 1.18777418\n",
      "Trained batch 571 batch loss 1.09162903 epoch total loss 1.18760586\n",
      "Trained batch 572 batch loss 1.09810793 epoch total loss 1.18744934\n",
      "Trained batch 573 batch loss 1.25826502 epoch total loss 1.18757284\n",
      "Trained batch 574 batch loss 1.15743065 epoch total loss 1.18752027\n",
      "Trained batch 575 batch loss 1.21606421 epoch total loss 1.18757\n",
      "Trained batch 576 batch loss 1.25314009 epoch total loss 1.1876837\n",
      "Trained batch 577 batch loss 1.23454559 epoch total loss 1.187765\n",
      "Trained batch 578 batch loss 1.45906854 epoch total loss 1.18823433\n",
      "Trained batch 579 batch loss 1.54179 epoch total loss 1.18884504\n",
      "Trained batch 580 batch loss 1.28612387 epoch total loss 1.18901277\n",
      "Trained batch 581 batch loss 1.06894588 epoch total loss 1.18880606\n",
      "Trained batch 582 batch loss 1.14194775 epoch total loss 1.18872559\n",
      "Trained batch 583 batch loss 1.37814784 epoch total loss 1.18905056\n",
      "Trained batch 584 batch loss 1.29451072 epoch total loss 1.18923116\n",
      "Trained batch 585 batch loss 1.32220626 epoch total loss 1.18945849\n",
      "Trained batch 586 batch loss 1.16083503 epoch total loss 1.18940961\n",
      "Trained batch 587 batch loss 1.11639178 epoch total loss 1.18928516\n",
      "Trained batch 588 batch loss 1.16627908 epoch total loss 1.18924606\n",
      "Trained batch 589 batch loss 1.11136866 epoch total loss 1.18911386\n",
      "Trained batch 590 batch loss 1.07150841 epoch total loss 1.18891454\n",
      "Trained batch 591 batch loss 1.17192459 epoch total loss 1.18888581\n",
      "Trained batch 592 batch loss 1.26863968 epoch total loss 1.18902051\n",
      "Trained batch 593 batch loss 1.2759347 epoch total loss 1.18916714\n",
      "Trained batch 594 batch loss 1.30280864 epoch total loss 1.18935835\n",
      "Trained batch 595 batch loss 1.26824164 epoch total loss 1.18949103\n",
      "Trained batch 596 batch loss 1.19157088 epoch total loss 1.18949449\n",
      "Trained batch 597 batch loss 1.11954284 epoch total loss 1.18937743\n",
      "Trained batch 598 batch loss 1.12505913 epoch total loss 1.18926978\n",
      "Trained batch 599 batch loss 1.0546068 epoch total loss 1.18904507\n",
      "Trained batch 600 batch loss 1.15326703 epoch total loss 1.18898535\n",
      "Trained batch 601 batch loss 1.31445277 epoch total loss 1.1891942\n",
      "Trained batch 602 batch loss 1.21737528 epoch total loss 1.18924093\n",
      "Trained batch 603 batch loss 1.21380281 epoch total loss 1.1892817\n",
      "Trained batch 604 batch loss 1.15008318 epoch total loss 1.18921673\n",
      "Trained batch 605 batch loss 1.19876933 epoch total loss 1.18923259\n",
      "Trained batch 606 batch loss 1.12184477 epoch total loss 1.18912137\n",
      "Trained batch 607 batch loss 1.09294474 epoch total loss 1.18896294\n",
      "Trained batch 608 batch loss 1.03604794 epoch total loss 1.18871152\n",
      "Trained batch 609 batch loss 1.10206127 epoch total loss 1.18856919\n",
      "Trained batch 610 batch loss 1.17570138 epoch total loss 1.18854809\n",
      "Trained batch 611 batch loss 1.27226675 epoch total loss 1.18868518\n",
      "Trained batch 612 batch loss 1.26429272 epoch total loss 1.18880868\n",
      "Trained batch 613 batch loss 1.1557759 epoch total loss 1.1887548\n",
      "Trained batch 614 batch loss 1.17457187 epoch total loss 1.18873167\n",
      "Trained batch 615 batch loss 1.14542341 epoch total loss 1.18866122\n",
      "Trained batch 616 batch loss 1.19906509 epoch total loss 1.18867815\n",
      "Trained batch 617 batch loss 1.21393132 epoch total loss 1.18871903\n",
      "Trained batch 618 batch loss 1.08020043 epoch total loss 1.18854344\n",
      "Trained batch 619 batch loss 1.15055847 epoch total loss 1.18848205\n",
      "Trained batch 620 batch loss 1.13158321 epoch total loss 1.18839037\n",
      "Trained batch 621 batch loss 1.17503095 epoch total loss 1.1883688\n",
      "Trained batch 622 batch loss 1.21900737 epoch total loss 1.18841803\n",
      "Trained batch 623 batch loss 1.2031033 epoch total loss 1.18844163\n",
      "Trained batch 624 batch loss 1.12043846 epoch total loss 1.18833268\n",
      "Trained batch 625 batch loss 1.02112234 epoch total loss 1.18806517\n",
      "Trained batch 626 batch loss 0.949587524 epoch total loss 1.18768418\n",
      "Trained batch 627 batch loss 0.882441044 epoch total loss 1.18719733\n",
      "Trained batch 628 batch loss 1.08659148 epoch total loss 1.18703723\n",
      "Trained batch 629 batch loss 1.06164646 epoch total loss 1.18683779\n",
      "Trained batch 630 batch loss 1.14163792 epoch total loss 1.18676615\n",
      "Trained batch 631 batch loss 1.08799243 epoch total loss 1.18660963\n",
      "Trained batch 632 batch loss 1.13691056 epoch total loss 1.18653095\n",
      "Trained batch 633 batch loss 1.18266332 epoch total loss 1.18652487\n",
      "Trained batch 634 batch loss 1.23786819 epoch total loss 1.18660581\n",
      "Trained batch 635 batch loss 1.1793685 epoch total loss 1.18659449\n",
      "Trained batch 636 batch loss 1.22282386 epoch total loss 1.18665147\n",
      "Trained batch 637 batch loss 1.36186767 epoch total loss 1.18692648\n",
      "Trained batch 638 batch loss 1.34229147 epoch total loss 1.18717\n",
      "Trained batch 639 batch loss 1.2828362 epoch total loss 1.18731976\n",
      "Trained batch 640 batch loss 1.18899703 epoch total loss 1.18732238\n",
      "Trained batch 641 batch loss 1.20901299 epoch total loss 1.18735623\n",
      "Trained batch 642 batch loss 1.17126179 epoch total loss 1.1873312\n",
      "Trained batch 643 batch loss 1.37610495 epoch total loss 1.18762469\n",
      "Trained batch 644 batch loss 1.12819827 epoch total loss 1.18753242\n",
      "Trained batch 645 batch loss 1.09544492 epoch total loss 1.18738961\n",
      "Trained batch 646 batch loss 1.1536932 epoch total loss 1.18733752\n",
      "Trained batch 647 batch loss 1.1401155 epoch total loss 1.18726456\n",
      "Trained batch 648 batch loss 1.03392 epoch total loss 1.18702793\n",
      "Trained batch 649 batch loss 1.14305544 epoch total loss 1.18696022\n",
      "Trained batch 650 batch loss 1.09346724 epoch total loss 1.18681633\n",
      "Trained batch 651 batch loss 1.09052587 epoch total loss 1.1866684\n",
      "Trained batch 652 batch loss 1.13805246 epoch total loss 1.18659389\n",
      "Trained batch 653 batch loss 1.07233799 epoch total loss 1.18641889\n",
      "Trained batch 654 batch loss 1.16016459 epoch total loss 1.18637872\n",
      "Trained batch 655 batch loss 1.20982921 epoch total loss 1.18641448\n",
      "Trained batch 656 batch loss 1.28621387 epoch total loss 1.18656659\n",
      "Trained batch 657 batch loss 1.21967387 epoch total loss 1.18661702\n",
      "Trained batch 658 batch loss 1.23596132 epoch total loss 1.186692\n",
      "Trained batch 659 batch loss 1.13587201 epoch total loss 1.18661487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 660 batch loss 1.07265651 epoch total loss 1.18644214\n",
      "Trained batch 661 batch loss 1.15162754 epoch total loss 1.18638945\n",
      "Trained batch 662 batch loss 1.12091935 epoch total loss 1.1862905\n",
      "Trained batch 663 batch loss 1.16033709 epoch total loss 1.1862514\n",
      "Trained batch 664 batch loss 1.21009445 epoch total loss 1.18628728\n",
      "Trained batch 665 batch loss 1.27656364 epoch total loss 1.18642306\n",
      "Trained batch 666 batch loss 1.24034 epoch total loss 1.18650401\n",
      "Trained batch 667 batch loss 1.16419852 epoch total loss 1.18647051\n",
      "Trained batch 668 batch loss 1.12332165 epoch total loss 1.18637609\n",
      "Trained batch 669 batch loss 1.23494232 epoch total loss 1.18644857\n",
      "Trained batch 670 batch loss 1.15084672 epoch total loss 1.18639541\n",
      "Trained batch 671 batch loss 1.22170305 epoch total loss 1.18644798\n",
      "Trained batch 672 batch loss 1.24100018 epoch total loss 1.18652928\n",
      "Trained batch 673 batch loss 1.26449776 epoch total loss 1.18664515\n",
      "Trained batch 674 batch loss 1.1453898 epoch total loss 1.18658388\n",
      "Trained batch 675 batch loss 1.11321378 epoch total loss 1.18647528\n",
      "Trained batch 676 batch loss 1.11552286 epoch total loss 1.18637025\n",
      "Trained batch 677 batch loss 1.06745541 epoch total loss 1.18619466\n",
      "Trained batch 678 batch loss 1.02396417 epoch total loss 1.18595541\n",
      "Trained batch 679 batch loss 1.05571842 epoch total loss 1.1857636\n",
      "Trained batch 680 batch loss 1.06480491 epoch total loss 1.18558574\n",
      "Trained batch 681 batch loss 1.05871546 epoch total loss 1.18539941\n",
      "Trained batch 682 batch loss 1.08546925 epoch total loss 1.1852529\n",
      "Trained batch 683 batch loss 1.15857339 epoch total loss 1.1852138\n",
      "Trained batch 684 batch loss 1.00364542 epoch total loss 1.18494844\n",
      "Trained batch 685 batch loss 1.17961574 epoch total loss 1.18494058\n",
      "Trained batch 686 batch loss 1.28027737 epoch total loss 1.18507957\n",
      "Trained batch 687 batch loss 1.18679833 epoch total loss 1.18508208\n",
      "Trained batch 688 batch loss 1.31766343 epoch total loss 1.18527484\n",
      "Trained batch 689 batch loss 1.2396214 epoch total loss 1.18535376\n",
      "Trained batch 690 batch loss 1.1463778 epoch total loss 1.18529725\n",
      "Trained batch 691 batch loss 1.20033157 epoch total loss 1.18531895\n",
      "Trained batch 692 batch loss 1.17080224 epoch total loss 1.18529797\n",
      "Trained batch 693 batch loss 1.06479275 epoch total loss 1.18512416\n",
      "Trained batch 694 batch loss 1.05915165 epoch total loss 1.1849426\n",
      "Trained batch 695 batch loss 1.22842205 epoch total loss 1.18500507\n",
      "Trained batch 696 batch loss 1.16869748 epoch total loss 1.1849817\n",
      "Trained batch 697 batch loss 1.14190757 epoch total loss 1.18491983\n",
      "Trained batch 698 batch loss 1.14502823 epoch total loss 1.18486273\n",
      "Trained batch 699 batch loss 1.33100414 epoch total loss 1.18507183\n",
      "Trained batch 700 batch loss 1.2840786 epoch total loss 1.18521321\n",
      "Trained batch 701 batch loss 1.26090121 epoch total loss 1.18532121\n",
      "Trained batch 702 batch loss 1.1960336 epoch total loss 1.18533647\n",
      "Trained batch 703 batch loss 1.13940477 epoch total loss 1.18527114\n",
      "Trained batch 704 batch loss 1.18121111 epoch total loss 1.18526542\n",
      "Trained batch 705 batch loss 1.27008867 epoch total loss 1.1853857\n",
      "Trained batch 706 batch loss 1.2435559 epoch total loss 1.18546808\n",
      "Trained batch 707 batch loss 1.44318736 epoch total loss 1.1858325\n",
      "Trained batch 708 batch loss 1.34555674 epoch total loss 1.18605816\n",
      "Trained batch 709 batch loss 1.26368213 epoch total loss 1.1861676\n",
      "Trained batch 710 batch loss 1.27396929 epoch total loss 1.18629134\n",
      "Trained batch 711 batch loss 1.27372479 epoch total loss 1.18641436\n",
      "Trained batch 712 batch loss 1.24689579 epoch total loss 1.18649924\n",
      "Trained batch 713 batch loss 1.39983678 epoch total loss 1.18679845\n",
      "Trained batch 714 batch loss 1.1536063 epoch total loss 1.18675196\n",
      "Trained batch 715 batch loss 1.0500356 epoch total loss 1.18656087\n",
      "Trained batch 716 batch loss 1.11179054 epoch total loss 1.18645644\n",
      "Trained batch 717 batch loss 1.25423956 epoch total loss 1.18655097\n",
      "Trained batch 718 batch loss 1.34028101 epoch total loss 1.18676507\n",
      "Trained batch 719 batch loss 1.32316434 epoch total loss 1.18695474\n",
      "Trained batch 720 batch loss 1.31830144 epoch total loss 1.18713713\n",
      "Trained batch 721 batch loss 1.17641842 epoch total loss 1.18712223\n",
      "Trained batch 722 batch loss 1.24796653 epoch total loss 1.18720663\n",
      "Trained batch 723 batch loss 1.31376982 epoch total loss 1.18738163\n",
      "Trained batch 724 batch loss 1.28739476 epoch total loss 1.18751979\n",
      "Trained batch 725 batch loss 1.28345513 epoch total loss 1.18765211\n",
      "Trained batch 726 batch loss 1.23049188 epoch total loss 1.18771112\n",
      "Trained batch 727 batch loss 1.21772599 epoch total loss 1.18775237\n",
      "Trained batch 728 batch loss 1.29929829 epoch total loss 1.18790567\n",
      "Trained batch 729 batch loss 1.30020678 epoch total loss 1.18805969\n",
      "Trained batch 730 batch loss 1.20965338 epoch total loss 1.18808925\n",
      "Trained batch 731 batch loss 1.23444104 epoch total loss 1.18815267\n",
      "Trained batch 732 batch loss 1.04696798 epoch total loss 1.18795979\n",
      "Trained batch 733 batch loss 1.07627702 epoch total loss 1.18780756\n",
      "Trained batch 734 batch loss 1.12024784 epoch total loss 1.18771541\n",
      "Trained batch 735 batch loss 1.14776778 epoch total loss 1.18766105\n",
      "Trained batch 736 batch loss 1.2505374 epoch total loss 1.18774652\n",
      "Trained batch 737 batch loss 1.29058897 epoch total loss 1.18788612\n",
      "Trained batch 738 batch loss 1.24624705 epoch total loss 1.18796515\n",
      "Trained batch 739 batch loss 1.13887417 epoch total loss 1.18789876\n",
      "Trained batch 740 batch loss 1.05639267 epoch total loss 1.18772101\n",
      "Trained batch 741 batch loss 1.0290184 epoch total loss 1.18750679\n",
      "Trained batch 742 batch loss 1.00359297 epoch total loss 1.18725896\n",
      "Trained batch 743 batch loss 1.24189734 epoch total loss 1.18733251\n",
      "Trained batch 744 batch loss 1.24091423 epoch total loss 1.18740451\n",
      "Trained batch 745 batch loss 1.16670978 epoch total loss 1.18737674\n",
      "Trained batch 746 batch loss 1.35659289 epoch total loss 1.18760347\n",
      "Trained batch 747 batch loss 1.2459023 epoch total loss 1.18768156\n",
      "Trained batch 748 batch loss 1.20442152 epoch total loss 1.18770397\n",
      "Trained batch 749 batch loss 1.16622281 epoch total loss 1.18767524\n",
      "Trained batch 750 batch loss 1.15663648 epoch total loss 1.18763375\n",
      "Trained batch 751 batch loss 1.13401127 epoch total loss 1.18756247\n",
      "Trained batch 752 batch loss 1.16194177 epoch total loss 1.18752837\n",
      "Trained batch 753 batch loss 0.924336314 epoch total loss 1.18717873\n",
      "Trained batch 754 batch loss 0.912430644 epoch total loss 1.18681431\n",
      "Trained batch 755 batch loss 1.17788124 epoch total loss 1.18680251\n",
      "Trained batch 756 batch loss 1.28654659 epoch total loss 1.18693447\n",
      "Trained batch 757 batch loss 1.34101796 epoch total loss 1.18713796\n",
      "Trained batch 758 batch loss 1.30089092 epoch total loss 1.18728805\n",
      "Trained batch 759 batch loss 1.28423476 epoch total loss 1.18741584\n",
      "Trained batch 760 batch loss 1.2274251 epoch total loss 1.18746841\n",
      "Trained batch 761 batch loss 1.12307918 epoch total loss 1.18738389\n",
      "Trained batch 762 batch loss 1.23146856 epoch total loss 1.18744171\n",
      "Trained batch 763 batch loss 1.26759446 epoch total loss 1.18754673\n",
      "Trained batch 764 batch loss 1.19963753 epoch total loss 1.18756258\n",
      "Trained batch 765 batch loss 1.1599431 epoch total loss 1.18752646\n",
      "Trained batch 766 batch loss 1.125875 epoch total loss 1.187446\n",
      "Trained batch 767 batch loss 1.09773064 epoch total loss 1.18732893\n",
      "Trained batch 768 batch loss 1.17008007 epoch total loss 1.18730652\n",
      "Trained batch 769 batch loss 1.07361603 epoch total loss 1.1871587\n",
      "Trained batch 770 batch loss 1.23460066 epoch total loss 1.18722034\n",
      "Trained batch 771 batch loss 1.08683264 epoch total loss 1.18709016\n",
      "Trained batch 772 batch loss 1.10623813 epoch total loss 1.18698549\n",
      "Trained batch 773 batch loss 1.10445547 epoch total loss 1.18687868\n",
      "Trained batch 774 batch loss 1.35623121 epoch total loss 1.18709743\n",
      "Trained batch 775 batch loss 1.23421097 epoch total loss 1.18715823\n",
      "Trained batch 776 batch loss 1.20783198 epoch total loss 1.18718481\n",
      "Trained batch 777 batch loss 1.17358398 epoch total loss 1.18716729\n",
      "Trained batch 778 batch loss 0.84722209 epoch total loss 1.18673038\n",
      "Trained batch 779 batch loss 0.842659295 epoch total loss 1.18628871\n",
      "Trained batch 780 batch loss 0.881125093 epoch total loss 1.18589747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 781 batch loss 1.00843632 epoch total loss 1.18567014\n",
      "Trained batch 782 batch loss 0.996464 epoch total loss 1.18542826\n",
      "Trained batch 783 batch loss 1.18431854 epoch total loss 1.18542683\n",
      "Trained batch 784 batch loss 1.26288879 epoch total loss 1.18552566\n",
      "Trained batch 785 batch loss 1.26115203 epoch total loss 1.18562198\n",
      "Trained batch 786 batch loss 1.24852276 epoch total loss 1.18570197\n",
      "Trained batch 787 batch loss 1.25481451 epoch total loss 1.18578982\n",
      "Trained batch 788 batch loss 1.23220444 epoch total loss 1.18584871\n",
      "Trained batch 789 batch loss 1.20438027 epoch total loss 1.1858722\n",
      "Trained batch 790 batch loss 1.34536695 epoch total loss 1.18607414\n",
      "Trained batch 791 batch loss 1.26233065 epoch total loss 1.18617046\n",
      "Trained batch 792 batch loss 1.21275783 epoch total loss 1.18620408\n",
      "Trained batch 793 batch loss 1.253582 epoch total loss 1.18628907\n",
      "Trained batch 794 batch loss 1.30335569 epoch total loss 1.18643653\n",
      "Trained batch 795 batch loss 1.27227616 epoch total loss 1.18654442\n",
      "Trained batch 796 batch loss 1.1466403 epoch total loss 1.18649435\n",
      "Trained batch 797 batch loss 1.18670344 epoch total loss 1.18649459\n",
      "Trained batch 798 batch loss 1.27277017 epoch total loss 1.18660271\n",
      "Trained batch 799 batch loss 1.21638036 epoch total loss 1.18664\n",
      "Trained batch 800 batch loss 1.28685355 epoch total loss 1.18676531\n",
      "Trained batch 801 batch loss 1.08073139 epoch total loss 1.18663299\n",
      "Trained batch 802 batch loss 1.14019942 epoch total loss 1.18657506\n",
      "Trained batch 803 batch loss 1.15734124 epoch total loss 1.1865387\n",
      "Trained batch 804 batch loss 1.14526856 epoch total loss 1.18648732\n",
      "Trained batch 805 batch loss 1.17966163 epoch total loss 1.18647885\n",
      "Trained batch 806 batch loss 1.21982014 epoch total loss 1.18652022\n",
      "Trained batch 807 batch loss 1.27894461 epoch total loss 1.18663478\n",
      "Trained batch 808 batch loss 1.25307262 epoch total loss 1.18671691\n",
      "Trained batch 809 batch loss 1.10533834 epoch total loss 1.18661642\n",
      "Trained batch 810 batch loss 1.23692918 epoch total loss 1.18667853\n",
      "Trained batch 811 batch loss 1.03926301 epoch total loss 1.18649673\n",
      "Trained batch 812 batch loss 1.21432233 epoch total loss 1.18653095\n",
      "Trained batch 813 batch loss 1.05728054 epoch total loss 1.18637192\n",
      "Trained batch 814 batch loss 1.12777734 epoch total loss 1.1863\n",
      "Trained batch 815 batch loss 1.20165694 epoch total loss 1.18631887\n",
      "Trained batch 816 batch loss 1.31498277 epoch total loss 1.18647659\n",
      "Trained batch 817 batch loss 1.46525621 epoch total loss 1.18681777\n",
      "Trained batch 818 batch loss 1.27588987 epoch total loss 1.1869266\n",
      "Trained batch 819 batch loss 1.38301718 epoch total loss 1.18716609\n",
      "Trained batch 820 batch loss 1.14301682 epoch total loss 1.18711221\n",
      "Trained batch 821 batch loss 1.03637242 epoch total loss 1.18692863\n",
      "Trained batch 822 batch loss 0.948242784 epoch total loss 1.18663824\n",
      "Trained batch 823 batch loss 0.925536513 epoch total loss 1.18632102\n",
      "Trained batch 824 batch loss 0.994233131 epoch total loss 1.18608785\n",
      "Trained batch 825 batch loss 1.07332659 epoch total loss 1.18595123\n",
      "Trained batch 826 batch loss 1.11940396 epoch total loss 1.18587065\n",
      "Trained batch 827 batch loss 1.222404 epoch total loss 1.18591475\n",
      "Trained batch 828 batch loss 1.12253809 epoch total loss 1.18583822\n",
      "Trained batch 829 batch loss 1.14810634 epoch total loss 1.1857928\n",
      "Trained batch 830 batch loss 1.10681367 epoch total loss 1.18569767\n",
      "Trained batch 831 batch loss 1.07184947 epoch total loss 1.18556058\n",
      "Trained batch 832 batch loss 1.14374399 epoch total loss 1.1855104\n",
      "Trained batch 833 batch loss 1.14342 epoch total loss 1.18545985\n",
      "Trained batch 834 batch loss 1.12032366 epoch total loss 1.18538165\n",
      "Trained batch 835 batch loss 1.22014725 epoch total loss 1.18542337\n",
      "Trained batch 836 batch loss 1.13053346 epoch total loss 1.18535769\n",
      "Trained batch 837 batch loss 1.2158097 epoch total loss 1.18539405\n",
      "Trained batch 838 batch loss 1.13735437 epoch total loss 1.18533671\n",
      "Trained batch 839 batch loss 1.18727815 epoch total loss 1.18533909\n",
      "Trained batch 840 batch loss 1.24897695 epoch total loss 1.18541479\n",
      "Trained batch 841 batch loss 1.23781443 epoch total loss 1.18547702\n",
      "Trained batch 842 batch loss 1.16200912 epoch total loss 1.18544912\n",
      "Trained batch 843 batch loss 1.1756413 epoch total loss 1.18543756\n",
      "Trained batch 844 batch loss 1.12537432 epoch total loss 1.18536639\n",
      "Trained batch 845 batch loss 1.15361774 epoch total loss 1.18532884\n",
      "Trained batch 846 batch loss 1.24500084 epoch total loss 1.18539929\n",
      "Trained batch 847 batch loss 1.13790321 epoch total loss 1.18534327\n",
      "Trained batch 848 batch loss 1.18455744 epoch total loss 1.18534231\n",
      "Trained batch 849 batch loss 1.24956524 epoch total loss 1.18541801\n",
      "Trained batch 850 batch loss 1.10641789 epoch total loss 1.18532503\n",
      "Trained batch 851 batch loss 1.12470925 epoch total loss 1.18525386\n",
      "Trained batch 852 batch loss 1.15128958 epoch total loss 1.18521392\n",
      "Trained batch 853 batch loss 1.05463517 epoch total loss 1.18506086\n",
      "Trained batch 854 batch loss 1.15285373 epoch total loss 1.18502319\n",
      "Trained batch 855 batch loss 0.968086183 epoch total loss 1.18476939\n",
      "Trained batch 856 batch loss 0.931897223 epoch total loss 1.18447399\n",
      "Trained batch 857 batch loss 1.00693786 epoch total loss 1.18426681\n",
      "Trained batch 858 batch loss 1.13246346 epoch total loss 1.18420649\n",
      "Trained batch 859 batch loss 1.02203262 epoch total loss 1.18401766\n",
      "Trained batch 860 batch loss 1.0566783 epoch total loss 1.1838696\n",
      "Trained batch 861 batch loss 1.06299067 epoch total loss 1.18372917\n",
      "Trained batch 862 batch loss 1.09563947 epoch total loss 1.18362701\n",
      "Trained batch 863 batch loss 1.17726302 epoch total loss 1.18361962\n",
      "Trained batch 864 batch loss 1.33251214 epoch total loss 1.183792\n",
      "Trained batch 865 batch loss 1.16770601 epoch total loss 1.1837734\n",
      "Trained batch 866 batch loss 1.31266952 epoch total loss 1.18392217\n",
      "Trained batch 867 batch loss 1.25698924 epoch total loss 1.18400645\n",
      "Trained batch 868 batch loss 1.35754704 epoch total loss 1.18420637\n",
      "Trained batch 869 batch loss 1.29272473 epoch total loss 1.18433118\n",
      "Trained batch 870 batch loss 1.19942093 epoch total loss 1.18434858\n",
      "Trained batch 871 batch loss 1.35479903 epoch total loss 1.18454444\n",
      "Trained batch 872 batch loss 1.27288163 epoch total loss 1.18464565\n",
      "Trained batch 873 batch loss 1.349401 epoch total loss 1.18483436\n",
      "Trained batch 874 batch loss 1.24514723 epoch total loss 1.18490326\n",
      "Trained batch 875 batch loss 1.13416779 epoch total loss 1.18484533\n",
      "Trained batch 876 batch loss 1.24034381 epoch total loss 1.18490863\n",
      "Trained batch 877 batch loss 1.16148925 epoch total loss 1.18488193\n",
      "Trained batch 878 batch loss 1.29511273 epoch total loss 1.18500757\n",
      "Trained batch 879 batch loss 1.23243427 epoch total loss 1.18506145\n",
      "Trained batch 880 batch loss 1.26794696 epoch total loss 1.18515575\n",
      "Trained batch 881 batch loss 1.23551905 epoch total loss 1.18521285\n",
      "Trained batch 882 batch loss 1.23293614 epoch total loss 1.18526685\n",
      "Trained batch 883 batch loss 1.29694557 epoch total loss 1.18539345\n",
      "Trained batch 884 batch loss 1.12581074 epoch total loss 1.1853261\n",
      "Trained batch 885 batch loss 1.05760276 epoch total loss 1.18518174\n",
      "Trained batch 886 batch loss 1.07683277 epoch total loss 1.18505943\n",
      "Trained batch 887 batch loss 1.25021362 epoch total loss 1.18513286\n",
      "Trained batch 888 batch loss 1.25997567 epoch total loss 1.18521726\n",
      "Trained batch 889 batch loss 1.18320119 epoch total loss 1.185215\n",
      "Trained batch 890 batch loss 1.26702905 epoch total loss 1.18530703\n",
      "Trained batch 891 batch loss 1.12922585 epoch total loss 1.18524408\n",
      "Trained batch 892 batch loss 1.20261574 epoch total loss 1.18526363\n",
      "Trained batch 893 batch loss 1.17402089 epoch total loss 1.18525112\n",
      "Trained batch 894 batch loss 1.18706107 epoch total loss 1.18525302\n",
      "Trained batch 895 batch loss 1.1292721 epoch total loss 1.18519044\n",
      "Trained batch 896 batch loss 1.12220383 epoch total loss 1.18512022\n",
      "Trained batch 897 batch loss 1.13181543 epoch total loss 1.18506074\n",
      "Trained batch 898 batch loss 1.2032212 epoch total loss 1.18508101\n",
      "Trained batch 899 batch loss 1.15588641 epoch total loss 1.18504858\n",
      "Trained batch 900 batch loss 1.22429264 epoch total loss 1.18509209\n",
      "Trained batch 901 batch loss 1.25382578 epoch total loss 1.18516839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 902 batch loss 1.1132071 epoch total loss 1.18508852\n",
      "Trained batch 903 batch loss 1.16016591 epoch total loss 1.18506086\n",
      "Trained batch 904 batch loss 1.16412294 epoch total loss 1.18503761\n",
      "Trained batch 905 batch loss 1.26194036 epoch total loss 1.18512261\n",
      "Trained batch 906 batch loss 1.12430334 epoch total loss 1.18505549\n",
      "Trained batch 907 batch loss 1.17778587 epoch total loss 1.18504739\n",
      "Trained batch 908 batch loss 1.12367702 epoch total loss 1.1849798\n",
      "Trained batch 909 batch loss 1.03550446 epoch total loss 1.18481541\n",
      "Trained batch 910 batch loss 1.2415663 epoch total loss 1.18487775\n",
      "Trained batch 911 batch loss 1.25359941 epoch total loss 1.18495309\n",
      "Trained batch 912 batch loss 1.15082479 epoch total loss 1.18491578\n",
      "Trained batch 913 batch loss 1.30815029 epoch total loss 1.18505073\n",
      "Trained batch 914 batch loss 1.28501987 epoch total loss 1.18516\n",
      "Trained batch 915 batch loss 1.13540757 epoch total loss 1.18510568\n",
      "Trained batch 916 batch loss 1.02215266 epoch total loss 1.1849277\n",
      "Trained batch 917 batch loss 1.08978045 epoch total loss 1.18482387\n",
      "Trained batch 918 batch loss 1.06011939 epoch total loss 1.18468797\n",
      "Trained batch 919 batch loss 1.23340189 epoch total loss 1.18474102\n",
      "Trained batch 920 batch loss 1.21241736 epoch total loss 1.18477106\n",
      "Trained batch 921 batch loss 1.18381524 epoch total loss 1.18477011\n",
      "Trained batch 922 batch loss 1.20011258 epoch total loss 1.18478668\n",
      "Trained batch 923 batch loss 1.14871037 epoch total loss 1.18474758\n",
      "Trained batch 924 batch loss 1.13903177 epoch total loss 1.1846981\n",
      "Trained batch 925 batch loss 1.17391038 epoch total loss 1.18468642\n",
      "Trained batch 926 batch loss 1.1267072 epoch total loss 1.18462384\n",
      "Trained batch 927 batch loss 1.33478642 epoch total loss 1.18478584\n",
      "Trained batch 928 batch loss 1.33269954 epoch total loss 1.18494523\n",
      "Trained batch 929 batch loss 1.07181644 epoch total loss 1.18482339\n",
      "Trained batch 930 batch loss 1.05729795 epoch total loss 1.18468618\n",
      "Trained batch 931 batch loss 1.0658257 epoch total loss 1.18455851\n",
      "Trained batch 932 batch loss 1.16320682 epoch total loss 1.18453562\n",
      "Trained batch 933 batch loss 1.13979495 epoch total loss 1.18448758\n",
      "Trained batch 934 batch loss 1.1391542 epoch total loss 1.18443906\n",
      "Trained batch 935 batch loss 1.1013335 epoch total loss 1.18435025\n",
      "Trained batch 936 batch loss 1.1370455 epoch total loss 1.18429971\n",
      "Trained batch 937 batch loss 1.13464904 epoch total loss 1.18424666\n",
      "Trained batch 938 batch loss 1.09348452 epoch total loss 1.18415\n",
      "Trained batch 939 batch loss 1.30335653 epoch total loss 1.18427694\n",
      "Trained batch 940 batch loss 1.27144945 epoch total loss 1.18436968\n",
      "Trained batch 941 batch loss 1.30063844 epoch total loss 1.1844933\n",
      "Trained batch 942 batch loss 1.30498993 epoch total loss 1.1846211\n",
      "Trained batch 943 batch loss 1.19012523 epoch total loss 1.18462706\n",
      "Trained batch 944 batch loss 1.14115679 epoch total loss 1.18458092\n",
      "Trained batch 945 batch loss 1.1945883 epoch total loss 1.18459153\n",
      "Trained batch 946 batch loss 1.1832087 epoch total loss 1.1845901\n",
      "Trained batch 947 batch loss 1.30702281 epoch total loss 1.18471932\n",
      "Trained batch 948 batch loss 1.20201421 epoch total loss 1.18473756\n",
      "Trained batch 949 batch loss 1.21570897 epoch total loss 1.18477023\n",
      "Trained batch 950 batch loss 1.23106229 epoch total loss 1.18481898\n",
      "Trained batch 951 batch loss 1.40858352 epoch total loss 1.18505418\n",
      "Trained batch 952 batch loss 1.33017778 epoch total loss 1.18520665\n",
      "Trained batch 953 batch loss 1.20033813 epoch total loss 1.18522251\n",
      "Trained batch 954 batch loss 1.11222303 epoch total loss 1.18514597\n",
      "Trained batch 955 batch loss 1.02584696 epoch total loss 1.1849792\n",
      "Trained batch 956 batch loss 1.05754232 epoch total loss 1.18484581\n",
      "Trained batch 957 batch loss 1.26713884 epoch total loss 1.18493176\n",
      "Trained batch 958 batch loss 1.25026202 epoch total loss 1.185\n",
      "Trained batch 959 batch loss 1.23270285 epoch total loss 1.18504965\n",
      "Trained batch 960 batch loss 1.24060869 epoch total loss 1.18510759\n",
      "Trained batch 961 batch loss 1.33590257 epoch total loss 1.18526447\n",
      "Trained batch 962 batch loss 1.35644066 epoch total loss 1.18544245\n",
      "Trained batch 963 batch loss 1.15544033 epoch total loss 1.18541121\n",
      "Trained batch 964 batch loss 1.23585129 epoch total loss 1.18546355\n",
      "Trained batch 965 batch loss 1.30377281 epoch total loss 1.18558621\n",
      "Trained batch 966 batch loss 1.15562785 epoch total loss 1.18555522\n",
      "Trained batch 967 batch loss 1.15420389 epoch total loss 1.18552279\n",
      "Trained batch 968 batch loss 1.08580446 epoch total loss 1.1854198\n",
      "Trained batch 969 batch loss 0.963860512 epoch total loss 1.18519115\n",
      "Trained batch 970 batch loss 1.03830957 epoch total loss 1.18503976\n",
      "Trained batch 971 batch loss 1.13805556 epoch total loss 1.18499136\n",
      "Trained batch 972 batch loss 1.21710026 epoch total loss 1.18502426\n",
      "Trained batch 973 batch loss 1.2113589 epoch total loss 1.18505132\n",
      "Trained batch 974 batch loss 1.26082277 epoch total loss 1.18512917\n",
      "Trained batch 975 batch loss 1.30342972 epoch total loss 1.18525052\n",
      "Trained batch 976 batch loss 1.36345482 epoch total loss 1.18543303\n",
      "Trained batch 977 batch loss 1.24234951 epoch total loss 1.18549132\n",
      "Trained batch 978 batch loss 1.11944127 epoch total loss 1.18542373\n",
      "Trained batch 979 batch loss 1.19351685 epoch total loss 1.18543196\n",
      "Trained batch 980 batch loss 1.32325661 epoch total loss 1.1855725\n",
      "Trained batch 981 batch loss 1.18885016 epoch total loss 1.18557584\n",
      "Trained batch 982 batch loss 1.19020247 epoch total loss 1.18558061\n",
      "Trained batch 983 batch loss 1.24855447 epoch total loss 1.18564463\n",
      "Trained batch 984 batch loss 1.20531535 epoch total loss 1.18566465\n",
      "Trained batch 985 batch loss 1.13183093 epoch total loss 1.18560994\n",
      "Trained batch 986 batch loss 1.22357297 epoch total loss 1.18564856\n",
      "Trained batch 987 batch loss 1.21201837 epoch total loss 1.18567526\n",
      "Trained batch 988 batch loss 1.27791929 epoch total loss 1.1857686\n",
      "Trained batch 989 batch loss 1.19713426 epoch total loss 1.18578017\n",
      "Trained batch 990 batch loss 1.13069677 epoch total loss 1.1857245\n",
      "Trained batch 991 batch loss 1.08968472 epoch total loss 1.1856277\n",
      "Trained batch 992 batch loss 0.954258561 epoch total loss 1.18539441\n",
      "Trained batch 993 batch loss 1.08769143 epoch total loss 1.18529594\n",
      "Trained batch 994 batch loss 1.06732965 epoch total loss 1.18517733\n",
      "Trained batch 995 batch loss 1.09393406 epoch total loss 1.18508565\n",
      "Trained batch 996 batch loss 1.08461952 epoch total loss 1.1849848\n",
      "Trained batch 997 batch loss 1.01402307 epoch total loss 1.18481338\n",
      "Trained batch 998 batch loss 1.17058635 epoch total loss 1.18479908\n",
      "Trained batch 999 batch loss 1.13041973 epoch total loss 1.1847446\n",
      "Trained batch 1000 batch loss 1.17848635 epoch total loss 1.18473828\n",
      "Trained batch 1001 batch loss 1.32594573 epoch total loss 1.1848793\n",
      "Trained batch 1002 batch loss 1.27674901 epoch total loss 1.18497097\n",
      "Trained batch 1003 batch loss 1.19637084 epoch total loss 1.18498242\n",
      "Trained batch 1004 batch loss 1.06599188 epoch total loss 1.18486392\n",
      "Trained batch 1005 batch loss 1.27534318 epoch total loss 1.18495405\n",
      "Trained batch 1006 batch loss 1.33811164 epoch total loss 1.18510628\n",
      "Trained batch 1007 batch loss 1.06173444 epoch total loss 1.18498385\n",
      "Trained batch 1008 batch loss 1.02094245 epoch total loss 1.18482113\n",
      "Trained batch 1009 batch loss 1.11364794 epoch total loss 1.18475056\n",
      "Trained batch 1010 batch loss 1.07234621 epoch total loss 1.18463933\n",
      "Trained batch 1011 batch loss 1.13950968 epoch total loss 1.18459475\n",
      "Trained batch 1012 batch loss 1.09549975 epoch total loss 1.18450665\n",
      "Trained batch 1013 batch loss 1.16637361 epoch total loss 1.18448877\n",
      "Trained batch 1014 batch loss 1.20169938 epoch total loss 1.1845057\n",
      "Trained batch 1015 batch loss 1.36954057 epoch total loss 1.18468797\n",
      "Trained batch 1016 batch loss 1.16541028 epoch total loss 1.1846689\n",
      "Trained batch 1017 batch loss 1.27182865 epoch total loss 1.18475473\n",
      "Trained batch 1018 batch loss 1.29875386 epoch total loss 1.18486667\n",
      "Trained batch 1019 batch loss 1.31191969 epoch total loss 1.18499124\n",
      "Trained batch 1020 batch loss 1.18889284 epoch total loss 1.18499506\n",
      "Trained batch 1021 batch loss 1.21439624 epoch total loss 1.18502378\n",
      "Trained batch 1022 batch loss 1.08568752 epoch total loss 1.18492663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1023 batch loss 1.12652576 epoch total loss 1.18486941\n",
      "Trained batch 1024 batch loss 1.15605259 epoch total loss 1.18484128\n",
      "Trained batch 1025 batch loss 1.20760345 epoch total loss 1.18486357\n",
      "Trained batch 1026 batch loss 1.17714262 epoch total loss 1.18485594\n",
      "Trained batch 1027 batch loss 1.17656422 epoch total loss 1.18484783\n",
      "Trained batch 1028 batch loss 1.23919213 epoch total loss 1.18490064\n",
      "Trained batch 1029 batch loss 1.2519666 epoch total loss 1.18496585\n",
      "Trained batch 1030 batch loss 1.22489834 epoch total loss 1.18500459\n",
      "Trained batch 1031 batch loss 1.20436573 epoch total loss 1.18502331\n",
      "Trained batch 1032 batch loss 1.13279974 epoch total loss 1.18497276\n",
      "Trained batch 1033 batch loss 1.04423904 epoch total loss 1.18483639\n",
      "Trained batch 1034 batch loss 1.18621361 epoch total loss 1.1848377\n",
      "Trained batch 1035 batch loss 1.20350552 epoch total loss 1.1848557\n",
      "Trained batch 1036 batch loss 1.42545545 epoch total loss 1.18508792\n",
      "Trained batch 1037 batch loss 1.24496579 epoch total loss 1.18514574\n",
      "Trained batch 1038 batch loss 1.18208563 epoch total loss 1.18514276\n",
      "Trained batch 1039 batch loss 1.16801465 epoch total loss 1.1851263\n",
      "Trained batch 1040 batch loss 1.15195739 epoch total loss 1.18509436\n",
      "Trained batch 1041 batch loss 1.17846918 epoch total loss 1.18508804\n",
      "Trained batch 1042 batch loss 1.24807191 epoch total loss 1.18514848\n",
      "Trained batch 1043 batch loss 1.23955226 epoch total loss 1.18520057\n",
      "Trained batch 1044 batch loss 1.38523662 epoch total loss 1.18539214\n",
      "Trained batch 1045 batch loss 1.26984859 epoch total loss 1.18547308\n",
      "Trained batch 1046 batch loss 1.19822955 epoch total loss 1.18548524\n",
      "Trained batch 1047 batch loss 1.18266988 epoch total loss 1.1854825\n",
      "Trained batch 1048 batch loss 1.17705095 epoch total loss 1.1854744\n",
      "Trained batch 1049 batch loss 1.07113695 epoch total loss 1.18536544\n",
      "Trained batch 1050 batch loss 1.24064541 epoch total loss 1.18541801\n",
      "Trained batch 1051 batch loss 1.194695 epoch total loss 1.18542695\n",
      "Trained batch 1052 batch loss 1.14195645 epoch total loss 1.18538558\n",
      "Trained batch 1053 batch loss 1.04200327 epoch total loss 1.18524945\n",
      "Trained batch 1054 batch loss 0.984857798 epoch total loss 1.18505931\n",
      "Trained batch 1055 batch loss 0.971097469 epoch total loss 1.18485641\n",
      "Trained batch 1056 batch loss 1.02227414 epoch total loss 1.1847024\n",
      "Trained batch 1057 batch loss 1.3529408 epoch total loss 1.18486154\n",
      "Trained batch 1058 batch loss 1.4351145 epoch total loss 1.18509805\n",
      "Trained batch 1059 batch loss 1.43682766 epoch total loss 1.18533576\n",
      "Trained batch 1060 batch loss 1.30155087 epoch total loss 1.18544531\n",
      "Trained batch 1061 batch loss 1.42603743 epoch total loss 1.18567204\n",
      "Trained batch 1062 batch loss 1.30137765 epoch total loss 1.185781\n",
      "Trained batch 1063 batch loss 1.18546391 epoch total loss 1.18578064\n",
      "Trained batch 1064 batch loss 1.35626888 epoch total loss 1.18594098\n",
      "Trained batch 1065 batch loss 1.38304079 epoch total loss 1.18612599\n",
      "Trained batch 1066 batch loss 1.31682706 epoch total loss 1.18624866\n",
      "Trained batch 1067 batch loss 1.37052774 epoch total loss 1.18642128\n",
      "Trained batch 1068 batch loss 1.28432024 epoch total loss 1.18651295\n",
      "Trained batch 1069 batch loss 1.25743103 epoch total loss 1.18657923\n",
      "Trained batch 1070 batch loss 1.21836698 epoch total loss 1.18660903\n",
      "Trained batch 1071 batch loss 1.15758538 epoch total loss 1.18658185\n",
      "Trained batch 1072 batch loss 1.34676826 epoch total loss 1.18673134\n",
      "Trained batch 1073 batch loss 1.21217275 epoch total loss 1.18675506\n",
      "Trained batch 1074 batch loss 1.13924074 epoch total loss 1.18671083\n",
      "Trained batch 1075 batch loss 1.23278582 epoch total loss 1.18675375\n",
      "Trained batch 1076 batch loss 1.21299243 epoch total loss 1.18677819\n",
      "Trained batch 1077 batch loss 1.15367949 epoch total loss 1.18674743\n",
      "Trained batch 1078 batch loss 1.11702383 epoch total loss 1.18668282\n",
      "Trained batch 1079 batch loss 0.985497534 epoch total loss 1.18649626\n",
      "Trained batch 1080 batch loss 1.01619077 epoch total loss 1.18633866\n",
      "Trained batch 1081 batch loss 1.1826576 epoch total loss 1.18633521\n",
      "Trained batch 1082 batch loss 1.308272 epoch total loss 1.18644786\n",
      "Trained batch 1083 batch loss 1.36828446 epoch total loss 1.18661571\n",
      "Trained batch 1084 batch loss 1.26809132 epoch total loss 1.18669093\n",
      "Trained batch 1085 batch loss 1.18942857 epoch total loss 1.18669343\n",
      "Trained batch 1086 batch loss 1.11163664 epoch total loss 1.18662441\n",
      "Trained batch 1087 batch loss 1.22799194 epoch total loss 1.18666244\n",
      "Trained batch 1088 batch loss 1.15822351 epoch total loss 1.18663633\n",
      "Trained batch 1089 batch loss 1.16602266 epoch total loss 1.18661737\n",
      "Trained batch 1090 batch loss 1.08798575 epoch total loss 1.18652689\n",
      "Trained batch 1091 batch loss 1.18800545 epoch total loss 1.18652821\n",
      "Trained batch 1092 batch loss 1.11168516 epoch total loss 1.18645966\n",
      "Trained batch 1093 batch loss 1.06647587 epoch total loss 1.18635\n",
      "Trained batch 1094 batch loss 0.983184218 epoch total loss 1.18616426\n",
      "Trained batch 1095 batch loss 0.919804931 epoch total loss 1.18592095\n",
      "Trained batch 1096 batch loss 1.2476871 epoch total loss 1.18597734\n",
      "Trained batch 1097 batch loss 1.26243949 epoch total loss 1.18604708\n",
      "Trained batch 1098 batch loss 1.18278432 epoch total loss 1.1860441\n",
      "Trained batch 1099 batch loss 1.18872106 epoch total loss 1.18604648\n",
      "Trained batch 1100 batch loss 1.09579039 epoch total loss 1.18596447\n",
      "Trained batch 1101 batch loss 1.03836048 epoch total loss 1.18583035\n",
      "Trained batch 1102 batch loss 1.09899318 epoch total loss 1.18575156\n",
      "Trained batch 1103 batch loss 1.1410073 epoch total loss 1.18571103\n",
      "Trained batch 1104 batch loss 1.13595676 epoch total loss 1.18566597\n",
      "Trained batch 1105 batch loss 1.23756146 epoch total loss 1.18571293\n",
      "Trained batch 1106 batch loss 1.12257493 epoch total loss 1.18565583\n",
      "Trained batch 1107 batch loss 1.16028106 epoch total loss 1.18563294\n",
      "Trained batch 1108 batch loss 1.11444855 epoch total loss 1.18556869\n",
      "Trained batch 1109 batch loss 1.18571234 epoch total loss 1.18556881\n",
      "Trained batch 1110 batch loss 1.05682254 epoch total loss 1.1854527\n",
      "Trained batch 1111 batch loss 1.16793406 epoch total loss 1.18543696\n",
      "Trained batch 1112 batch loss 1.49800193 epoch total loss 1.18571818\n",
      "Trained batch 1113 batch loss 1.29752028 epoch total loss 1.18581855\n",
      "Trained batch 1114 batch loss 1.39028513 epoch total loss 1.18600202\n",
      "Trained batch 1115 batch loss 1.2052381 epoch total loss 1.1860193\n",
      "Trained batch 1116 batch loss 1.20654476 epoch total loss 1.18603766\n",
      "Trained batch 1117 batch loss 1.24473202 epoch total loss 1.18609023\n",
      "Trained batch 1118 batch loss 1.22456491 epoch total loss 1.18612468\n",
      "Trained batch 1119 batch loss 1.24184084 epoch total loss 1.18617451\n",
      "Trained batch 1120 batch loss 1.15577364 epoch total loss 1.18614733\n",
      "Trained batch 1121 batch loss 1.1613524 epoch total loss 1.18612516\n",
      "Trained batch 1122 batch loss 1.22360802 epoch total loss 1.18615866\n",
      "Trained batch 1123 batch loss 1.2128799 epoch total loss 1.1861825\n",
      "Trained batch 1124 batch loss 1.23361695 epoch total loss 1.1862247\n",
      "Trained batch 1125 batch loss 1.12998307 epoch total loss 1.18617475\n",
      "Trained batch 1126 batch loss 1.12142873 epoch total loss 1.18611717\n",
      "Trained batch 1127 batch loss 1.20135796 epoch total loss 1.18613076\n",
      "Trained batch 1128 batch loss 1.17527318 epoch total loss 1.18612123\n",
      "Trained batch 1129 batch loss 1.08541536 epoch total loss 1.18603206\n",
      "Trained batch 1130 batch loss 1.14123976 epoch total loss 1.18599236\n",
      "Trained batch 1131 batch loss 1.11661398 epoch total loss 1.18593097\n",
      "Trained batch 1132 batch loss 1.16093659 epoch total loss 1.18590891\n",
      "Trained batch 1133 batch loss 1.05201674 epoch total loss 1.18579066\n",
      "Trained batch 1134 batch loss 1.23633444 epoch total loss 1.18583524\n",
      "Trained batch 1135 batch loss 1.05719936 epoch total loss 1.18572199\n",
      "Trained batch 1136 batch loss 1.11664319 epoch total loss 1.1856612\n",
      "Trained batch 1137 batch loss 1.07720017 epoch total loss 1.18556583\n",
      "Trained batch 1138 batch loss 1.18567204 epoch total loss 1.18556583\n",
      "Trained batch 1139 batch loss 1.10718048 epoch total loss 1.18549705\n",
      "Trained batch 1140 batch loss 0.979629159 epoch total loss 1.18531644\n",
      "Trained batch 1141 batch loss 1.13097978 epoch total loss 1.18526888\n",
      "Trained batch 1142 batch loss 1.18114889 epoch total loss 1.18526518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1143 batch loss 1.1489073 epoch total loss 1.18523347\n",
      "Trained batch 1144 batch loss 1.13354135 epoch total loss 1.18518829\n",
      "Trained batch 1145 batch loss 1.28269529 epoch total loss 1.18527341\n",
      "Trained batch 1146 batch loss 1.21473575 epoch total loss 1.18529916\n",
      "Trained batch 1147 batch loss 1.27706492 epoch total loss 1.18537915\n",
      "Trained batch 1148 batch loss 1.27201593 epoch total loss 1.18545461\n",
      "Trained batch 1149 batch loss 1.18393064 epoch total loss 1.1854533\n",
      "Trained batch 1150 batch loss 1.13211417 epoch total loss 1.18540692\n",
      "Trained batch 1151 batch loss 1.30368555 epoch total loss 1.18550968\n",
      "Trained batch 1152 batch loss 1.20682979 epoch total loss 1.18552816\n",
      "Trained batch 1153 batch loss 1.16481578 epoch total loss 1.18551016\n",
      "Trained batch 1154 batch loss 1.38458419 epoch total loss 1.18568265\n",
      "Trained batch 1155 batch loss 1.22117698 epoch total loss 1.18571341\n",
      "Trained batch 1156 batch loss 1.21876907 epoch total loss 1.18574202\n",
      "Trained batch 1157 batch loss 1.19070923 epoch total loss 1.18574631\n",
      "Trained batch 1158 batch loss 1.33641136 epoch total loss 1.18587637\n",
      "Trained batch 1159 batch loss 1.30544472 epoch total loss 1.1859796\n",
      "Trained batch 1160 batch loss 1.2086699 epoch total loss 1.18599904\n",
      "Trained batch 1161 batch loss 1.23330343 epoch total loss 1.18603981\n",
      "Trained batch 1162 batch loss 1.24100828 epoch total loss 1.18608701\n",
      "Trained batch 1163 batch loss 1.22527957 epoch total loss 1.18612075\n",
      "Trained batch 1164 batch loss 1.18219876 epoch total loss 1.18611741\n",
      "Trained batch 1165 batch loss 1.20104814 epoch total loss 1.18613017\n",
      "Trained batch 1166 batch loss 1.19004631 epoch total loss 1.18613362\n",
      "Trained batch 1167 batch loss 1.25585699 epoch total loss 1.18619335\n",
      "Trained batch 1168 batch loss 1.15383744 epoch total loss 1.18616557\n",
      "Trained batch 1169 batch loss 1.34585965 epoch total loss 1.18630219\n",
      "Trained batch 1170 batch loss 1.1893481 epoch total loss 1.18630481\n",
      "Trained batch 1171 batch loss 1.24584198 epoch total loss 1.18635559\n",
      "Trained batch 1172 batch loss 1.20702851 epoch total loss 1.18637323\n",
      "Trained batch 1173 batch loss 1.25785804 epoch total loss 1.18643415\n",
      "Trained batch 1174 batch loss 1.16984689 epoch total loss 1.18642\n",
      "Trained batch 1175 batch loss 1.14329565 epoch total loss 1.18638325\n",
      "Trained batch 1176 batch loss 1.1613301 epoch total loss 1.18636203\n",
      "Trained batch 1177 batch loss 1.18859744 epoch total loss 1.18636394\n",
      "Trained batch 1178 batch loss 1.23569739 epoch total loss 1.18640578\n",
      "Trained batch 1179 batch loss 1.05999243 epoch total loss 1.18629861\n",
      "Trained batch 1180 batch loss 1.00503767 epoch total loss 1.18614495\n",
      "Trained batch 1181 batch loss 0.999497473 epoch total loss 1.18598688\n",
      "Trained batch 1182 batch loss 1.13440526 epoch total loss 1.18594325\n",
      "Trained batch 1183 batch loss 1.06391406 epoch total loss 1.18584013\n",
      "Trained batch 1184 batch loss 0.993534386 epoch total loss 1.18567777\n",
      "Trained batch 1185 batch loss 1.12837493 epoch total loss 1.18562937\n",
      "Trained batch 1186 batch loss 1.18422866 epoch total loss 1.18562818\n",
      "Trained batch 1187 batch loss 1.1692636 epoch total loss 1.18561447\n",
      "Trained batch 1188 batch loss 1.13678622 epoch total loss 1.18557334\n",
      "Trained batch 1189 batch loss 1.16006 epoch total loss 1.18555188\n",
      "Trained batch 1190 batch loss 1.18818521 epoch total loss 1.18555415\n",
      "Trained batch 1191 batch loss 1.30823863 epoch total loss 1.18565714\n",
      "Trained batch 1192 batch loss 1.2122339 epoch total loss 1.18567944\n",
      "Trained batch 1193 batch loss 1.13541782 epoch total loss 1.18563735\n",
      "Trained batch 1194 batch loss 1.22677135 epoch total loss 1.18567181\n",
      "Trained batch 1195 batch loss 1.28220415 epoch total loss 1.18575263\n",
      "Trained batch 1196 batch loss 1.31397486 epoch total loss 1.1858598\n",
      "Trained batch 1197 batch loss 1.26173258 epoch total loss 1.18592322\n",
      "Trained batch 1198 batch loss 1.24170613 epoch total loss 1.18596971\n",
      "Trained batch 1199 batch loss 1.25839007 epoch total loss 1.18603015\n",
      "Trained batch 1200 batch loss 1.15364671 epoch total loss 1.18600321\n",
      "Trained batch 1201 batch loss 1.21651328 epoch total loss 1.1860286\n",
      "Trained batch 1202 batch loss 1.21572757 epoch total loss 1.18605328\n",
      "Trained batch 1203 batch loss 1.23508346 epoch total loss 1.18609416\n",
      "Trained batch 1204 batch loss 1.28094661 epoch total loss 1.18617296\n",
      "Trained batch 1205 batch loss 1.29088068 epoch total loss 1.18625987\n",
      "Trained batch 1206 batch loss 1.1716907 epoch total loss 1.18624771\n",
      "Trained batch 1207 batch loss 1.25956488 epoch total loss 1.18630838\n",
      "Trained batch 1208 batch loss 1.11477089 epoch total loss 1.18624914\n",
      "Trained batch 1209 batch loss 1.31633091 epoch total loss 1.18635678\n",
      "Trained batch 1210 batch loss 1.36779451 epoch total loss 1.18650675\n",
      "Trained batch 1211 batch loss 1.32900941 epoch total loss 1.18662429\n",
      "Trained batch 1212 batch loss 1.26967859 epoch total loss 1.18669283\n",
      "Trained batch 1213 batch loss 1.15569401 epoch total loss 1.1866672\n",
      "Trained batch 1214 batch loss 1.07610667 epoch total loss 1.18657613\n",
      "Trained batch 1215 batch loss 1.17089844 epoch total loss 1.18656325\n",
      "Trained batch 1216 batch loss 1.19641912 epoch total loss 1.18657136\n",
      "Trained batch 1217 batch loss 1.14794266 epoch total loss 1.18653953\n",
      "Trained batch 1218 batch loss 1.24116731 epoch total loss 1.18658447\n",
      "Trained batch 1219 batch loss 1.11375284 epoch total loss 1.18652475\n",
      "Trained batch 1220 batch loss 1.28267956 epoch total loss 1.18660355\n",
      "Trained batch 1221 batch loss 1.16424704 epoch total loss 1.18658531\n",
      "Trained batch 1222 batch loss 1.15610909 epoch total loss 1.18656039\n",
      "Trained batch 1223 batch loss 1.12857664 epoch total loss 1.18651295\n",
      "Trained batch 1224 batch loss 1.06235611 epoch total loss 1.1864115\n",
      "Trained batch 1225 batch loss 1.20940971 epoch total loss 1.18643022\n",
      "Trained batch 1226 batch loss 1.35687339 epoch total loss 1.18656933\n",
      "Trained batch 1227 batch loss 1.17617583 epoch total loss 1.18656087\n",
      "Trained batch 1228 batch loss 1.2100594 epoch total loss 1.18658\n",
      "Trained batch 1229 batch loss 1.18952966 epoch total loss 1.18658245\n",
      "Trained batch 1230 batch loss 1.08878946 epoch total loss 1.18650293\n",
      "Trained batch 1231 batch loss 1.24529576 epoch total loss 1.18655062\n",
      "Trained batch 1232 batch loss 1.01937449 epoch total loss 1.18641496\n",
      "Trained batch 1233 batch loss 1.25168133 epoch total loss 1.18646789\n",
      "Trained batch 1234 batch loss 1.26523805 epoch total loss 1.18653178\n",
      "Trained batch 1235 batch loss 1.18853223 epoch total loss 1.18653333\n",
      "Trained batch 1236 batch loss 1.17221856 epoch total loss 1.18652177\n",
      "Trained batch 1237 batch loss 1.14096141 epoch total loss 1.18648493\n",
      "Trained batch 1238 batch loss 1.24546611 epoch total loss 1.18653262\n",
      "Trained batch 1239 batch loss 1.26332724 epoch total loss 1.18659461\n",
      "Trained batch 1240 batch loss 1.18471646 epoch total loss 1.18659306\n",
      "Trained batch 1241 batch loss 1.39064932 epoch total loss 1.18675745\n",
      "Trained batch 1242 batch loss 1.39783347 epoch total loss 1.18692744\n",
      "Trained batch 1243 batch loss 1.2889452 epoch total loss 1.18700945\n",
      "Trained batch 1244 batch loss 1.12309349 epoch total loss 1.18695807\n",
      "Trained batch 1245 batch loss 1.01827371 epoch total loss 1.18682253\n",
      "Trained batch 1246 batch loss 0.929551601 epoch total loss 1.18661606\n",
      "Trained batch 1247 batch loss 1.13746881 epoch total loss 1.18657672\n",
      "Trained batch 1248 batch loss 1.11230493 epoch total loss 1.18651712\n",
      "Trained batch 1249 batch loss 0.910436511 epoch total loss 1.18629611\n",
      "Trained batch 1250 batch loss 0.958365083 epoch total loss 1.18611372\n",
      "Trained batch 1251 batch loss 0.877632856 epoch total loss 1.18586719\n",
      "Trained batch 1252 batch loss 1.01550746 epoch total loss 1.18573117\n",
      "Trained batch 1253 batch loss 1.11020827 epoch total loss 1.18567085\n",
      "Trained batch 1254 batch loss 1.12575126 epoch total loss 1.18562305\n",
      "Trained batch 1255 batch loss 1.26871085 epoch total loss 1.18568933\n",
      "Trained batch 1256 batch loss 1.07881308 epoch total loss 1.18560421\n",
      "Trained batch 1257 batch loss 1.23818493 epoch total loss 1.18564606\n",
      "Trained batch 1258 batch loss 1.17742348 epoch total loss 1.1856395\n",
      "Trained batch 1259 batch loss 1.19024718 epoch total loss 1.1856432\n",
      "Trained batch 1260 batch loss 1.21112204 epoch total loss 1.18566346\n",
      "Trained batch 1261 batch loss 1.06605375 epoch total loss 1.18556857\n",
      "Trained batch 1262 batch loss 1.08020687 epoch total loss 1.18548512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1263 batch loss 1.08533263 epoch total loss 1.18540573\n",
      "Trained batch 1264 batch loss 1.1036973 epoch total loss 1.18534112\n",
      "Trained batch 1265 batch loss 1.24058414 epoch total loss 1.18538475\n",
      "Trained batch 1266 batch loss 1.25513315 epoch total loss 1.18543983\n",
      "Trained batch 1267 batch loss 1.19153202 epoch total loss 1.18544459\n",
      "Trained batch 1268 batch loss 1.14256799 epoch total loss 1.18541086\n",
      "Trained batch 1269 batch loss 1.15587878 epoch total loss 1.18538761\n",
      "Trained batch 1270 batch loss 1.25798357 epoch total loss 1.18544471\n",
      "Trained batch 1271 batch loss 1.26799357 epoch total loss 1.18550956\n",
      "Trained batch 1272 batch loss 1.18179739 epoch total loss 1.1855067\n",
      "Trained batch 1273 batch loss 1.28802156 epoch total loss 1.18558717\n",
      "Trained batch 1274 batch loss 1.17255 epoch total loss 1.18557692\n",
      "Trained batch 1275 batch loss 1.08946776 epoch total loss 1.18550158\n",
      "Trained batch 1276 batch loss 1.10186219 epoch total loss 1.18543601\n",
      "Trained batch 1277 batch loss 1.17609847 epoch total loss 1.18542874\n",
      "Trained batch 1278 batch loss 1.20969617 epoch total loss 1.18544769\n",
      "Trained batch 1279 batch loss 1.15651965 epoch total loss 1.18542504\n",
      "Trained batch 1280 batch loss 1.13201737 epoch total loss 1.18538332\n",
      "Trained batch 1281 batch loss 1.16196764 epoch total loss 1.18536508\n",
      "Trained batch 1282 batch loss 1.2779808 epoch total loss 1.18543732\n",
      "Trained batch 1283 batch loss 1.16233444 epoch total loss 1.18541932\n",
      "Trained batch 1284 batch loss 1.14287329 epoch total loss 1.18538606\n",
      "Trained batch 1285 batch loss 1.21012592 epoch total loss 1.18540537\n",
      "Trained batch 1286 batch loss 1.08615899 epoch total loss 1.18532813\n",
      "Trained batch 1287 batch loss 1.09840178 epoch total loss 1.18526065\n",
      "Trained batch 1288 batch loss 1.16625428 epoch total loss 1.18524587\n",
      "Trained batch 1289 batch loss 1.1562953 epoch total loss 1.18522334\n",
      "Trained batch 1290 batch loss 1.07569504 epoch total loss 1.18513846\n",
      "Trained batch 1291 batch loss 1.19352913 epoch total loss 1.1851449\n",
      "Trained batch 1292 batch loss 1.09916043 epoch total loss 1.18507838\n",
      "Trained batch 1293 batch loss 1.07053709 epoch total loss 1.18498981\n",
      "Trained batch 1294 batch loss 1.15036845 epoch total loss 1.18496299\n",
      "Trained batch 1295 batch loss 1.12780821 epoch total loss 1.18491888\n",
      "Trained batch 1296 batch loss 1.07987714 epoch total loss 1.18483782\n",
      "Trained batch 1297 batch loss 1.13687027 epoch total loss 1.18480086\n",
      "Trained batch 1298 batch loss 1.07527804 epoch total loss 1.18471646\n",
      "Trained batch 1299 batch loss 1.08790946 epoch total loss 1.18464196\n",
      "Trained batch 1300 batch loss 1.22320414 epoch total loss 1.18467152\n",
      "Trained batch 1301 batch loss 1.24794436 epoch total loss 1.18472016\n",
      "Trained batch 1302 batch loss 1.27777553 epoch total loss 1.18479168\n",
      "Trained batch 1303 batch loss 1.33558106 epoch total loss 1.18490744\n",
      "Trained batch 1304 batch loss 1.26061094 epoch total loss 1.18496549\n",
      "Trained batch 1305 batch loss 1.1947484 epoch total loss 1.18497288\n",
      "Trained batch 1306 batch loss 1.25491118 epoch total loss 1.18502641\n",
      "Trained batch 1307 batch loss 1.30749846 epoch total loss 1.18512011\n",
      "Trained batch 1308 batch loss 1.0799427 epoch total loss 1.18503976\n",
      "Trained batch 1309 batch loss 1.17498517 epoch total loss 1.18503201\n",
      "Trained batch 1310 batch loss 0.985496461 epoch total loss 1.18487966\n",
      "Trained batch 1311 batch loss 0.969226 epoch total loss 1.18471515\n",
      "Trained batch 1312 batch loss 1.04872501 epoch total loss 1.18461156\n",
      "Trained batch 1313 batch loss 1.05231023 epoch total loss 1.18451083\n",
      "Trained batch 1314 batch loss 1.07119799 epoch total loss 1.18442452\n",
      "Trained batch 1315 batch loss 0.881393254 epoch total loss 1.18419409\n",
      "Trained batch 1316 batch loss 0.891467094 epoch total loss 1.18397164\n",
      "Trained batch 1317 batch loss 0.850831628 epoch total loss 1.18371868\n",
      "Trained batch 1318 batch loss 1.11462951 epoch total loss 1.18366623\n",
      "Trained batch 1319 batch loss 1.09047508 epoch total loss 1.18359566\n",
      "Trained batch 1320 batch loss 1.12884367 epoch total loss 1.18355405\n",
      "Trained batch 1321 batch loss 1.06752765 epoch total loss 1.1834662\n",
      "Trained batch 1322 batch loss 1.20889223 epoch total loss 1.18348539\n",
      "Trained batch 1323 batch loss 1.28847742 epoch total loss 1.18356478\n",
      "Trained batch 1324 batch loss 1.20886111 epoch total loss 1.18358386\n",
      "Trained batch 1325 batch loss 1.21541357 epoch total loss 1.18360794\n",
      "Trained batch 1326 batch loss 1.20284367 epoch total loss 1.18362248\n",
      "Trained batch 1327 batch loss 1.28667355 epoch total loss 1.18370008\n",
      "Trained batch 1328 batch loss 1.36647379 epoch total loss 1.18383765\n",
      "Trained batch 1329 batch loss 1.28309226 epoch total loss 1.1839124\n",
      "Trained batch 1330 batch loss 1.23440599 epoch total loss 1.1839503\n",
      "Trained batch 1331 batch loss 1.29979062 epoch total loss 1.18403733\n",
      "Trained batch 1332 batch loss 1.3064239 epoch total loss 1.18412924\n",
      "Trained batch 1333 batch loss 1.2024132 epoch total loss 1.18414295\n",
      "Trained batch 1334 batch loss 1.20912933 epoch total loss 1.18416166\n",
      "Trained batch 1335 batch loss 1.12996423 epoch total loss 1.18412113\n",
      "Trained batch 1336 batch loss 1.07316566 epoch total loss 1.18403804\n",
      "Trained batch 1337 batch loss 1.24691081 epoch total loss 1.18408501\n",
      "Trained batch 1338 batch loss 1.19392371 epoch total loss 1.1840924\n",
      "Trained batch 1339 batch loss 1.19991708 epoch total loss 1.18410432\n",
      "Trained batch 1340 batch loss 1.12885177 epoch total loss 1.18406308\n",
      "Trained batch 1341 batch loss 1.26331306 epoch total loss 1.1841222\n",
      "Trained batch 1342 batch loss 1.08659244 epoch total loss 1.18404949\n",
      "Trained batch 1343 batch loss 1.20123518 epoch total loss 1.18406236\n",
      "Trained batch 1344 batch loss 1.08918417 epoch total loss 1.18399179\n",
      "Trained batch 1345 batch loss 1.00266123 epoch total loss 1.18385696\n",
      "Trained batch 1346 batch loss 1.02108598 epoch total loss 1.18373609\n",
      "Trained batch 1347 batch loss 1.22975373 epoch total loss 1.18377018\n",
      "Trained batch 1348 batch loss 1.21600413 epoch total loss 1.18379414\n",
      "Trained batch 1349 batch loss 1.25234413 epoch total loss 1.18384492\n",
      "Trained batch 1350 batch loss 1.19075751 epoch total loss 1.18385\n",
      "Trained batch 1351 batch loss 1.31944203 epoch total loss 1.18395042\n",
      "Trained batch 1352 batch loss 1.11720562 epoch total loss 1.18390107\n",
      "Trained batch 1353 batch loss 1.03800356 epoch total loss 1.18379319\n",
      "Trained batch 1354 batch loss 1.08629549 epoch total loss 1.18372118\n",
      "Trained batch 1355 batch loss 1.21602249 epoch total loss 1.18374515\n",
      "Trained batch 1356 batch loss 1.17803574 epoch total loss 1.18374085\n",
      "Trained batch 1357 batch loss 1.23394883 epoch total loss 1.18377793\n",
      "Trained batch 1358 batch loss 1.26057458 epoch total loss 1.18383443\n",
      "Trained batch 1359 batch loss 1.21992552 epoch total loss 1.18386102\n",
      "Trained batch 1360 batch loss 1.28460884 epoch total loss 1.18393517\n",
      "Trained batch 1361 batch loss 1.22306824 epoch total loss 1.18396389\n",
      "Trained batch 1362 batch loss 1.39954185 epoch total loss 1.1841222\n",
      "Trained batch 1363 batch loss 1.22378957 epoch total loss 1.18415129\n",
      "Trained batch 1364 batch loss 1.37824869 epoch total loss 1.18429363\n",
      "Trained batch 1365 batch loss 1.29638338 epoch total loss 1.18437576\n",
      "Trained batch 1366 batch loss 1.29404938 epoch total loss 1.18445599\n",
      "Trained batch 1367 batch loss 1.19388151 epoch total loss 1.1844629\n",
      "Trained batch 1368 batch loss 1.09817266 epoch total loss 1.18439984\n",
      "Trained batch 1369 batch loss 1.17740273 epoch total loss 1.1843946\n",
      "Trained batch 1370 batch loss 1.21063101 epoch total loss 1.18441379\n",
      "Trained batch 1371 batch loss 1.10228789 epoch total loss 1.18435383\n",
      "Trained batch 1372 batch loss 1.23877192 epoch total loss 1.18439353\n",
      "Trained batch 1373 batch loss 0.99529928 epoch total loss 1.18425572\n",
      "Trained batch 1374 batch loss 1.18538618 epoch total loss 1.18425655\n",
      "Trained batch 1375 batch loss 1.31271291 epoch total loss 1.18435\n",
      "Trained batch 1376 batch loss 1.2631458 epoch total loss 1.18440735\n",
      "Trained batch 1377 batch loss 1.21299636 epoch total loss 1.1844281\n",
      "Trained batch 1378 batch loss 1.14569283 epoch total loss 1.18440008\n",
      "Trained batch 1379 batch loss 1.13053155 epoch total loss 1.18436098\n",
      "Trained batch 1380 batch loss 1.11031795 epoch total loss 1.18430734\n",
      "Trained batch 1381 batch loss 1.24264729 epoch total loss 1.18434954\n",
      "Trained batch 1382 batch loss 1.26621211 epoch total loss 1.1844089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1383 batch loss 1.33101201 epoch total loss 1.18451488\n",
      "Trained batch 1384 batch loss 1.29051793 epoch total loss 1.18459153\n",
      "Trained batch 1385 batch loss 1.24554741 epoch total loss 1.18463552\n",
      "Trained batch 1386 batch loss 1.23625684 epoch total loss 1.18467271\n",
      "Trained batch 1387 batch loss 1.1506232 epoch total loss 1.18464816\n",
      "Trained batch 1388 batch loss 1.0952791 epoch total loss 1.1845839\n",
      "Epoch 5 train loss 1.1845839023590088\n",
      "Validated batch 1 batch loss 1.26104105\n",
      "Validated batch 2 batch loss 1.22136259\n",
      "Validated batch 3 batch loss 1.12423217\n",
      "Validated batch 4 batch loss 1.15794802\n",
      "Validated batch 5 batch loss 1.05289638\n",
      "Validated batch 6 batch loss 1.19900417\n",
      "Validated batch 7 batch loss 1.23993516\n",
      "Validated batch 8 batch loss 1.04862666\n",
      "Validated batch 9 batch loss 1.1938833\n",
      "Validated batch 10 batch loss 1.13481092\n",
      "Validated batch 11 batch loss 1.24239421\n",
      "Validated batch 12 batch loss 1.17442977\n",
      "Validated batch 13 batch loss 1.18994272\n",
      "Validated batch 14 batch loss 1.13882172\n",
      "Validated batch 15 batch loss 1.09612989\n",
      "Validated batch 16 batch loss 1.18266892\n",
      "Validated batch 17 batch loss 1.12176073\n",
      "Validated batch 18 batch loss 1.256845\n",
      "Validated batch 19 batch loss 1.24179077\n",
      "Validated batch 20 batch loss 1.40946555\n",
      "Validated batch 21 batch loss 1.20468295\n",
      "Validated batch 22 batch loss 1.19609618\n",
      "Validated batch 23 batch loss 1.01782489\n",
      "Validated batch 24 batch loss 1.06783319\n",
      "Validated batch 25 batch loss 1.14714301\n",
      "Validated batch 26 batch loss 1.15174842\n",
      "Validated batch 27 batch loss 1.12763906\n",
      "Validated batch 28 batch loss 1.23728275\n",
      "Validated batch 29 batch loss 1.33187652\n",
      "Validated batch 30 batch loss 0.998125851\n",
      "Validated batch 31 batch loss 1.16325295\n",
      "Validated batch 32 batch loss 1.15245616\n",
      "Validated batch 33 batch loss 1.26088572\n",
      "Validated batch 34 batch loss 1.18484104\n",
      "Validated batch 35 batch loss 1.00195289\n",
      "Validated batch 36 batch loss 1.03536856\n",
      "Validated batch 37 batch loss 1.08928788\n",
      "Validated batch 38 batch loss 1.11246097\n",
      "Validated batch 39 batch loss 1.12479818\n",
      "Validated batch 40 batch loss 1.15741849\n",
      "Validated batch 41 batch loss 1.06660748\n",
      "Validated batch 42 batch loss 1.2124691\n",
      "Validated batch 43 batch loss 1.27330017\n",
      "Validated batch 44 batch loss 1.25085068\n",
      "Validated batch 45 batch loss 1.15307415\n",
      "Validated batch 46 batch loss 1.07098293\n",
      "Validated batch 47 batch loss 1.13659334\n",
      "Validated batch 48 batch loss 1.12724864\n",
      "Validated batch 49 batch loss 1.14678133\n",
      "Validated batch 50 batch loss 1.12116086\n",
      "Validated batch 51 batch loss 1.12612295\n",
      "Validated batch 52 batch loss 1.2652024\n",
      "Validated batch 53 batch loss 1.16700315\n",
      "Validated batch 54 batch loss 1.0252341\n",
      "Validated batch 55 batch loss 1.10395515\n",
      "Validated batch 56 batch loss 1.11478317\n",
      "Validated batch 57 batch loss 1.06325102\n",
      "Validated batch 58 batch loss 1.18482769\n",
      "Validated batch 59 batch loss 1.15280426\n",
      "Validated batch 60 batch loss 1.11769235\n",
      "Validated batch 61 batch loss 1.27693272\n",
      "Validated batch 62 batch loss 1.25843477\n",
      "Validated batch 63 batch loss 1.14108193\n",
      "Validated batch 64 batch loss 1.30287445\n",
      "Validated batch 65 batch loss 0.951861501\n",
      "Validated batch 66 batch loss 1.14606977\n",
      "Validated batch 67 batch loss 1.10039079\n",
      "Validated batch 68 batch loss 1.16282153\n",
      "Validated batch 69 batch loss 1.35346067\n",
      "Validated batch 70 batch loss 1.1259439\n",
      "Validated batch 71 batch loss 1.05969512\n",
      "Validated batch 72 batch loss 1.16510677\n",
      "Validated batch 73 batch loss 1.11665523\n",
      "Validated batch 74 batch loss 1.12631202\n",
      "Validated batch 75 batch loss 1.19270122\n",
      "Validated batch 76 batch loss 1.18771744\n",
      "Validated batch 77 batch loss 1.13139629\n",
      "Validated batch 78 batch loss 1.21422064\n",
      "Validated batch 79 batch loss 1.14012313\n",
      "Validated batch 80 batch loss 1.20653915\n",
      "Validated batch 81 batch loss 1.21698117\n",
      "Validated batch 82 batch loss 1.11069274\n",
      "Validated batch 83 batch loss 1.15851223\n",
      "Validated batch 84 batch loss 1.21843219\n",
      "Validated batch 85 batch loss 1.12469244\n",
      "Validated batch 86 batch loss 1.34572399\n",
      "Validated batch 87 batch loss 1.19591093\n",
      "Validated batch 88 batch loss 1.10340607\n",
      "Validated batch 89 batch loss 1.23252857\n",
      "Validated batch 90 batch loss 1.17988837\n",
      "Validated batch 91 batch loss 1.0952965\n",
      "Validated batch 92 batch loss 1.17769289\n",
      "Validated batch 93 batch loss 1.18668723\n",
      "Validated batch 94 batch loss 1.23328662\n",
      "Validated batch 95 batch loss 1.10156107\n",
      "Validated batch 96 batch loss 1.17547297\n",
      "Validated batch 97 batch loss 1.18254\n",
      "Validated batch 98 batch loss 1.16598582\n",
      "Validated batch 99 batch loss 1.19941974\n",
      "Validated batch 100 batch loss 1.20237255\n",
      "Validated batch 101 batch loss 1.13310814\n",
      "Validated batch 102 batch loss 1.29370916\n",
      "Validated batch 103 batch loss 1.10343313\n",
      "Validated batch 104 batch loss 1.06794608\n",
      "Validated batch 105 batch loss 1.15007496\n",
      "Validated batch 106 batch loss 1.26404762\n",
      "Validated batch 107 batch loss 1.29852545\n",
      "Validated batch 108 batch loss 1.34451342\n",
      "Validated batch 109 batch loss 1.16221333\n",
      "Validated batch 110 batch loss 1.32537162\n",
      "Validated batch 111 batch loss 1.20172691\n",
      "Validated batch 112 batch loss 1.26467526\n",
      "Validated batch 113 batch loss 1.23486412\n",
      "Validated batch 114 batch loss 0.930141151\n",
      "Validated batch 115 batch loss 1.1858108\n",
      "Validated batch 116 batch loss 1.1753844\n",
      "Validated batch 117 batch loss 1.09413695\n",
      "Validated batch 118 batch loss 1.14967513\n",
      "Validated batch 119 batch loss 1.08824086\n",
      "Validated batch 120 batch loss 1.12621558\n",
      "Validated batch 121 batch loss 1.25405526\n",
      "Validated batch 122 batch loss 1.16632915\n",
      "Validated batch 123 batch loss 1.22853398\n",
      "Validated batch 124 batch loss 1.22274852\n",
      "Validated batch 125 batch loss 1.2209549\n",
      "Validated batch 126 batch loss 1.19765329\n",
      "Validated batch 127 batch loss 1.3956759\n",
      "Validated batch 128 batch loss 1.19921947\n",
      "Validated batch 129 batch loss 1.32837141\n",
      "Validated batch 130 batch loss 1.26443946\n",
      "Validated batch 131 batch loss 1.31738687\n",
      "Validated batch 132 batch loss 1.25724018\n",
      "Validated batch 133 batch loss 1.12374413\n",
      "Validated batch 134 batch loss 1.19281816\n",
      "Validated batch 135 batch loss 1.22810662\n",
      "Validated batch 136 batch loss 1.2669152\n",
      "Validated batch 137 batch loss 1.14824736\n",
      "Validated batch 138 batch loss 1.22918975\n",
      "Validated batch 139 batch loss 1.19361269\n",
      "Validated batch 140 batch loss 1.18920219\n",
      "Validated batch 141 batch loss 1.18132639\n",
      "Validated batch 142 batch loss 1.17305124\n",
      "Validated batch 143 batch loss 1.25847793\n",
      "Validated batch 144 batch loss 1.36970508\n",
      "Validated batch 145 batch loss 1.13747585\n",
      "Validated batch 146 batch loss 1.23227382\n",
      "Validated batch 147 batch loss 1.11024523\n",
      "Validated batch 148 batch loss 1.28274715\n",
      "Validated batch 149 batch loss 1.21051979\n",
      "Validated batch 150 batch loss 1.14667547\n",
      "Validated batch 151 batch loss 1.2563237\n",
      "Validated batch 152 batch loss 1.21783197\n",
      "Validated batch 153 batch loss 1.25290203\n",
      "Validated batch 154 batch loss 1.22018135\n",
      "Validated batch 155 batch loss 1.23748517\n",
      "Validated batch 156 batch loss 1.16132665\n",
      "Validated batch 157 batch loss 1.11956215\n",
      "Validated batch 158 batch loss 1.1786027\n",
      "Validated batch 159 batch loss 1.12736535\n",
      "Validated batch 160 batch loss 1.18092418\n",
      "Validated batch 161 batch loss 1.13000154\n",
      "Validated batch 162 batch loss 1.21741796\n",
      "Validated batch 163 batch loss 1.18684745\n",
      "Validated batch 164 batch loss 1.21373391\n",
      "Validated batch 165 batch loss 1.06495214\n",
      "Validated batch 166 batch loss 1.18198943\n",
      "Validated batch 167 batch loss 1.21699345\n",
      "Validated batch 168 batch loss 1.25998783\n",
      "Validated batch 169 batch loss 1.3088634\n",
      "Validated batch 170 batch loss 1.22061753\n",
      "Validated batch 171 batch loss 1.12028086\n",
      "Validated batch 172 batch loss 1.15759254\n",
      "Validated batch 173 batch loss 1.21631658\n",
      "Validated batch 174 batch loss 1.18758905\n",
      "Validated batch 175 batch loss 1.27014589\n",
      "Validated batch 176 batch loss 1.31538177\n",
      "Validated batch 177 batch loss 1.2083075\n",
      "Validated batch 178 batch loss 1.27798212\n",
      "Validated batch 179 batch loss 1.22496128\n",
      "Validated batch 180 batch loss 1.14165831\n",
      "Validated batch 181 batch loss 1.16297674\n",
      "Validated batch 182 batch loss 1.06067979\n",
      "Validated batch 183 batch loss 1.26101327\n",
      "Validated batch 184 batch loss 1.13864851\n",
      "Validated batch 185 batch loss 1.21961713\n",
      "Epoch 5 val loss 1.1812769174575806\n",
      "Model /aiffel/aiffel/mpii/models/model_HG-epoch-5-loss-1.1813.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋과 모델, 훈련용 객체를 조립하는 함수\n",
    "# 주의할 점은 with strategy.scope():부분이 반드시 필요\n",
    "# 또 데이터셋도 experimental_distribute_dataset를 통해 연결해 줘야 한다는 것도 중요\n",
    "\n",
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "print('슝=3')\n",
    "\n",
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2ea11",
   "metadata": {},
   "source": [
    "### 7. 예측 엔진 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db3343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습한 모델 불러옴\n",
    "\n",
    "WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model_HG-epoch-5-loss-1.1813.h5')\n",
    "\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "# 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n",
    "# model.load_weights(best_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90ea5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용했던 keypoint 들을 사용해야 하기 때문에 필요한 변수를 지정해 줍니다. \n",
    "# 변수에 저장되는 것은 해당 부위를 나타내는 인덱스예요\n",
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]\n",
    "\n",
    "print('슝=3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0ec83c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# heatmap에서 최대값을 찾는 함수\n",
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ba138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "#  3x3 필터를 이용해서 근사치를 구해줌\n",
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd92ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 이미지와 keypoint를 출력하는 함수\n",
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "989834a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "#  keypoint들과 뼈대연결 그림 함수\n",
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd9630fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트 이미지를 이용해 모델의 성능을 확인\n",
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62f51e",
   "metadata": {},
   "source": [
    "###  8. Project: 모델 바꿔보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7991ab6",
   "metadata": {},
   "source": [
    "### simplebaseline 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f63a6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d14cbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder: Backbone: ResNet\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f6b0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder: Deconv Module + Upsampling\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(2,2), strides=2))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff473e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dafa556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplebaseline 모델 생성 함수\n",
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172a805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e977289",
   "metadata": {},
   "source": [
    "### 학습 엔진 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40942f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class TrainerB(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model_BL-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fbddf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋과 모델, 훈련용 객체를 조립하는 함수\n",
    "# 주의할 점은 with strategy.scope():부분이 반드시 필요\n",
    "# 또 데이터셋도 experimental_distribute_dataset를 통해 연결해 줘야 한다는 것도 중요\n",
    "\n",
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "#       model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        model = Simplebaseline(IMAGE_SHAPE)\n",
    "\n",
    "        trainer = TrainerB(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "100c18b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    /tmp/ipykernel_84/3116070808.py:53 train_step  *\n",
      "        loss = self.compute_loss(labels, outputs)\n",
      "    /tmp/ipykernel_84/3116070808.py:44 compute_loss  *\n",
      "        loss += tf.math.reduce_mean(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n",
      "        raise e\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n",
      "        return gen_math_ops.add_v2(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n",
      "        raise TypeError(\n",
      "\n",
      "    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 695, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "TypeError: in user code:\n",
      "\n",
      "    /tmp/ipykernel_84/3116070808.py:53 train_step  *\n",
      "        loss = self.compute_loss(labels, outputs)\n",
      "    /tmp/ipykernel_84/3116070808.py:44 compute_loss  *\n",
      "        loss += tf.math.reduce_mean(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n",
      "        raise e\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n",
      "        return gen_math_ops.add_v2(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n",
      "        raise TypeError(\n",
      "\n",
      "    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_84/3116070808.py:75 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_84/3116070808.py:53 train_step  *\n        loss = self.compute_loss(labels, outputs)\n    /tmp/ipykernel_84/3116070808.py:44 compute_loss  *\n        loss += tf.math.reduce_mean(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84/1927619030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0007\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbest_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tfrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tfrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_84/1586948299.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dist_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'슝=3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_84/3116070808.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_dist_dataset, val_dist_dataset)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 epoch, self.current_learning_rate))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             train_total_loss, num_train_batches = distributed_train_epoch(\n\u001b[0m\u001b[1;32m    111\u001b[0m                 train_dist_dataset)\n\u001b[1;32m    112\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /tmp/ipykernel_84/3116070808.py:75 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_84/3116070808.py:53 train_step  *\n        loss = self.compute_loss(labels, outputs)\n    /tmp/ipykernel_84/3116070808.py:44 compute_loss  *\n        loss += tf.math.reduce_mean(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba0b89",
   "metadata": {},
   "source": [
    "### 9. 결론 및 회고\n",
    "\n",
    "* 휴먼포즈 에스티메이션 모델의 예측결과가 썩 좋게 나오지 않음. 생각보다 어려운 과제인 것 같음\n",
    "* Hourglass 모델에 의한 결과 오른쪽 팔꿈치 및 오른쪽 무릎을 인식하지 못하는 문제 발생: 옆면의 인식률이 떨어짐\n",
    "* simplebaseline 모델 내용은 맞게 한 것 같으나, 훈련과정에서 tfrecord를 처리하지 못한는 문제 발생함. 코드는 이전 기수들 및 기타 자료들을 참고했을 때 맞게 작성한 것 같음 --> 에러 내용을 자세히 보니 내부적으로 type mismatch로 나오는데 conv 내부의 어디서 생긴 오류인지 결국 찾지 못했음 --> 딥러닝에서 차원과 타입을 맞추는 작업이 중요함을 알게 되었음\n",
    "* Deep Going 마지막 과제를 끝내서 홀가분하기는 하나, 아이펠톤 프로젝트가 기다리고 있음: 더욱 집중해야겠음\n",
    "\n",
    "< reference >\n",
    "* (paper) Articulated human detection with flexible mixtures-of-parts    \n",
    "* (paper) DeepPose: Human Pose Estimation via Deep Neural Networks     \n",
    "* https://yeomko.tistory.com/21\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
